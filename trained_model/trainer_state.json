{
  "best_global_step": 33605,
  "best_metric": 0.7093276338435434,
  "best_model_checkpoint": "C:\\Users\\kulka\\Downloads\\real_daddy\\checkpoint-33605",
  "epoch": 47.0,
  "eval_steps": 500,
  "global_step": 33605,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03498950314905528,
      "grad_norm": 6.350561618804932,
      "learning_rate": 5.995972027972028e-05,
      "loss": 2.964693603515625,
      "step": 25
    },
    {
      "epoch": 0.06997900629811056,
      "grad_norm": 6.027331352233887,
      "learning_rate": 5.9917762237762237e-05,
      "loss": 2.170542449951172,
      "step": 50
    },
    {
      "epoch": 0.10496850944716585,
      "grad_norm": 4.809444427490234,
      "learning_rate": 5.98758041958042e-05,
      "loss": 1.9105560302734375,
      "step": 75
    },
    {
      "epoch": 0.13995801259622112,
      "grad_norm": 3.795482873916626,
      "learning_rate": 5.9833846153846156e-05,
      "loss": 1.6500830078125,
      "step": 100
    },
    {
      "epoch": 0.17494751574527642,
      "grad_norm": 3.3891801834106445,
      "learning_rate": 5.9793566433566435e-05,
      "loss": 1.5419279479980468,
      "step": 125
    },
    {
      "epoch": 0.2099370188943317,
      "grad_norm": 4.034114837646484,
      "learning_rate": 5.975160839160839e-05,
      "loss": 1.4625106811523438,
      "step": 150
    },
    {
      "epoch": 0.244926522043387,
      "grad_norm": 3.3351829051971436,
      "learning_rate": 5.9709650349650354e-05,
      "loss": 1.3503421020507813,
      "step": 175
    },
    {
      "epoch": 0.27991602519244224,
      "grad_norm": 2.11344575881958,
      "learning_rate": 5.966769230769231e-05,
      "loss": 1.1896604919433593,
      "step": 200
    },
    {
      "epoch": 0.31490552834149754,
      "grad_norm": 2.1579511165618896,
      "learning_rate": 5.9625734265734267e-05,
      "loss": 1.1879769134521485,
      "step": 225
    },
    {
      "epoch": 0.34989503149055284,
      "grad_norm": 2.6125807762145996,
      "learning_rate": 5.958377622377622e-05,
      "loss": 1.0867613983154296,
      "step": 250
    },
    {
      "epoch": 0.38488453463960814,
      "grad_norm": 2.1388351917266846,
      "learning_rate": 5.954181818181818e-05,
      "loss": 1.0694518280029297,
      "step": 275
    },
    {
      "epoch": 0.4198740377886634,
      "grad_norm": 1.7436003684997559,
      "learning_rate": 5.949986013986014e-05,
      "loss": 1.0976732635498048,
      "step": 300
    },
    {
      "epoch": 0.4548635409377187,
      "grad_norm": 2.4802253246307373,
      "learning_rate": 5.945958041958042e-05,
      "loss": 0.9917805480957032,
      "step": 325
    },
    {
      "epoch": 0.489853044086774,
      "grad_norm": 1.7078733444213867,
      "learning_rate": 5.941762237762238e-05,
      "loss": 0.9601499938964844,
      "step": 350
    },
    {
      "epoch": 0.5248425472358292,
      "grad_norm": 2.679063558578491,
      "learning_rate": 5.937566433566434e-05,
      "loss": 0.9781071472167969,
      "step": 375
    },
    {
      "epoch": 0.5598320503848845,
      "grad_norm": 1.4287163019180298,
      "learning_rate": 5.9333706293706297e-05,
      "loss": 0.9317006683349609,
      "step": 400
    },
    {
      "epoch": 0.5948215535339398,
      "grad_norm": 1.9581820964813232,
      "learning_rate": 5.929174825174825e-05,
      "loss": 0.9989088439941406,
      "step": 425
    },
    {
      "epoch": 0.6298110566829951,
      "grad_norm": Infinity,
      "learning_rate": 5.9249790209790216e-05,
      "loss": 0.9186995697021484,
      "step": 450
    },
    {
      "epoch": 0.6648005598320503,
      "grad_norm": 2.023176431655884,
      "learning_rate": 5.9209510489510495e-05,
      "loss": 0.9252787780761719,
      "step": 475
    },
    {
      "epoch": 0.6997900629811057,
      "grad_norm": 4.048007965087891,
      "learning_rate": 5.916755244755245e-05,
      "loss": 0.9475395965576172,
      "step": 500
    },
    {
      "epoch": 0.7347795661301609,
      "grad_norm": 2.247709274291992,
      "learning_rate": 5.912559440559441e-05,
      "loss": 0.981253890991211,
      "step": 525
    },
    {
      "epoch": 0.7697690692792163,
      "grad_norm": 1.487727165222168,
      "learning_rate": 5.9083636363636363e-05,
      "loss": 0.8934674835205079,
      "step": 550
    },
    {
      "epoch": 0.8047585724282715,
      "grad_norm": 1.2730344533920288,
      "learning_rate": 5.904167832167832e-05,
      "loss": 0.8246761322021484,
      "step": 575
    },
    {
      "epoch": 0.8397480755773268,
      "grad_norm": 1.152481198310852,
      "learning_rate": 5.899972027972028e-05,
      "loss": 0.8271739959716797,
      "step": 600
    },
    {
      "epoch": 0.8747375787263821,
      "grad_norm": 1.210861086845398,
      "learning_rate": 5.895776223776224e-05,
      "loss": 0.8392269897460938,
      "step": 625
    },
    {
      "epoch": 0.9097270818754374,
      "grad_norm": 1.663046956062317,
      "learning_rate": 5.8915804195804195e-05,
      "loss": 0.8687741088867188,
      "step": 650
    },
    {
      "epoch": 0.9447165850244926,
      "grad_norm": 1.7629990577697754,
      "learning_rate": 5.887384615384616e-05,
      "loss": 0.876166763305664,
      "step": 675
    },
    {
      "epoch": 0.979706088173548,
      "grad_norm": 1.3061397075653076,
      "learning_rate": 5.8831888111888115e-05,
      "loss": 0.8721937561035156,
      "step": 700
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.38299015164375305,
      "eval_mean_accuracy": 0.6827567439442115,
      "eval_mean_iou": 0.5892862419823014,
      "eval_overall_accuracy": 0.8557850449619233,
      "eval_per_category_accuracy": [
        0.8723136833329415,
        0.8665515482105448,
        0.4174504990497225,
        0.16627561565762955,
        0.7807295512772877,
        0.9932195661371432
      ],
      "eval_per_category_iou": [
        0.7133262776682344,
        0.6816064694391608,
        0.3483609800367033,
        0.16158067190399117,
        0.6500523032546032,
        0.9807907495911158
      ],
      "eval_runtime": 114.6872,
      "eval_samples_per_second": 2.764,
      "eval_steps_per_second": 1.386,
      "step": 715
    },
    {
      "epoch": 1.0139958012596222,
      "grad_norm": 1.175480842590332,
      "learning_rate": 5.878993006993007e-05,
      "loss": 0.8383302307128906,
      "step": 725
    },
    {
      "epoch": 1.0489853044086774,
      "grad_norm": 2.657634973526001,
      "learning_rate": 5.8747972027972034e-05,
      "loss": 0.7635447692871093,
      "step": 750
    },
    {
      "epoch": 1.0839748075577327,
      "grad_norm": 1.2850217819213867,
      "learning_rate": 5.870601398601398e-05,
      "loss": 0.7944428253173829,
      "step": 775
    },
    {
      "epoch": 1.118964310706788,
      "grad_norm": 0.9845923185348511,
      "learning_rate": 5.8664055944055946e-05,
      "loss": 0.800896987915039,
      "step": 800
    },
    {
      "epoch": 1.1539538138558432,
      "grad_norm": 1.232102394104004,
      "learning_rate": 5.86220979020979e-05,
      "loss": 0.8424488067626953,
      "step": 825
    },
    {
      "epoch": 1.1889433170048984,
      "grad_norm": 1.8988394737243652,
      "learning_rate": 5.858013986013986e-05,
      "loss": 0.7309683990478516,
      "step": 850
    },
    {
      "epoch": 1.2239328201539539,
      "grad_norm": 0.9544891715049744,
      "learning_rate": 5.853818181818182e-05,
      "loss": 0.7910940551757812,
      "step": 875
    },
    {
      "epoch": 1.2589223233030091,
      "grad_norm": 1.0930825471878052,
      "learning_rate": 5.849622377622378e-05,
      "loss": 0.7766700744628906,
      "step": 900
    },
    {
      "epoch": 1.2939118264520644,
      "grad_norm": 1.3997631072998047,
      "learning_rate": 5.8454265734265735e-05,
      "loss": 0.8407450103759766,
      "step": 925
    },
    {
      "epoch": 1.3289013296011196,
      "grad_norm": 1.0645573139190674,
      "learning_rate": 5.84123076923077e-05,
      "loss": 0.7828648376464844,
      "step": 950
    },
    {
      "epoch": 1.363890832750175,
      "grad_norm": 0.7439548373222351,
      "learning_rate": 5.8370349650349654e-05,
      "loss": 0.7658182525634766,
      "step": 975
    },
    {
      "epoch": 1.3988803358992303,
      "grad_norm": 2.111132860183716,
      "learning_rate": 5.832839160839161e-05,
      "loss": 0.7429065704345703,
      "step": 1000
    },
    {
      "epoch": 1.4338698390482856,
      "grad_norm": 0.5862048268318176,
      "learning_rate": 5.8286433566433566e-05,
      "loss": 0.8100264739990234,
      "step": 1025
    },
    {
      "epoch": 1.4688593421973408,
      "grad_norm": 1.4827238321304321,
      "learning_rate": 5.824447552447552e-05,
      "loss": 0.7797738647460938,
      "step": 1050
    },
    {
      "epoch": 1.503848845346396,
      "grad_norm": 2.885782241821289,
      "learning_rate": 5.8202517482517486e-05,
      "loss": 0.7304294586181641,
      "step": 1075
    },
    {
      "epoch": 1.5388383484954513,
      "grad_norm": 15.107955932617188,
      "learning_rate": 5.816055944055944e-05,
      "loss": 0.8469554138183594,
      "step": 1100
    },
    {
      "epoch": 1.5738278516445066,
      "grad_norm": 1.5158218145370483,
      "learning_rate": 5.81186013986014e-05,
      "loss": 0.7934796905517578,
      "step": 1125
    },
    {
      "epoch": 1.6088173547935618,
      "grad_norm": 1.492841124534607,
      "learning_rate": 5.807664335664336e-05,
      "loss": 0.7418997192382812,
      "step": 1150
    },
    {
      "epoch": 1.643806857942617,
      "grad_norm": 16.4323787689209,
      "learning_rate": 5.803468531468532e-05,
      "loss": 0.7019097900390625,
      "step": 1175
    },
    {
      "epoch": 1.6787963610916725,
      "grad_norm": 1.2003321647644043,
      "learning_rate": 5.799272727272728e-05,
      "loss": 0.7258333587646484,
      "step": 1200
    },
    {
      "epoch": 1.7137858642407278,
      "grad_norm": 1.4387775659561157,
      "learning_rate": 5.795076923076923e-05,
      "loss": 0.7024622344970703,
      "step": 1225
    },
    {
      "epoch": 1.7487753673897832,
      "grad_norm": 2.480107307434082,
      "learning_rate": 5.7908811188811186e-05,
      "loss": 0.7934458923339843,
      "step": 1250
    },
    {
      "epoch": 1.7837648705388385,
      "grad_norm": 0.8766248226165771,
      "learning_rate": 5.786685314685315e-05,
      "loss": 0.7509917449951172,
      "step": 1275
    },
    {
      "epoch": 1.8187543736878937,
      "grad_norm": 2.274631977081299,
      "learning_rate": 5.7824895104895106e-05,
      "loss": 0.7029474639892578,
      "step": 1300
    },
    {
      "epoch": 1.853743876836949,
      "grad_norm": 1.150897741317749,
      "learning_rate": 5.778293706293706e-05,
      "loss": 0.7817769622802735,
      "step": 1325
    },
    {
      "epoch": 1.8887333799860042,
      "grad_norm": 2.0067355632781982,
      "learning_rate": 5.7740979020979025e-05,
      "loss": 0.7737173461914062,
      "step": 1350
    },
    {
      "epoch": 1.9237228831350595,
      "grad_norm": 1.3844538927078247,
      "learning_rate": 5.769902097902098e-05,
      "loss": 0.742679214477539,
      "step": 1375
    },
    {
      "epoch": 1.9587123862841147,
      "grad_norm": 0.5830986499786377,
      "learning_rate": 5.7657062937062944e-05,
      "loss": 0.6600216674804688,
      "step": 1400
    },
    {
      "epoch": 1.99370188943317,
      "grad_norm": 1.400020718574524,
      "learning_rate": 5.76151048951049e-05,
      "loss": 0.7472808074951172,
      "step": 1425
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3423532545566559,
      "eval_mean_accuracy": 0.7386383600658867,
      "eval_mean_iou": 0.6422225843839198,
      "eval_overall_accuracy": 0.867893989153841,
      "eval_per_category_accuracy": [
        0.8731266238373202,
        0.850862045817177,
        0.49452431926539253,
        0.4101695721360303,
        0.8089550058345231,
        0.9941925935048765
      ],
      "eval_per_category_iou": [
        0.7406321209010689,
        0.7015640610732078,
        0.39883419142516885,
        0.3570423142605786,
        0.6731608405665165,
        0.9821019780769776
      ],
      "eval_runtime": 32.0458,
      "eval_samples_per_second": 9.892,
      "eval_steps_per_second": 4.962,
      "step": 1430
    },
    {
      "epoch": 2.0279916025192444,
      "grad_norm": 1.6783244609832764,
      "learning_rate": 5.757314685314685e-05,
      "loss": 0.7735440826416016,
      "step": 1450
    },
    {
      "epoch": 2.0629811056682996,
      "grad_norm": 1.1146693229675293,
      "learning_rate": 5.753118881118881e-05,
      "loss": 0.786917724609375,
      "step": 1475
    },
    {
      "epoch": 2.097970608817355,
      "grad_norm": 1.771989107131958,
      "learning_rate": 5.748923076923077e-05,
      "loss": 0.7476705932617187,
      "step": 1500
    },
    {
      "epoch": 2.13296011196641,
      "grad_norm": 1.175536036491394,
      "learning_rate": 5.7447272727272726e-05,
      "loss": 0.7122637176513672,
      "step": 1525
    },
    {
      "epoch": 2.1679496151154654,
      "grad_norm": 2.7640328407287598,
      "learning_rate": 5.740531468531469e-05,
      "loss": 0.7030294036865234,
      "step": 1550
    },
    {
      "epoch": 2.2029391182645206,
      "grad_norm": 2.465026617050171,
      "learning_rate": 5.7363356643356645e-05,
      "loss": 0.7007377624511719,
      "step": 1575
    },
    {
      "epoch": 2.237928621413576,
      "grad_norm": 1.4169209003448486,
      "learning_rate": 5.732139860139861e-05,
      "loss": 0.7154148864746094,
      "step": 1600
    },
    {
      "epoch": 2.272918124562631,
      "grad_norm": 3.132849931716919,
      "learning_rate": 5.7279440559440564e-05,
      "loss": 0.7418103790283204,
      "step": 1625
    },
    {
      "epoch": 2.3079076277116863,
      "grad_norm": 1.263192892074585,
      "learning_rate": 5.723748251748252e-05,
      "loss": 0.725542221069336,
      "step": 1650
    },
    {
      "epoch": 2.3428971308607416,
      "grad_norm": 3.580636739730835,
      "learning_rate": 5.719552447552448e-05,
      "loss": 0.7728473663330078,
      "step": 1675
    },
    {
      "epoch": 2.377886634009797,
      "grad_norm": 0.600355327129364,
      "learning_rate": 5.715356643356643e-05,
      "loss": 0.6960719299316406,
      "step": 1700
    },
    {
      "epoch": 2.4128761371588525,
      "grad_norm": 0.6585419774055481,
      "learning_rate": 5.711160839160839e-05,
      "loss": 0.6674386596679688,
      "step": 1725
    },
    {
      "epoch": 2.4478656403079078,
      "grad_norm": 1.1649935245513916,
      "learning_rate": 5.706965034965035e-05,
      "loss": 0.7121977233886718,
      "step": 1750
    },
    {
      "epoch": 2.482855143456963,
      "grad_norm": 1.3292250633239746,
      "learning_rate": 5.702769230769231e-05,
      "loss": 0.699734115600586,
      "step": 1775
    },
    {
      "epoch": 2.5178446466060183,
      "grad_norm": 0.6181634068489075,
      "learning_rate": 5.698573426573427e-05,
      "loss": 0.6637566375732422,
      "step": 1800
    },
    {
      "epoch": 2.5528341497550735,
      "grad_norm": 2.069950580596924,
      "learning_rate": 5.694377622377623e-05,
      "loss": 0.7059149169921874,
      "step": 1825
    },
    {
      "epoch": 2.5878236529041287,
      "grad_norm": 1.747460126876831,
      "learning_rate": 5.6901818181818184e-05,
      "loss": 0.6831433868408203,
      "step": 1850
    },
    {
      "epoch": 2.622813156053184,
      "grad_norm": 1.368407964706421,
      "learning_rate": 5.685986013986014e-05,
      "loss": 0.6446116638183593,
      "step": 1875
    },
    {
      "epoch": 2.6578026592022392,
      "grad_norm": 0.8423328399658203,
      "learning_rate": 5.6817902097902097e-05,
      "loss": 0.6575716400146484,
      "step": 1900
    },
    {
      "epoch": 2.6927921623512945,
      "grad_norm": 0.8201741576194763,
      "learning_rate": 5.677594405594405e-05,
      "loss": 0.7134691619873047,
      "step": 1925
    },
    {
      "epoch": 2.72778166550035,
      "grad_norm": 0.600385844707489,
      "learning_rate": 5.6733986013986016e-05,
      "loss": 0.6438329315185547,
      "step": 1950
    },
    {
      "epoch": 2.7627711686494054,
      "grad_norm": 1.038720726966858,
      "learning_rate": 5.669202797202797e-05,
      "loss": 0.710013427734375,
      "step": 1975
    },
    {
      "epoch": 2.7977606717984607,
      "grad_norm": 1.5567060708999634,
      "learning_rate": 5.6650069930069935e-05,
      "loss": 0.6388373184204101,
      "step": 2000
    },
    {
      "epoch": 2.832750174947516,
      "grad_norm": 0.7169402837753296,
      "learning_rate": 5.660811188811189e-05,
      "loss": 0.6364174270629883,
      "step": 2025
    },
    {
      "epoch": 2.867739678096571,
      "grad_norm": 1.8279646635055542,
      "learning_rate": 5.656615384615385e-05,
      "loss": 0.683311767578125,
      "step": 2050
    },
    {
      "epoch": 2.9027291812456264,
      "grad_norm": 1.4730579853057861,
      "learning_rate": 5.652419580419581e-05,
      "loss": 0.692446517944336,
      "step": 2075
    },
    {
      "epoch": 2.9377186843946816,
      "grad_norm": 1.399781346321106,
      "learning_rate": 5.648223776223776e-05,
      "loss": 0.6547748565673828,
      "step": 2100
    },
    {
      "epoch": 2.972708187543737,
      "grad_norm": 1.331574559211731,
      "learning_rate": 5.6440279720279717e-05,
      "loss": 0.6984664916992187,
      "step": 2125
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.3240225911140442,
      "eval_mean_accuracy": 0.7625300371226396,
      "eval_mean_iou": 0.6607928066485776,
      "eval_overall_accuracy": 0.8735893562389097,
      "eval_per_category_accuracy": [
        0.903435628346801,
        0.8417786837866702,
        0.511876995890851,
        0.5040267684984632,
        0.8227914715329336,
        0.9912706746801182
      ],
      "eval_per_category_iou": [
        0.7557433469518146,
        0.7093924638583905,
        0.41919171608207345,
        0.41226685296277266,
        0.683974439196307,
        0.9841880208401075
      ],
      "eval_runtime": 31.4763,
      "eval_samples_per_second": 10.071,
      "eval_steps_per_second": 5.051,
      "step": 2145
    },
    {
      "epoch": 3.006997900629811,
      "grad_norm": 0.7641597986221313,
      "learning_rate": 5.639832167832168e-05,
      "loss": 0.6163851165771485,
      "step": 2150
    },
    {
      "epoch": 3.041987403778866,
      "grad_norm": 1.4066956043243408,
      "learning_rate": 5.6356363636363636e-05,
      "loss": 0.6148993301391602,
      "step": 2175
    },
    {
      "epoch": 3.076976906927922,
      "grad_norm": 1.6213104724884033,
      "learning_rate": 5.63144055944056e-05,
      "loss": 0.6618194580078125,
      "step": 2200
    },
    {
      "epoch": 3.111966410076977,
      "grad_norm": 1.1603670120239258,
      "learning_rate": 5.6272447552447555e-05,
      "loss": 0.6659984588623047,
      "step": 2225
    },
    {
      "epoch": 3.1469559132260323,
      "grad_norm": 0.6019548773765564,
      "learning_rate": 5.623048951048951e-05,
      "loss": 0.6827577209472656,
      "step": 2250
    },
    {
      "epoch": 3.1819454163750875,
      "grad_norm": 1.5834856033325195,
      "learning_rate": 5.6188531468531474e-05,
      "loss": 0.7140967559814453,
      "step": 2275
    },
    {
      "epoch": 3.216934919524143,
      "grad_norm": 0.6468294262886047,
      "learning_rate": 5.614657342657343e-05,
      "loss": 0.6410005187988281,
      "step": 2300
    },
    {
      "epoch": 3.251924422673198,
      "grad_norm": 0.9065769910812378,
      "learning_rate": 5.610461538461538e-05,
      "loss": 0.6885283660888671,
      "step": 2325
    },
    {
      "epoch": 3.2869139258222533,
      "grad_norm": 1.260847568511963,
      "learning_rate": 5.606265734265734e-05,
      "loss": 0.6985356140136719,
      "step": 2350
    },
    {
      "epoch": 3.3219034289713085,
      "grad_norm": 0.5449833273887634,
      "learning_rate": 5.60206993006993e-05,
      "loss": 0.6452439117431641,
      "step": 2375
    },
    {
      "epoch": 3.3568929321203638,
      "grad_norm": 2.311706781387329,
      "learning_rate": 5.597874125874126e-05,
      "loss": 0.6420149230957031,
      "step": 2400
    },
    {
      "epoch": 3.391882435269419,
      "grad_norm": 0.5596990585327148,
      "learning_rate": 5.593678321678322e-05,
      "loss": 0.6796179962158203,
      "step": 2425
    },
    {
      "epoch": 3.4268719384184747,
      "grad_norm": 0.67029869556427,
      "learning_rate": 5.5894825174825175e-05,
      "loss": 0.6104664230346679,
      "step": 2450
    },
    {
      "epoch": 3.46186144156753,
      "grad_norm": 1.3907686471939087,
      "learning_rate": 5.585286713286714e-05,
      "loss": 0.6452155303955078,
      "step": 2475
    },
    {
      "epoch": 3.496850944716585,
      "grad_norm": 0.8872125148773193,
      "learning_rate": 5.5810909090909094e-05,
      "loss": 0.65432861328125,
      "step": 2500
    },
    {
      "epoch": 3.5318404478656404,
      "grad_norm": 1.632864236831665,
      "learning_rate": 5.576895104895105e-05,
      "loss": 0.6310539245605469,
      "step": 2525
    },
    {
      "epoch": 3.5668299510146957,
      "grad_norm": 1.6596544981002808,
      "learning_rate": 5.572699300699301e-05,
      "loss": 0.6599636077880859,
      "step": 2550
    },
    {
      "epoch": 3.601819454163751,
      "grad_norm": 2.2133519649505615,
      "learning_rate": 5.568503496503496e-05,
      "loss": 0.7175893402099609,
      "step": 2575
    },
    {
      "epoch": 3.636808957312806,
      "grad_norm": 0.9862116575241089,
      "learning_rate": 5.5643076923076926e-05,
      "loss": 0.7086774444580078,
      "step": 2600
    },
    {
      "epoch": 3.6717984604618614,
      "grad_norm": 3.6606380939483643,
      "learning_rate": 5.560111888111888e-05,
      "loss": 0.6867181396484375,
      "step": 2625
    },
    {
      "epoch": 3.7067879636109167,
      "grad_norm": 2.538208484649658,
      "learning_rate": 5.555916083916084e-05,
      "loss": 0.6881270599365235,
      "step": 2650
    },
    {
      "epoch": 3.741777466759972,
      "grad_norm": 0.9722257256507874,
      "learning_rate": 5.55172027972028e-05,
      "loss": 0.6824803924560547,
      "step": 2675
    },
    {
      "epoch": 3.776766969909027,
      "grad_norm": 0.6339722871780396,
      "learning_rate": 5.547524475524476e-05,
      "loss": 0.6175627517700195,
      "step": 2700
    },
    {
      "epoch": 3.8117564730580824,
      "grad_norm": 0.5318447351455688,
      "learning_rate": 5.5433286713286714e-05,
      "loss": 0.6031735229492188,
      "step": 2725
    },
    {
      "epoch": 3.8467459762071377,
      "grad_norm": 0.6846171617507935,
      "learning_rate": 5.539300699300699e-05,
      "loss": 0.640469741821289,
      "step": 2750
    },
    {
      "epoch": 3.881735479356193,
      "grad_norm": 0.623170018196106,
      "learning_rate": 5.5351048951048956e-05,
      "loss": 0.6481835174560547,
      "step": 2775
    },
    {
      "epoch": 3.9167249825052486,
      "grad_norm": 0.6556963324546814,
      "learning_rate": 5.530909090909091e-05,
      "loss": 0.7382675933837891,
      "step": 2800
    },
    {
      "epoch": 3.951714485654304,
      "grad_norm": 1.0694425106048584,
      "learning_rate": 5.526713286713287e-05,
      "loss": 0.6725161743164062,
      "step": 2825
    },
    {
      "epoch": 3.986703988803359,
      "grad_norm": 1.164157509803772,
      "learning_rate": 5.5225174825174825e-05,
      "loss": 0.6378307342529297,
      "step": 2850
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.3145260512828827,
      "eval_mean_accuracy": 0.7679889092171682,
      "eval_mean_iou": 0.6681629329770868,
      "eval_overall_accuracy": 0.8764119795068205,
      "eval_per_category_accuracy": [
        0.8953374902454897,
        0.8614587813586302,
        0.5036022688088075,
        0.5347041256542012,
        0.8208790782688667,
        0.9919517109670142
      ],
      "eval_per_category_iou": [
        0.7642588735190428,
        0.7149306028814049,
        0.42292540561649256,
        0.43429813089753133,
        0.6879753307919427,
        0.9845892541561062
      ],
      "eval_runtime": 30.9972,
      "eval_samples_per_second": 10.227,
      "eval_steps_per_second": 5.129,
      "step": 2860
    },
    {
      "epoch": 4.0209937018894335,
      "grad_norm": 0.5204030871391296,
      "learning_rate": 5.518321678321678e-05,
      "loss": 0.6750411224365235,
      "step": 2875
    },
    {
      "epoch": 4.055983205038489,
      "grad_norm": 0.5194701552391052,
      "learning_rate": 5.5141258741258744e-05,
      "loss": 0.6078520202636719,
      "step": 2900
    },
    {
      "epoch": 4.090972708187544,
      "grad_norm": 2.2272732257843018,
      "learning_rate": 5.50993006993007e-05,
      "loss": 0.6788324737548828,
      "step": 2925
    },
    {
      "epoch": 4.125962211336599,
      "grad_norm": 1.1706136465072632,
      "learning_rate": 5.505734265734266e-05,
      "loss": 0.6634780883789062,
      "step": 2950
    },
    {
      "epoch": 4.1609517144856545,
      "grad_norm": 1.215396523475647,
      "learning_rate": 5.501538461538462e-05,
      "loss": 0.6289207458496093,
      "step": 2975
    },
    {
      "epoch": 4.19594121763471,
      "grad_norm": 0.9872535467147827,
      "learning_rate": 5.4973426573426576e-05,
      "loss": 0.680127182006836,
      "step": 3000
    },
    {
      "epoch": 4.230930720783765,
      "grad_norm": 0.5762001872062683,
      "learning_rate": 5.493146853146854e-05,
      "loss": 0.6208908843994141,
      "step": 3025
    },
    {
      "epoch": 4.26592022393282,
      "grad_norm": 0.5963050127029419,
      "learning_rate": 5.488951048951049e-05,
      "loss": 0.6116069030761718,
      "step": 3050
    },
    {
      "epoch": 4.3009097270818755,
      "grad_norm": 1.9518903493881226,
      "learning_rate": 5.4847552447552445e-05,
      "loss": 0.6785287475585937,
      "step": 3075
    },
    {
      "epoch": 4.335899230230931,
      "grad_norm": 0.6759105920791626,
      "learning_rate": 5.480559440559441e-05,
      "loss": 0.6256984329223633,
      "step": 3100
    },
    {
      "epoch": 4.370888733379986,
      "grad_norm": 1.826861023902893,
      "learning_rate": 5.4763636363636364e-05,
      "loss": 0.6506615447998046,
      "step": 3125
    },
    {
      "epoch": 4.405878236529041,
      "grad_norm": 1.0569756031036377,
      "learning_rate": 5.472167832167832e-05,
      "loss": 0.6413391876220703,
      "step": 3150
    },
    {
      "epoch": 4.4408677396780964,
      "grad_norm": 0.5070586800575256,
      "learning_rate": 5.4679720279720283e-05,
      "loss": 0.5995641708374023,
      "step": 3175
    },
    {
      "epoch": 4.475857242827152,
      "grad_norm": 0.7818279266357422,
      "learning_rate": 5.463776223776224e-05,
      "loss": 0.6505359649658203,
      "step": 3200
    },
    {
      "epoch": 4.510846745976207,
      "grad_norm": 0.7759609818458557,
      "learning_rate": 5.45958041958042e-05,
      "loss": 0.6247867965698242,
      "step": 3225
    },
    {
      "epoch": 4.545836249125262,
      "grad_norm": 0.4070606231689453,
      "learning_rate": 5.455384615384615e-05,
      "loss": 0.6996167755126953,
      "step": 3250
    },
    {
      "epoch": 4.580825752274317,
      "grad_norm": 1.0086467266082764,
      "learning_rate": 5.451188811188811e-05,
      "loss": 0.6439447021484375,
      "step": 3275
    },
    {
      "epoch": 4.615815255423373,
      "grad_norm": 1.024124026298523,
      "learning_rate": 5.446993006993007e-05,
      "loss": 0.592793083190918,
      "step": 3300
    },
    {
      "epoch": 4.650804758572428,
      "grad_norm": 0.8635032773017883,
      "learning_rate": 5.442797202797203e-05,
      "loss": 0.6623133850097657,
      "step": 3325
    },
    {
      "epoch": 4.685794261721483,
      "grad_norm": 1.6117134094238281,
      "learning_rate": 5.4386013986013984e-05,
      "loss": 0.6395328140258789,
      "step": 3350
    },
    {
      "epoch": 4.720783764870539,
      "grad_norm": 0.9040787220001221,
      "learning_rate": 5.434405594405595e-05,
      "loss": 0.6379338073730468,
      "step": 3375
    },
    {
      "epoch": 4.755773268019594,
      "grad_norm": 0.4202480614185333,
      "learning_rate": 5.4302097902097903e-05,
      "loss": 0.6475178527832032,
      "step": 3400
    },
    {
      "epoch": 4.79076277116865,
      "grad_norm": 0.8303734660148621,
      "learning_rate": 5.4260139860139866e-05,
      "loss": 0.7045294189453125,
      "step": 3425
    },
    {
      "epoch": 4.825752274317705,
      "grad_norm": 0.39480653405189514,
      "learning_rate": 5.421818181818182e-05,
      "loss": 0.6235169219970703,
      "step": 3450
    },
    {
      "epoch": 4.86074177746676,
      "grad_norm": 1.5808161497116089,
      "learning_rate": 5.417622377622377e-05,
      "loss": 0.6447249603271484,
      "step": 3475
    },
    {
      "epoch": 4.8957312806158155,
      "grad_norm": 1.4757431745529175,
      "learning_rate": 5.4134265734265735e-05,
      "loss": 0.5740582275390625,
      "step": 3500
    },
    {
      "epoch": 4.930720783764871,
      "grad_norm": 0.5482572913169861,
      "learning_rate": 5.409230769230769e-05,
      "loss": 0.6319955444335937,
      "step": 3525
    },
    {
      "epoch": 4.965710286913926,
      "grad_norm": 0.5650365948677063,
      "learning_rate": 5.405034965034965e-05,
      "loss": 0.6582250213623047,
      "step": 3550
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.5496521592140198,
      "learning_rate": 5.400839160839161e-05,
      "loss": 0.652398681640625,
      "step": 3575
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.3088545501232147,
      "eval_mean_accuracy": 0.7712799693296143,
      "eval_mean_iou": 0.6743028807431585,
      "eval_overall_accuracy": 0.8782652845894124,
      "eval_per_category_accuracy": [
        0.9083662581000177,
        0.839762187190995,
        0.5535258126734505,
        0.5048452012690662,
        0.8293227356877012,
        0.991857621056455
      ],
      "eval_per_category_iou": [
        0.7673468762305439,
        0.7148496569536149,
        0.44801303000331993,
        0.4385842740345797,
        0.6922184237756559,
        0.9848050234612374
      ],
      "eval_runtime": 30.4559,
      "eval_samples_per_second": 10.408,
      "eval_steps_per_second": 5.221,
      "step": 3575
    },
    {
      "epoch": 5.034989503149055,
      "grad_norm": 0.6785487532615662,
      "learning_rate": 5.396643356643357e-05,
      "loss": 0.6673529052734375,
      "step": 3600
    },
    {
      "epoch": 5.0699790062981105,
      "grad_norm": 1.3622685670852661,
      "learning_rate": 5.392447552447553e-05,
      "loss": 0.6140386581420898,
      "step": 3625
    },
    {
      "epoch": 5.104968509447166,
      "grad_norm": 0.6366313099861145,
      "learning_rate": 5.3882517482517486e-05,
      "loss": 0.5859353256225586,
      "step": 3650
    },
    {
      "epoch": 5.139958012596221,
      "grad_norm": 0.5496285557746887,
      "learning_rate": 5.384055944055944e-05,
      "loss": 0.6472185516357422,
      "step": 3675
    },
    {
      "epoch": 5.174947515745276,
      "grad_norm": 0.5543274879455566,
      "learning_rate": 5.37986013986014e-05,
      "loss": 0.6121658325195313,
      "step": 3700
    },
    {
      "epoch": 5.2099370188943315,
      "grad_norm": 1.3909692764282227,
      "learning_rate": 5.3756643356643355e-05,
      "loss": 0.6275715637207031,
      "step": 3725
    },
    {
      "epoch": 5.244926522043387,
      "grad_norm": 0.511917233467102,
      "learning_rate": 5.371468531468531e-05,
      "loss": 0.6234700012207032,
      "step": 3750
    },
    {
      "epoch": 5.279916025192442,
      "grad_norm": 0.5754797458648682,
      "learning_rate": 5.3672727272727274e-05,
      "loss": 0.5976237869262695,
      "step": 3775
    },
    {
      "epoch": 5.314905528341497,
      "grad_norm": 0.7853637337684631,
      "learning_rate": 5.363076923076923e-05,
      "loss": 0.6225475311279297,
      "step": 3800
    },
    {
      "epoch": 5.3498950314905525,
      "grad_norm": 0.7983298301696777,
      "learning_rate": 5.3588811188811194e-05,
      "loss": 0.6161379241943359,
      "step": 3825
    },
    {
      "epoch": 5.384884534639609,
      "grad_norm": 0.6273944973945618,
      "learning_rate": 5.354685314685315e-05,
      "loss": 0.6023226928710937,
      "step": 3850
    },
    {
      "epoch": 5.419874037788663,
      "grad_norm": 0.5273428559303284,
      "learning_rate": 5.3504895104895106e-05,
      "loss": 0.6390038681030273,
      "step": 3875
    },
    {
      "epoch": 5.454863540937719,
      "grad_norm": 2.432908535003662,
      "learning_rate": 5.346293706293707e-05,
      "loss": 0.6143883895874024,
      "step": 3900
    },
    {
      "epoch": 5.489853044086774,
      "grad_norm": 0.5991243720054626,
      "learning_rate": 5.342097902097902e-05,
      "loss": 0.6548857879638672,
      "step": 3925
    },
    {
      "epoch": 5.52484254723583,
      "grad_norm": 0.4502994120121002,
      "learning_rate": 5.3379020979020975e-05,
      "loss": 0.6577656555175782,
      "step": 3950
    },
    {
      "epoch": 5.559832050384885,
      "grad_norm": 0.7342044711112976,
      "learning_rate": 5.333706293706294e-05,
      "loss": 0.6413287353515625,
      "step": 3975
    },
    {
      "epoch": 5.59482155353394,
      "grad_norm": 1.2729270458221436,
      "learning_rate": 5.3295104895104894e-05,
      "loss": 0.6373369979858399,
      "step": 4000
    },
    {
      "epoch": 5.629811056682995,
      "grad_norm": 0.8291492462158203,
      "learning_rate": 5.325314685314686e-05,
      "loss": 0.6362504577636718,
      "step": 4025
    },
    {
      "epoch": 5.6648005598320506,
      "grad_norm": 0.8355749845504761,
      "learning_rate": 5.3211188811188814e-05,
      "loss": 0.6254707717895508,
      "step": 4050
    },
    {
      "epoch": 5.699790062981106,
      "grad_norm": 1.8568404912948608,
      "learning_rate": 5.316923076923077e-05,
      "loss": 0.6572555541992188,
      "step": 4075
    },
    {
      "epoch": 5.734779566130161,
      "grad_norm": 1.1327359676361084,
      "learning_rate": 5.312727272727273e-05,
      "loss": 0.6366152954101563,
      "step": 4100
    },
    {
      "epoch": 5.769769069279216,
      "grad_norm": 0.39386245608329773,
      "learning_rate": 5.308531468531469e-05,
      "loss": 0.6708284759521485,
      "step": 4125
    },
    {
      "epoch": 5.8047585724282715,
      "grad_norm": 0.7092975974082947,
      "learning_rate": 5.304335664335664e-05,
      "loss": 0.6563416290283203,
      "step": 4150
    },
    {
      "epoch": 5.839748075577327,
      "grad_norm": 0.6432331204414368,
      "learning_rate": 5.30013986013986e-05,
      "loss": 0.6034230804443359,
      "step": 4175
    },
    {
      "epoch": 5.874737578726382,
      "grad_norm": 0.3659704029560089,
      "learning_rate": 5.295944055944056e-05,
      "loss": 0.6028037261962891,
      "step": 4200
    },
    {
      "epoch": 5.909727081875437,
      "grad_norm": 0.8267989158630371,
      "learning_rate": 5.291748251748252e-05,
      "loss": 0.6116258239746094,
      "step": 4225
    },
    {
      "epoch": 5.9447165850244925,
      "grad_norm": 1.0733015537261963,
      "learning_rate": 5.287552447552448e-05,
      "loss": 0.63281982421875,
      "step": 4250
    },
    {
      "epoch": 5.979706088173548,
      "grad_norm": 2.1807644367218018,
      "learning_rate": 5.2833566433566434e-05,
      "loss": 0.6506564331054687,
      "step": 4275
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.3030133545398712,
      "eval_mean_accuracy": 0.7803491177620083,
      "eval_mean_iou": 0.6810148967316563,
      "eval_overall_accuracy": 0.8799868562620144,
      "eval_per_category_accuracy": [
        0.8983125040202485,
        0.8674953161973128,
        0.5632030540310857,
        0.5492833263644344,
        0.8118878078816986,
        0.9919126980772701
      ],
      "eval_per_category_iou": [
        0.7723064178608936,
        0.7197107954985145,
        0.45656336114873297,
        0.459661979351935,
        0.6930392581426378,
        0.9848075683872235
      ],
      "eval_runtime": 30.2538,
      "eval_samples_per_second": 10.478,
      "eval_steps_per_second": 5.256,
      "step": 4290
    },
    {
      "epoch": 6.013995801259622,
      "grad_norm": 0.5249394774436951,
      "learning_rate": 5.27916083916084e-05,
      "loss": 0.600844612121582,
      "step": 4300
    },
    {
      "epoch": 6.048985304408677,
      "grad_norm": 0.877570629119873,
      "learning_rate": 5.274965034965035e-05,
      "loss": 0.6449540710449219,
      "step": 4325
    },
    {
      "epoch": 6.083974807557732,
      "grad_norm": 0.882365882396698,
      "learning_rate": 5.270769230769231e-05,
      "loss": 0.602541389465332,
      "step": 4350
    },
    {
      "epoch": 6.118964310706788,
      "grad_norm": 1.1386775970458984,
      "learning_rate": 5.2665734265734265e-05,
      "loss": 0.601654167175293,
      "step": 4375
    },
    {
      "epoch": 6.153953813855844,
      "grad_norm": 0.49724966287612915,
      "learning_rate": 5.262377622377622e-05,
      "loss": 0.6363157653808593,
      "step": 4400
    },
    {
      "epoch": 6.188943317004899,
      "grad_norm": 2.1240525245666504,
      "learning_rate": 5.2581818181818185e-05,
      "loss": 0.6443531036376953,
      "step": 4425
    },
    {
      "epoch": 6.223932820153954,
      "grad_norm": 0.4248741567134857,
      "learning_rate": 5.253986013986014e-05,
      "loss": 0.6614412689208984,
      "step": 4450
    },
    {
      "epoch": 6.258922323303009,
      "grad_norm": 0.6942499876022339,
      "learning_rate": 5.24979020979021e-05,
      "loss": 0.6467098999023437,
      "step": 4475
    },
    {
      "epoch": 6.293911826452065,
      "grad_norm": 0.6314564347267151,
      "learning_rate": 5.245594405594406e-05,
      "loss": 0.6205204391479492,
      "step": 4500
    },
    {
      "epoch": 6.32890132960112,
      "grad_norm": 1.1796150207519531,
      "learning_rate": 5.2413986013986017e-05,
      "loss": 0.6629433441162109,
      "step": 4525
    },
    {
      "epoch": 6.363890832750175,
      "grad_norm": 0.6724736094474792,
      "learning_rate": 5.237202797202797e-05,
      "loss": 0.566428451538086,
      "step": 4550
    },
    {
      "epoch": 6.39888033589923,
      "grad_norm": 0.7810184955596924,
      "learning_rate": 5.233006993006993e-05,
      "loss": 0.6403610229492187,
      "step": 4575
    },
    {
      "epoch": 6.433869839048286,
      "grad_norm": 0.6313885450363159,
      "learning_rate": 5.2288111888111885e-05,
      "loss": 0.6293313598632813,
      "step": 4600
    },
    {
      "epoch": 6.468859342197341,
      "grad_norm": 0.7936954498291016,
      "learning_rate": 5.224615384615385e-05,
      "loss": 0.619962272644043,
      "step": 4625
    },
    {
      "epoch": 6.503848845346396,
      "grad_norm": 0.8459281325340271,
      "learning_rate": 5.2204195804195805e-05,
      "loss": 0.6425261688232422,
      "step": 4650
    },
    {
      "epoch": 6.538838348495451,
      "grad_norm": 0.6662110090255737,
      "learning_rate": 5.216223776223776e-05,
      "loss": 0.5873116683959961,
      "step": 4675
    },
    {
      "epoch": 6.573827851644507,
      "grad_norm": 0.6616698503494263,
      "learning_rate": 5.2120279720279724e-05,
      "loss": 0.5901433944702148,
      "step": 4700
    },
    {
      "epoch": 6.608817354793562,
      "grad_norm": 0.559081494808197,
      "learning_rate": 5.207832167832168e-05,
      "loss": 0.6307137298583985,
      "step": 4725
    },
    {
      "epoch": 6.643806857942617,
      "grad_norm": 0.5878550410270691,
      "learning_rate": 5.2036363636363637e-05,
      "loss": 0.6451882171630859,
      "step": 4750
    },
    {
      "epoch": 6.678796361091672,
      "grad_norm": 0.44484755396842957,
      "learning_rate": 5.19944055944056e-05,
      "loss": 0.6084480285644531,
      "step": 4775
    },
    {
      "epoch": 6.7137858642407275,
      "grad_norm": 0.9550893306732178,
      "learning_rate": 5.195244755244755e-05,
      "loss": 0.6021532821655273,
      "step": 4800
    },
    {
      "epoch": 6.748775367389783,
      "grad_norm": 1.741071343421936,
      "learning_rate": 5.191048951048951e-05,
      "loss": 0.6223680114746094,
      "step": 4825
    },
    {
      "epoch": 6.783764870538838,
      "grad_norm": 0.9019598960876465,
      "learning_rate": 5.186853146853147e-05,
      "loss": 0.6215939331054687,
      "step": 4850
    },
    {
      "epoch": 6.818754373687893,
      "grad_norm": 0.6781762838363647,
      "learning_rate": 5.1826573426573425e-05,
      "loss": 0.5813679122924804,
      "step": 4875
    },
    {
      "epoch": 6.853743876836949,
      "grad_norm": 0.475730299949646,
      "learning_rate": 5.178461538461539e-05,
      "loss": 0.5839945220947266,
      "step": 4900
    },
    {
      "epoch": 6.888733379986004,
      "grad_norm": 0.6640080809593201,
      "learning_rate": 5.1742657342657344e-05,
      "loss": 0.5786778259277344,
      "step": 4925
    },
    {
      "epoch": 6.92372288313506,
      "grad_norm": 0.8566504716873169,
      "learning_rate": 5.17006993006993e-05,
      "loss": 0.6280381393432617,
      "step": 4950
    },
    {
      "epoch": 6.958712386284115,
      "grad_norm": 0.4262753129005432,
      "learning_rate": 5.165874125874126e-05,
      "loss": 0.6575125885009766,
      "step": 4975
    },
    {
      "epoch": 6.99370188943317,
      "grad_norm": 0.4589586853981018,
      "learning_rate": 5.161678321678322e-05,
      "loss": 0.5859783172607422,
      "step": 5000
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.299780935049057,
      "eval_mean_accuracy": 0.7769845585383753,
      "eval_mean_iou": 0.6819513496884336,
      "eval_overall_accuracy": 0.8806330804193058,
      "eval_per_category_accuracy": [
        0.8954717471549768,
        0.8422604900927622,
        0.5400469254776343,
        0.5479734375959875,
        0.8444242086035879,
        0.9917305423053036
      ],
      "eval_per_category_iou": [
        0.7756367172713378,
        0.7169434600795639,
        0.4489216390498614,
        0.46679062873753663,
        0.6984636270991806,
        0.9849520258931213
      ],
      "eval_runtime": 28.3918,
      "eval_samples_per_second": 11.165,
      "eval_steps_per_second": 5.6,
      "step": 5005
    },
    {
      "epoch": 7.027991602519244,
      "grad_norm": 0.5106598138809204,
      "learning_rate": 5.1574825174825176e-05,
      "loss": 0.6315861511230468,
      "step": 5025
    },
    {
      "epoch": 7.0629811056683,
      "grad_norm": 3.8924002647399902,
      "learning_rate": 5.153286713286713e-05,
      "loss": 0.5792488861083984,
      "step": 5050
    },
    {
      "epoch": 7.097970608817355,
      "grad_norm": 2.370727300643921,
      "learning_rate": 5.149090909090909e-05,
      "loss": 0.5892275619506836,
      "step": 5075
    },
    {
      "epoch": 7.13296011196641,
      "grad_norm": 0.5767578482627869,
      "learning_rate": 5.144895104895105e-05,
      "loss": 0.6520233917236328,
      "step": 5100
    },
    {
      "epoch": 7.167949615115465,
      "grad_norm": 1.30486261844635,
      "learning_rate": 5.140699300699301e-05,
      "loss": 0.6055976104736328,
      "step": 5125
    },
    {
      "epoch": 7.202939118264521,
      "grad_norm": 0.38247647881507874,
      "learning_rate": 5.1365034965034964e-05,
      "loss": 0.5990764999389648,
      "step": 5150
    },
    {
      "epoch": 7.237928621413576,
      "grad_norm": 0.5510661005973816,
      "learning_rate": 5.132307692307693e-05,
      "loss": 0.5942102813720703,
      "step": 5175
    },
    {
      "epoch": 7.272918124562631,
      "grad_norm": 1.7462855577468872,
      "learning_rate": 5.128111888111888e-05,
      "loss": 0.6240926361083985,
      "step": 5200
    },
    {
      "epoch": 7.307907627711686,
      "grad_norm": 0.519395649433136,
      "learning_rate": 5.1239160839160846e-05,
      "loss": 0.598558578491211,
      "step": 5225
    },
    {
      "epoch": 7.342897130860742,
      "grad_norm": 0.41511696577072144,
      "learning_rate": 5.1197202797202796e-05,
      "loss": 0.6070619201660157,
      "step": 5250
    },
    {
      "epoch": 7.377886634009797,
      "grad_norm": 1.1701627969741821,
      "learning_rate": 5.115524475524475e-05,
      "loss": 0.62059814453125,
      "step": 5275
    },
    {
      "epoch": 7.412876137158852,
      "grad_norm": 0.8231493830680847,
      "learning_rate": 5.1113286713286715e-05,
      "loss": 0.6369314956665039,
      "step": 5300
    },
    {
      "epoch": 7.447865640307907,
      "grad_norm": 5.053681373596191,
      "learning_rate": 5.107132867132867e-05,
      "loss": 0.6031607818603516,
      "step": 5325
    },
    {
      "epoch": 7.482855143456963,
      "grad_norm": 0.5182908773422241,
      "learning_rate": 5.102937062937063e-05,
      "loss": 0.6481076812744141,
      "step": 5350
    },
    {
      "epoch": 7.517844646606018,
      "grad_norm": 0.7278526425361633,
      "learning_rate": 5.098741258741259e-05,
      "loss": 0.5932178497314453,
      "step": 5375
    },
    {
      "epoch": 7.552834149755073,
      "grad_norm": 0.95259028673172,
      "learning_rate": 5.094545454545455e-05,
      "loss": 0.5961886596679687,
      "step": 5400
    },
    {
      "epoch": 7.587823652904129,
      "grad_norm": 0.7674578428268433,
      "learning_rate": 5.090349650349651e-05,
      "loss": 0.6137759399414062,
      "step": 5425
    },
    {
      "epoch": 7.6228131560531835,
      "grad_norm": 1.1081340312957764,
      "learning_rate": 5.0861538461538466e-05,
      "loss": 0.5849206161499023,
      "step": 5450
    },
    {
      "epoch": 7.65780265920224,
      "grad_norm": 1.7009389400482178,
      "learning_rate": 5.0819580419580416e-05,
      "loss": 0.6230030059814453,
      "step": 5475
    },
    {
      "epoch": 7.692792162351295,
      "grad_norm": 1.0026092529296875,
      "learning_rate": 5.077762237762238e-05,
      "loss": 0.6246591186523438,
      "step": 5500
    },
    {
      "epoch": 7.72778166550035,
      "grad_norm": 0.368832528591156,
      "learning_rate": 5.0735664335664335e-05,
      "loss": 0.6104314804077149,
      "step": 5525
    },
    {
      "epoch": 7.762771168649405,
      "grad_norm": 0.3699518144130707,
      "learning_rate": 5.069370629370629e-05,
      "loss": 0.6022538757324218,
      "step": 5550
    },
    {
      "epoch": 7.797760671798461,
      "grad_norm": 0.6638069748878479,
      "learning_rate": 5.0651748251748254e-05,
      "loss": 0.6106375122070312,
      "step": 5575
    },
    {
      "epoch": 7.832750174947516,
      "grad_norm": 0.8862537145614624,
      "learning_rate": 5.060979020979021e-05,
      "loss": 0.600302848815918,
      "step": 5600
    },
    {
      "epoch": 7.867739678096571,
      "grad_norm": 0.8819276094436646,
      "learning_rate": 5.0567832167832174e-05,
      "loss": 0.5927967453002929,
      "step": 5625
    },
    {
      "epoch": 7.902729181245626,
      "grad_norm": 0.9091945290565491,
      "learning_rate": 5.052587412587413e-05,
      "loss": 0.6199308013916016,
      "step": 5650
    },
    {
      "epoch": 7.937718684394682,
      "grad_norm": 0.9733881950378418,
      "learning_rate": 5.0483916083916086e-05,
      "loss": 0.6351939392089844,
      "step": 5675
    },
    {
      "epoch": 7.972708187543737,
      "grad_norm": 3.5849504470825195,
      "learning_rate": 5.044195804195804e-05,
      "loss": 0.6121944046020508,
      "step": 5700
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.2977438271045685,
      "eval_mean_accuracy": 0.777619116122095,
      "eval_mean_iou": 0.6820383125913931,
      "eval_overall_accuracy": 0.8812491720783222,
      "eval_per_category_accuracy": [
        0.902352861289222,
        0.8549876080030488,
        0.5091462538100834,
        0.5665070755791948,
        0.8410266594070067,
        0.991694238644014
      ],
      "eval_per_category_iou": [
        0.7763550562134621,
        0.7208152335259898,
        0.4372283989674062,
        0.473885313363807,
        0.6990110712794235,
        0.9849348021982705
      ],
      "eval_runtime": 29.8917,
      "eval_samples_per_second": 10.605,
      "eval_steps_per_second": 5.319,
      "step": 5720
    },
    {
      "epoch": 8.00699790062981,
      "grad_norm": 0.942154049873352,
      "learning_rate": 5.040167832167833e-05,
      "loss": 0.62352294921875,
      "step": 5725
    },
    {
      "epoch": 8.041987403778867,
      "grad_norm": 1.4186257123947144,
      "learning_rate": 5.035972027972028e-05,
      "loss": 0.6252261352539062,
      "step": 5750
    },
    {
      "epoch": 8.076976906927921,
      "grad_norm": 0.8338136672973633,
      "learning_rate": 5.031776223776224e-05,
      "loss": 0.6279706954956055,
      "step": 5775
    },
    {
      "epoch": 8.111966410076977,
      "grad_norm": 1.1844972372055054,
      "learning_rate": 5.02758041958042e-05,
      "loss": 0.5759858322143555,
      "step": 5800
    },
    {
      "epoch": 8.146955913226032,
      "grad_norm": 1.451628565788269,
      "learning_rate": 5.023384615384615e-05,
      "loss": 0.6156016159057617,
      "step": 5825
    },
    {
      "epoch": 8.181945416375088,
      "grad_norm": 2.0767338275909424,
      "learning_rate": 5.0191888111888116e-05,
      "loss": 0.5996998977661133,
      "step": 5850
    },
    {
      "epoch": 8.216934919524142,
      "grad_norm": 0.4673709571361542,
      "learning_rate": 5.014993006993007e-05,
      "loss": 0.5743143463134766,
      "step": 5875
    },
    {
      "epoch": 8.251924422673198,
      "grad_norm": 0.8611520528793335,
      "learning_rate": 5.010797202797203e-05,
      "loss": 0.6271451187133789,
      "step": 5900
    },
    {
      "epoch": 8.286913925822253,
      "grad_norm": 1.6908881664276123,
      "learning_rate": 5.006601398601399e-05,
      "loss": 0.6388996887207031,
      "step": 5925
    },
    {
      "epoch": 8.321903428971309,
      "grad_norm": 0.738675057888031,
      "learning_rate": 5.002405594405595e-05,
      "loss": 0.6676346588134766,
      "step": 5950
    },
    {
      "epoch": 8.356892932120363,
      "grad_norm": 1.01461923122406,
      "learning_rate": 4.9982097902097904e-05,
      "loss": 0.6496328735351562,
      "step": 5975
    },
    {
      "epoch": 8.39188243526942,
      "grad_norm": 1.231520175933838,
      "learning_rate": 4.994013986013986e-05,
      "loss": 0.5777140045166016,
      "step": 6000
    },
    {
      "epoch": 8.426871938418474,
      "grad_norm": 0.8060991168022156,
      "learning_rate": 4.989818181818182e-05,
      "loss": 0.588763771057129,
      "step": 6025
    },
    {
      "epoch": 8.46186144156753,
      "grad_norm": 0.477630078792572,
      "learning_rate": 4.985622377622378e-05,
      "loss": 0.6198821640014649,
      "step": 6050
    },
    {
      "epoch": 8.496850944716584,
      "grad_norm": 1.470032811164856,
      "learning_rate": 4.9814265734265736e-05,
      "loss": 0.6288646697998047,
      "step": 6075
    },
    {
      "epoch": 8.53184044786564,
      "grad_norm": 1.0536527633666992,
      "learning_rate": 4.977230769230769e-05,
      "loss": 0.5791899490356446,
      "step": 6100
    },
    {
      "epoch": 8.566829951014695,
      "grad_norm": 0.6307972073554993,
      "learning_rate": 4.9730349650349655e-05,
      "loss": 0.603074951171875,
      "step": 6125
    },
    {
      "epoch": 8.601819454163751,
      "grad_norm": 0.9494679570198059,
      "learning_rate": 4.968839160839161e-05,
      "loss": 0.5668829727172852,
      "step": 6150
    },
    {
      "epoch": 8.636808957312805,
      "grad_norm": 0.6116297841072083,
      "learning_rate": 4.964643356643357e-05,
      "loss": 0.5815203094482422,
      "step": 6175
    },
    {
      "epoch": 8.671798460461861,
      "grad_norm": 0.8381626009941101,
      "learning_rate": 4.9604475524475524e-05,
      "loss": 0.6049332427978515,
      "step": 6200
    },
    {
      "epoch": 8.706787963610918,
      "grad_norm": 0.973737359046936,
      "learning_rate": 4.956251748251748e-05,
      "loss": 0.5773615264892578,
      "step": 6225
    },
    {
      "epoch": 8.741777466759972,
      "grad_norm": 0.877646803855896,
      "learning_rate": 4.952055944055944e-05,
      "loss": 0.615462760925293,
      "step": 6250
    },
    {
      "epoch": 8.776766969909028,
      "grad_norm": 0.4504320025444031,
      "learning_rate": 4.94786013986014e-05,
      "loss": 0.5906855010986328,
      "step": 6275
    },
    {
      "epoch": 8.811756473058082,
      "grad_norm": 1.507692813873291,
      "learning_rate": 4.9436643356643356e-05,
      "loss": 0.6040005111694335,
      "step": 6300
    },
    {
      "epoch": 8.846745976207139,
      "grad_norm": 0.7510802745819092,
      "learning_rate": 4.939468531468532e-05,
      "loss": 0.6163809204101562,
      "step": 6325
    },
    {
      "epoch": 8.881735479356193,
      "grad_norm": 0.8514718413352966,
      "learning_rate": 4.9352727272727275e-05,
      "loss": 0.5949136352539063,
      "step": 6350
    },
    {
      "epoch": 8.916724982505249,
      "grad_norm": 0.6325560808181763,
      "learning_rate": 4.931076923076924e-05,
      "loss": 0.6096785354614258,
      "step": 6375
    },
    {
      "epoch": 8.951714485654303,
      "grad_norm": 0.34004655480384827,
      "learning_rate": 4.926881118881119e-05,
      "loss": 0.5791630554199219,
      "step": 6400
    },
    {
      "epoch": 8.98670398880336,
      "grad_norm": 0.5574741959571838,
      "learning_rate": 4.9226853146853144e-05,
      "loss": 0.6281651306152344,
      "step": 6425
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.2939758896827698,
      "eval_mean_accuracy": 0.7808693556057832,
      "eval_mean_iou": 0.6859442747075835,
      "eval_overall_accuracy": 0.8822356142832277,
      "eval_per_category_accuracy": [
        0.9111269381774977,
        0.8586814170923348,
        0.5393447754736075,
        0.5538283035057855,
        0.8308363403619612,
        0.9913983590235125
      ],
      "eval_per_category_iou": [
        0.7774094196608266,
        0.7224379788415114,
        0.45297560413744614,
        0.4784113240620913,
        0.6992849224629323,
        0.9851463990806931
      ],
      "eval_runtime": 29.4766,
      "eval_samples_per_second": 10.754,
      "eval_steps_per_second": 5.394,
      "step": 6435
    },
    {
      "epoch": 9.020993701889433,
      "grad_norm": 0.6460337042808533,
      "learning_rate": 4.918489510489511e-05,
      "loss": 0.6026431655883789,
      "step": 6450
    },
    {
      "epoch": 9.055983205038489,
      "grad_norm": 0.5324429869651794,
      "learning_rate": 4.914293706293706e-05,
      "loss": 0.6127277755737305,
      "step": 6475
    },
    {
      "epoch": 9.090972708187543,
      "grad_norm": 0.2976040542125702,
      "learning_rate": 4.910097902097902e-05,
      "loss": 0.592356071472168,
      "step": 6500
    },
    {
      "epoch": 9.1259622113366,
      "grad_norm": 0.40423324704170227,
      "learning_rate": 4.905902097902098e-05,
      "loss": 0.6028085708618164,
      "step": 6525
    },
    {
      "epoch": 9.160951714485654,
      "grad_norm": 4.967430591583252,
      "learning_rate": 4.901706293706294e-05,
      "loss": 0.6081914138793946,
      "step": 6550
    },
    {
      "epoch": 9.19594121763471,
      "grad_norm": 0.6055846810340881,
      "learning_rate": 4.89751048951049e-05,
      "loss": 0.588531494140625,
      "step": 6575
    },
    {
      "epoch": 9.230930720783764,
      "grad_norm": 0.4030820429325104,
      "learning_rate": 4.893314685314686e-05,
      "loss": 0.6082214736938476,
      "step": 6600
    },
    {
      "epoch": 9.26592022393282,
      "grad_norm": 1.072285532951355,
      "learning_rate": 4.889118881118881e-05,
      "loss": 0.6194109344482421,
      "step": 6625
    },
    {
      "epoch": 9.300909727081875,
      "grad_norm": 0.7063739895820618,
      "learning_rate": 4.884923076923077e-05,
      "loss": 0.5926945495605469,
      "step": 6650
    },
    {
      "epoch": 9.33589923023093,
      "grad_norm": 0.9995561242103577,
      "learning_rate": 4.880727272727273e-05,
      "loss": 0.5791917419433594,
      "step": 6675
    },
    {
      "epoch": 9.370888733379987,
      "grad_norm": 0.6222540140151978,
      "learning_rate": 4.876531468531468e-05,
      "loss": 0.5992728805541992,
      "step": 6700
    },
    {
      "epoch": 9.405878236529041,
      "grad_norm": 0.3672638535499573,
      "learning_rate": 4.8723356643356646e-05,
      "loss": 0.6290166091918945,
      "step": 6725
    },
    {
      "epoch": 9.440867739678097,
      "grad_norm": 0.37589702010154724,
      "learning_rate": 4.86813986013986e-05,
      "loss": 0.6531430816650391,
      "step": 6750
    },
    {
      "epoch": 9.475857242827152,
      "grad_norm": 0.4815174639225006,
      "learning_rate": 4.8639440559440566e-05,
      "loss": 0.6107879638671875,
      "step": 6775
    },
    {
      "epoch": 9.510846745976208,
      "grad_norm": 0.284898966550827,
      "learning_rate": 4.859748251748252e-05,
      "loss": 0.5798380279541016,
      "step": 6800
    },
    {
      "epoch": 9.545836249125262,
      "grad_norm": 0.3997577726840973,
      "learning_rate": 4.855552447552448e-05,
      "loss": 0.6196766662597656,
      "step": 6825
    },
    {
      "epoch": 9.580825752274318,
      "grad_norm": 1.64169442653656,
      "learning_rate": 4.8513566433566434e-05,
      "loss": 0.5816829299926758,
      "step": 6850
    },
    {
      "epoch": 9.615815255423373,
      "grad_norm": 0.8375020623207092,
      "learning_rate": 4.847160839160839e-05,
      "loss": 0.6264281463623047,
      "step": 6875
    },
    {
      "epoch": 9.650804758572429,
      "grad_norm": 0.7736727595329285,
      "learning_rate": 4.842965034965035e-05,
      "loss": 0.5981005859375,
      "step": 6900
    },
    {
      "epoch": 9.685794261721483,
      "grad_norm": 1.0497031211853027,
      "learning_rate": 4.838769230769231e-05,
      "loss": 0.6049394989013672,
      "step": 6925
    },
    {
      "epoch": 9.72078376487054,
      "grad_norm": 0.6530268788337708,
      "learning_rate": 4.8345734265734266e-05,
      "loss": 0.6093789291381836,
      "step": 6950
    },
    {
      "epoch": 9.755773268019594,
      "grad_norm": 0.6453509330749512,
      "learning_rate": 4.830377622377623e-05,
      "loss": 0.576314811706543,
      "step": 6975
    },
    {
      "epoch": 9.79076277116865,
      "grad_norm": 0.5657827258110046,
      "learning_rate": 4.8261818181818185e-05,
      "loss": 0.5951481628417968,
      "step": 7000
    },
    {
      "epoch": 9.825752274317704,
      "grad_norm": 0.38380274176597595,
      "learning_rate": 4.821986013986014e-05,
      "loss": 0.6279211807250976,
      "step": 7025
    },
    {
      "epoch": 9.86074177746676,
      "grad_norm": 0.5571205019950867,
      "learning_rate": 4.8177902097902105e-05,
      "loss": 0.5736864471435547,
      "step": 7050
    },
    {
      "epoch": 9.895731280615815,
      "grad_norm": 0.9587305188179016,
      "learning_rate": 4.8135944055944054e-05,
      "loss": 0.5887752532958984,
      "step": 7075
    },
    {
      "epoch": 9.93072078376487,
      "grad_norm": 0.4591243267059326,
      "learning_rate": 4.809398601398601e-05,
      "loss": 0.5746500396728516,
      "step": 7100
    },
    {
      "epoch": 9.965710286913925,
      "grad_norm": 0.7744358777999878,
      "learning_rate": 4.8052027972027974e-05,
      "loss": 0.5539785766601563,
      "step": 7125
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.3336397707462311,
      "learning_rate": 4.801006993006993e-05,
      "loss": 0.5649756622314454,
      "step": 7150
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.29158568382263184,
      "eval_mean_accuracy": 0.7862688674573919,
      "eval_mean_iou": 0.6898428316768203,
      "eval_overall_accuracy": 0.8831566169810972,
      "eval_per_category_accuracy": [
        0.9022612085723455,
        0.863676668514906,
        0.5507234627209598,
        0.5799913995200378,
        0.8294723193742816,
        0.9914881460418205
      ],
      "eval_per_category_iou": [
        0.7806707537186997,
        0.7239197134862514,
        0.46033247909672725,
        0.48842873628533023,
        0.7004731824991001,
        0.9852321249748127
      ],
      "eval_runtime": 29.4721,
      "eval_samples_per_second": 10.756,
      "eval_steps_per_second": 5.395,
      "step": 7150
    },
    {
      "epoch": 10.034989503149056,
      "grad_norm": 0.47368690371513367,
      "learning_rate": 4.796811188811189e-05,
      "loss": 0.5725191116333008,
      "step": 7175
    },
    {
      "epoch": 10.06997900629811,
      "grad_norm": 1.2438640594482422,
      "learning_rate": 4.792615384615385e-05,
      "loss": 0.6011537170410156,
      "step": 7200
    },
    {
      "epoch": 10.104968509447167,
      "grad_norm": 0.5422938466072083,
      "learning_rate": 4.7884195804195805e-05,
      "loss": 0.5894782257080078,
      "step": 7225
    },
    {
      "epoch": 10.139958012596221,
      "grad_norm": 0.41158774495124817,
      "learning_rate": 4.784223776223777e-05,
      "loss": 0.5704452896118164,
      "step": 7250
    },
    {
      "epoch": 10.174947515745277,
      "grad_norm": 6.7909111976623535,
      "learning_rate": 4.7800279720279725e-05,
      "loss": 0.6007511520385742,
      "step": 7275
    },
    {
      "epoch": 10.209937018894331,
      "grad_norm": 0.702378511428833,
      "learning_rate": 4.7758321678321674e-05,
      "loss": 0.5628812408447266,
      "step": 7300
    },
    {
      "epoch": 10.244926522043388,
      "grad_norm": 0.4181881546974182,
      "learning_rate": 4.771636363636364e-05,
      "loss": 0.5773606872558594,
      "step": 7325
    },
    {
      "epoch": 10.279916025192442,
      "grad_norm": 2.075099229812622,
      "learning_rate": 4.7674405594405593e-05,
      "loss": 0.5681010437011719,
      "step": 7350
    },
    {
      "epoch": 10.314905528341498,
      "grad_norm": 0.39328208565711975,
      "learning_rate": 4.7632447552447557e-05,
      "loss": 0.5964621353149414,
      "step": 7375
    },
    {
      "epoch": 10.349895031490552,
      "grad_norm": 0.5641208291053772,
      "learning_rate": 4.759048951048951e-05,
      "loss": 0.6124051284790039,
      "step": 7400
    },
    {
      "epoch": 10.384884534639609,
      "grad_norm": 0.30292534828186035,
      "learning_rate": 4.754853146853147e-05,
      "loss": 0.5885390090942383,
      "step": 7425
    },
    {
      "epoch": 10.419874037788663,
      "grad_norm": 0.7830162048339844,
      "learning_rate": 4.750657342657343e-05,
      "loss": 0.6647960662841796,
      "step": 7450
    },
    {
      "epoch": 10.454863540937719,
      "grad_norm": 2.369232416152954,
      "learning_rate": 4.746461538461539e-05,
      "loss": 0.6015251541137695,
      "step": 7475
    },
    {
      "epoch": 10.489853044086773,
      "grad_norm": 0.7971821427345276,
      "learning_rate": 4.742265734265734e-05,
      "loss": 0.6090465927124024,
      "step": 7500
    },
    {
      "epoch": 10.52484254723583,
      "grad_norm": 0.49525904655456543,
      "learning_rate": 4.73806993006993e-05,
      "loss": 0.6136074066162109,
      "step": 7525
    },
    {
      "epoch": 10.559832050384884,
      "grad_norm": 0.2583949863910675,
      "learning_rate": 4.733874125874126e-05,
      "loss": 0.5851440811157227,
      "step": 7550
    },
    {
      "epoch": 10.59482155353394,
      "grad_norm": 0.3437162935733795,
      "learning_rate": 4.729678321678322e-05,
      "loss": 0.5870180892944336,
      "step": 7575
    },
    {
      "epoch": 10.629811056682994,
      "grad_norm": 0.4048153758049011,
      "learning_rate": 4.7254825174825176e-05,
      "loss": 0.5924134826660157,
      "step": 7600
    },
    {
      "epoch": 10.66480055983205,
      "grad_norm": 0.5141425728797913,
      "learning_rate": 4.721286713286713e-05,
      "loss": 0.6280804061889649,
      "step": 7625
    },
    {
      "epoch": 10.699790062981105,
      "grad_norm": 0.9719255566596985,
      "learning_rate": 4.7170909090909096e-05,
      "loss": 0.575724983215332,
      "step": 7650
    },
    {
      "epoch": 10.734779566130161,
      "grad_norm": 0.8267336487770081,
      "learning_rate": 4.712895104895105e-05,
      "loss": 0.6008329772949219,
      "step": 7675
    },
    {
      "epoch": 10.769769069279217,
      "grad_norm": 0.3388083875179291,
      "learning_rate": 4.708699300699301e-05,
      "loss": 0.5852348327636718,
      "step": 7700
    },
    {
      "epoch": 10.804758572428272,
      "grad_norm": 0.8439804315567017,
      "learning_rate": 4.7045034965034965e-05,
      "loss": 0.5472151947021484,
      "step": 7725
    },
    {
      "epoch": 10.839748075577326,
      "grad_norm": 0.40140771865844727,
      "learning_rate": 4.700307692307692e-05,
      "loss": 0.5761281204223633,
      "step": 7750
    },
    {
      "epoch": 10.874737578726382,
      "grad_norm": 0.549631655216217,
      "learning_rate": 4.69627972027972e-05,
      "loss": 0.6916178894042969,
      "step": 7775
    },
    {
      "epoch": 10.909727081875438,
      "grad_norm": 0.42637336254119873,
      "learning_rate": 4.692083916083916e-05,
      "loss": 0.6082349014282227,
      "step": 7800
    },
    {
      "epoch": 10.944716585024493,
      "grad_norm": 0.71455317735672,
      "learning_rate": 4.687888111888112e-05,
      "loss": 0.6182440567016602,
      "step": 7825
    },
    {
      "epoch": 10.979706088173549,
      "grad_norm": 0.8038988709449768,
      "learning_rate": 4.6836923076923075e-05,
      "loss": 0.5697647476196289,
      "step": 7850
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.28962162137031555,
      "eval_mean_accuracy": 0.7843905389049581,
      "eval_mean_iou": 0.6906031843428574,
      "eval_overall_accuracy": 0.8836093024124483,
      "eval_per_category_accuracy": [
        0.8999676232004055,
        0.8583530680379754,
        0.5409344345112084,
        0.5756485534745741,
        0.840000429262153,
        0.9914391249434329
      ],
      "eval_per_category_iou": [
        0.7828617591727696,
        0.7240341093241839,
        0.45685414233805344,
        0.4921993939121808,
        0.7023976327335505,
        0.985272068576406
      ],
      "eval_runtime": 29.6028,
      "eval_samples_per_second": 10.708,
      "eval_steps_per_second": 5.371,
      "step": 7865
    },
    {
      "epoch": 11.013995801259622,
      "grad_norm": 0.7635749578475952,
      "learning_rate": 4.679496503496504e-05,
      "loss": 0.5793519973754883,
      "step": 7875
    },
    {
      "epoch": 11.048985304408678,
      "grad_norm": 0.5666685104370117,
      "learning_rate": 4.6753006993006995e-05,
      "loss": 0.5888129043579101,
      "step": 7900
    },
    {
      "epoch": 11.083974807557732,
      "grad_norm": 1.470889925956726,
      "learning_rate": 4.671104895104895e-05,
      "loss": 0.6221688079833985,
      "step": 7925
    },
    {
      "epoch": 11.118964310706788,
      "grad_norm": 1.070164442062378,
      "learning_rate": 4.6669090909090914e-05,
      "loss": 0.6043564224243164,
      "step": 7950
    },
    {
      "epoch": 11.153953813855843,
      "grad_norm": 2.345756769180298,
      "learning_rate": 4.662713286713287e-05,
      "loss": 0.6018467330932618,
      "step": 7975
    },
    {
      "epoch": 11.188943317004899,
      "grad_norm": 0.2545855939388275,
      "learning_rate": 4.6585174825174826e-05,
      "loss": 0.5880290222167969,
      "step": 8000
    },
    {
      "epoch": 11.223932820153953,
      "grad_norm": 0.9456899762153625,
      "learning_rate": 4.654321678321678e-05,
      "loss": 0.585481834411621,
      "step": 8025
    },
    {
      "epoch": 11.25892232330301,
      "grad_norm": 0.5697915554046631,
      "learning_rate": 4.650125874125874e-05,
      "loss": 0.5932183456420899,
      "step": 8050
    },
    {
      "epoch": 11.293911826452064,
      "grad_norm": 0.7295617461204529,
      "learning_rate": 4.64593006993007e-05,
      "loss": 0.5626591110229492,
      "step": 8075
    },
    {
      "epoch": 11.32890132960112,
      "grad_norm": 0.42456334829330444,
      "learning_rate": 4.641734265734266e-05,
      "loss": 0.5735394668579101,
      "step": 8100
    },
    {
      "epoch": 11.363890832750174,
      "grad_norm": 0.6520422697067261,
      "learning_rate": 4.6375384615384614e-05,
      "loss": 0.6051164245605469,
      "step": 8125
    },
    {
      "epoch": 11.39888033589923,
      "grad_norm": 0.5867430567741394,
      "learning_rate": 4.633342657342658e-05,
      "loss": 0.5692935180664063,
      "step": 8150
    },
    {
      "epoch": 11.433869839048285,
      "grad_norm": 0.9932511448860168,
      "learning_rate": 4.6291468531468534e-05,
      "loss": 0.6217315673828125,
      "step": 8175
    },
    {
      "epoch": 11.46885934219734,
      "grad_norm": 1.9944179058074951,
      "learning_rate": 4.62495104895105e-05,
      "loss": 0.5742315292358399,
      "step": 8200
    },
    {
      "epoch": 11.503848845346397,
      "grad_norm": 0.3613065183162689,
      "learning_rate": 4.6207552447552446e-05,
      "loss": 0.5547092056274414,
      "step": 8225
    },
    {
      "epoch": 11.538838348495451,
      "grad_norm": 0.8025244474411011,
      "learning_rate": 4.61655944055944e-05,
      "loss": 0.5914897155761719,
      "step": 8250
    },
    {
      "epoch": 11.573827851644507,
      "grad_norm": 0.6433390378952026,
      "learning_rate": 4.6123636363636366e-05,
      "loss": 0.615620231628418,
      "step": 8275
    },
    {
      "epoch": 11.608817354793562,
      "grad_norm": 0.4077147841453552,
      "learning_rate": 4.608167832167832e-05,
      "loss": 0.5950692749023437,
      "step": 8300
    },
    {
      "epoch": 11.643806857942618,
      "grad_norm": 0.33923259377479553,
      "learning_rate": 4.603972027972028e-05,
      "loss": 0.5902388763427734,
      "step": 8325
    },
    {
      "epoch": 11.678796361091672,
      "grad_norm": 1.5969222784042358,
      "learning_rate": 4.599776223776224e-05,
      "loss": 0.6424570465087891,
      "step": 8350
    },
    {
      "epoch": 11.713785864240728,
      "grad_norm": 0.3497375249862671,
      "learning_rate": 4.59558041958042e-05,
      "loss": 0.6236612319946289,
      "step": 8375
    },
    {
      "epoch": 11.748775367389783,
      "grad_norm": 0.6723785996437073,
      "learning_rate": 4.591384615384616e-05,
      "loss": 0.6035808563232422,
      "step": 8400
    },
    {
      "epoch": 11.783764870538839,
      "grad_norm": 0.7316650152206421,
      "learning_rate": 4.587188811188812e-05,
      "loss": 0.5725051498413086,
      "step": 8425
    },
    {
      "epoch": 11.818754373687893,
      "grad_norm": 0.48370611667633057,
      "learning_rate": 4.5829930069930066e-05,
      "loss": 0.5657574081420899,
      "step": 8450
    },
    {
      "epoch": 11.85374387683695,
      "grad_norm": 0.8113285303115845,
      "learning_rate": 4.578797202797203e-05,
      "loss": 0.5739101028442383,
      "step": 8475
    },
    {
      "epoch": 11.888733379986004,
      "grad_norm": 0.4736769497394562,
      "learning_rate": 4.5746013986013985e-05,
      "loss": 0.6046260070800781,
      "step": 8500
    },
    {
      "epoch": 11.92372288313506,
      "grad_norm": 0.520061731338501,
      "learning_rate": 4.570405594405594e-05,
      "loss": 0.58307373046875,
      "step": 8525
    },
    {
      "epoch": 11.958712386284114,
      "grad_norm": 0.36634162068367004,
      "learning_rate": 4.5662097902097905e-05,
      "loss": 0.6040757751464844,
      "step": 8550
    },
    {
      "epoch": 11.99370188943317,
      "grad_norm": 0.3326001763343811,
      "learning_rate": 4.562013986013986e-05,
      "loss": 0.5837796401977539,
      "step": 8575
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.28856533765792847,
      "eval_mean_accuracy": 0.784011557588367,
      "eval_mean_iou": 0.6912011337262091,
      "eval_overall_accuracy": 0.88392174657587,
      "eval_per_category_accuracy": [
        0.8987251799251431,
        0.8589406338314471,
        0.5453678724944164,
        0.569235845373675,
        0.8402810695188032,
        0.9915187443867177
      ],
      "eval_per_category_iou": [
        0.7827957116664365,
        0.7251110846868373,
        0.45848587902423,
        0.49240900274361005,
        0.7031019004657821,
        0.9853032237703582
      ],
      "eval_runtime": 30.2252,
      "eval_samples_per_second": 10.488,
      "eval_steps_per_second": 5.261,
      "step": 8580
    },
    {
      "epoch": 12.027991602519243,
      "grad_norm": 1.7923710346221924,
      "learning_rate": 4.5578181818181824e-05,
      "loss": 0.5767718505859375,
      "step": 8600
    },
    {
      "epoch": 12.0629811056683,
      "grad_norm": 0.3594007194042206,
      "learning_rate": 4.553622377622378e-05,
      "loss": 0.6179180145263672,
      "step": 8625
    },
    {
      "epoch": 12.097970608817354,
      "grad_norm": 0.37595465779304504,
      "learning_rate": 4.549426573426574e-05,
      "loss": 0.5440825271606445,
      "step": 8650
    },
    {
      "epoch": 12.13296011196641,
      "grad_norm": 0.42218849062919617,
      "learning_rate": 4.545230769230769e-05,
      "loss": 0.6142815399169922,
      "step": 8675
    },
    {
      "epoch": 12.167949615115464,
      "grad_norm": 1.6800881624221802,
      "learning_rate": 4.541034965034965e-05,
      "loss": 0.6085757064819336,
      "step": 8700
    },
    {
      "epoch": 12.20293911826452,
      "grad_norm": 0.4320789575576782,
      "learning_rate": 4.5368391608391605e-05,
      "loss": 0.5872242736816407,
      "step": 8725
    },
    {
      "epoch": 12.237928621413577,
      "grad_norm": 0.5844225883483887,
      "learning_rate": 4.532643356643357e-05,
      "loss": 0.601260757446289,
      "step": 8750
    },
    {
      "epoch": 12.272918124562631,
      "grad_norm": 0.5784216523170471,
      "learning_rate": 4.5284475524475525e-05,
      "loss": 0.5341876220703125,
      "step": 8775
    },
    {
      "epoch": 12.307907627711687,
      "grad_norm": 0.6547353863716125,
      "learning_rate": 4.524251748251749e-05,
      "loss": 0.599637336730957,
      "step": 8800
    },
    {
      "epoch": 12.342897130860742,
      "grad_norm": 1.0818626880645752,
      "learning_rate": 4.5200559440559444e-05,
      "loss": 0.5729117965698243,
      "step": 8825
    },
    {
      "epoch": 12.377886634009798,
      "grad_norm": 0.5041584968566895,
      "learning_rate": 4.51586013986014e-05,
      "loss": 0.5773816680908204,
      "step": 8850
    },
    {
      "epoch": 12.412876137158852,
      "grad_norm": 0.62913578748703,
      "learning_rate": 4.5116643356643357e-05,
      "loss": 0.6447451782226562,
      "step": 8875
    },
    {
      "epoch": 12.447865640307908,
      "grad_norm": 0.8907202482223511,
      "learning_rate": 4.507468531468531e-05,
      "loss": 0.5901473617553711,
      "step": 8900
    },
    {
      "epoch": 12.482855143456963,
      "grad_norm": 0.6528974771499634,
      "learning_rate": 4.503272727272727e-05,
      "loss": 0.6313160705566406,
      "step": 8925
    },
    {
      "epoch": 12.517844646606019,
      "grad_norm": 0.631339967250824,
      "learning_rate": 4.499076923076923e-05,
      "loss": 0.5685769653320313,
      "step": 8950
    },
    {
      "epoch": 12.552834149755073,
      "grad_norm": 0.6744922995567322,
      "learning_rate": 4.494881118881119e-05,
      "loss": 0.6544225311279297,
      "step": 8975
    },
    {
      "epoch": 12.58782365290413,
      "grad_norm": 0.3762761354446411,
      "learning_rate": 4.490685314685315e-05,
      "loss": 0.5500931167602539,
      "step": 9000
    },
    {
      "epoch": 12.622813156053184,
      "grad_norm": 0.29449236392974854,
      "learning_rate": 4.486489510489511e-05,
      "loss": 0.5764006042480468,
      "step": 9025
    },
    {
      "epoch": 12.65780265920224,
      "grad_norm": 0.623757004737854,
      "learning_rate": 4.4822937062937064e-05,
      "loss": 0.5925287246704102,
      "step": 9050
    },
    {
      "epoch": 12.692792162351294,
      "grad_norm": 0.5744723081588745,
      "learning_rate": 4.478097902097903e-05,
      "loss": 0.6256658935546875,
      "step": 9075
    },
    {
      "epoch": 12.72778166550035,
      "grad_norm": 0.20984511077404022,
      "learning_rate": 4.4739020979020976e-05,
      "loss": 0.5388032531738282,
      "step": 9100
    },
    {
      "epoch": 12.762771168649405,
      "grad_norm": 0.6380760669708252,
      "learning_rate": 4.469706293706293e-05,
      "loss": 0.5550904083251953,
      "step": 9125
    },
    {
      "epoch": 12.79776067179846,
      "grad_norm": 0.7563312649726868,
      "learning_rate": 4.4655104895104896e-05,
      "loss": 0.546419563293457,
      "step": 9150
    },
    {
      "epoch": 12.832750174947515,
      "grad_norm": 0.5991966724395752,
      "learning_rate": 4.461314685314685e-05,
      "loss": 0.551454963684082,
      "step": 9175
    },
    {
      "epoch": 12.867739678096571,
      "grad_norm": 0.8109058141708374,
      "learning_rate": 4.4571188811188815e-05,
      "loss": 0.6132026290893555,
      "step": 9200
    },
    {
      "epoch": 12.902729181245626,
      "grad_norm": 0.5150099992752075,
      "learning_rate": 4.452923076923077e-05,
      "loss": 0.6055131912231445,
      "step": 9225
    },
    {
      "epoch": 12.937718684394682,
      "grad_norm": 1.0336846113204956,
      "learning_rate": 4.448727272727273e-05,
      "loss": 0.5654550170898438,
      "step": 9250
    },
    {
      "epoch": 12.972708187543738,
      "grad_norm": 0.641270637512207,
      "learning_rate": 4.444531468531469e-05,
      "loss": 0.58756591796875,
      "step": 9275
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.2883179485797882,
      "eval_mean_accuracy": 0.7858116318038192,
      "eval_mean_iou": 0.6920795853208442,
      "eval_overall_accuracy": 0.8840759108871316,
      "eval_per_category_accuracy": [
        0.9054911314659172,
        0.8621542265401196,
        0.5404703226443555,
        0.5788479716540863,
        0.836715561380742,
        0.9911905771376944
      ],
      "eval_per_category_iou": [
        0.7836891537660908,
        0.7250922161090663,
        0.45860025389209796,
        0.497095454750444,
        0.7026526146108053,
        0.9853478187965609
      ],
      "eval_runtime": 29.3595,
      "eval_samples_per_second": 10.797,
      "eval_steps_per_second": 5.416,
      "step": 9295
    },
    {
      "epoch": 13.00699790062981,
      "grad_norm": 0.3494628369808197,
      "learning_rate": 4.440335664335665e-05,
      "loss": 0.5778348541259766,
      "step": 9300
    },
    {
      "epoch": 13.041987403778867,
      "grad_norm": 0.5121039152145386,
      "learning_rate": 4.4361398601398596e-05,
      "loss": 0.5915211105346679,
      "step": 9325
    },
    {
      "epoch": 13.076976906927921,
      "grad_norm": 0.5384925007820129,
      "learning_rate": 4.431944055944056e-05,
      "loss": 0.5820891571044922,
      "step": 9350
    },
    {
      "epoch": 13.111966410076977,
      "grad_norm": 3.623046636581421,
      "learning_rate": 4.4277482517482516e-05,
      "loss": 0.5828774642944335,
      "step": 9375
    },
    {
      "epoch": 13.146955913226032,
      "grad_norm": 0.8387861847877502,
      "learning_rate": 4.423552447552448e-05,
      "loss": 0.6238012313842773,
      "step": 9400
    },
    {
      "epoch": 13.181945416375088,
      "grad_norm": 0.2794761061668396,
      "learning_rate": 4.4193566433566435e-05,
      "loss": 0.5689554595947266,
      "step": 9425
    },
    {
      "epoch": 13.216934919524142,
      "grad_norm": 1.462607979774475,
      "learning_rate": 4.415160839160839e-05,
      "loss": 0.5996968078613282,
      "step": 9450
    },
    {
      "epoch": 13.251924422673198,
      "grad_norm": 1.860133409500122,
      "learning_rate": 4.4109650349650354e-05,
      "loss": 0.5946464157104492,
      "step": 9475
    },
    {
      "epoch": 13.286913925822253,
      "grad_norm": 1.074191689491272,
      "learning_rate": 4.406769230769231e-05,
      "loss": 0.6196063613891601,
      "step": 9500
    },
    {
      "epoch": 13.321903428971309,
      "grad_norm": 0.9915130138397217,
      "learning_rate": 4.402573426573427e-05,
      "loss": 0.5853318786621093,
      "step": 9525
    },
    {
      "epoch": 13.356892932120363,
      "grad_norm": 0.6273157000541687,
      "learning_rate": 4.398377622377622e-05,
      "loss": 0.555142707824707,
      "step": 9550
    },
    {
      "epoch": 13.39188243526942,
      "grad_norm": 1.5596470832824707,
      "learning_rate": 4.394181818181818e-05,
      "loss": 0.5896990966796875,
      "step": 9575
    },
    {
      "epoch": 13.426871938418474,
      "grad_norm": 0.5443332195281982,
      "learning_rate": 4.389986013986014e-05,
      "loss": 0.6064574050903321,
      "step": 9600
    },
    {
      "epoch": 13.46186144156753,
      "grad_norm": 0.3883919417858124,
      "learning_rate": 4.38579020979021e-05,
      "loss": 0.5452542495727539,
      "step": 9625
    },
    {
      "epoch": 13.496850944716584,
      "grad_norm": 0.46607914566993713,
      "learning_rate": 4.3815944055944055e-05,
      "loss": 0.5785845947265625,
      "step": 9650
    },
    {
      "epoch": 13.53184044786564,
      "grad_norm": 0.8240091800689697,
      "learning_rate": 4.377398601398602e-05,
      "loss": 0.5835233688354492,
      "step": 9675
    },
    {
      "epoch": 13.566829951014695,
      "grad_norm": 0.554826557636261,
      "learning_rate": 4.3732027972027974e-05,
      "loss": 0.6016419982910156,
      "step": 9700
    },
    {
      "epoch": 13.601819454163751,
      "grad_norm": 0.40923216938972473,
      "learning_rate": 4.369006993006993e-05,
      "loss": 0.5456550598144532,
      "step": 9725
    },
    {
      "epoch": 13.636808957312805,
      "grad_norm": 0.4732873737812042,
      "learning_rate": 4.3648111888111894e-05,
      "loss": 0.5909709930419922,
      "step": 9750
    },
    {
      "epoch": 13.671798460461861,
      "grad_norm": 0.42677250504493713,
      "learning_rate": 4.360615384615384e-05,
      "loss": 0.5803918838500977,
      "step": 9775
    },
    {
      "epoch": 13.706787963610918,
      "grad_norm": 0.3507055938243866,
      "learning_rate": 4.3564195804195806e-05,
      "loss": 0.5846903991699218,
      "step": 9800
    },
    {
      "epoch": 13.741777466759972,
      "grad_norm": 0.6335877776145935,
      "learning_rate": 4.352223776223776e-05,
      "loss": 0.5845813751220703,
      "step": 9825
    },
    {
      "epoch": 13.776766969909028,
      "grad_norm": 0.39073994755744934,
      "learning_rate": 4.348027972027972e-05,
      "loss": 0.5959120559692382,
      "step": 9850
    },
    {
      "epoch": 13.811756473058082,
      "grad_norm": 1.3618966341018677,
      "learning_rate": 4.343832167832168e-05,
      "loss": 0.586882209777832,
      "step": 9875
    },
    {
      "epoch": 13.846745976207139,
      "grad_norm": 0.4710772633552551,
      "learning_rate": 4.339636363636364e-05,
      "loss": 0.6008409118652344,
      "step": 9900
    },
    {
      "epoch": 13.881735479356193,
      "grad_norm": 1.6086723804473877,
      "learning_rate": 4.3354405594405594e-05,
      "loss": 0.5965111923217773,
      "step": 9925
    },
    {
      "epoch": 13.916724982505249,
      "grad_norm": 2.240999698638916,
      "learning_rate": 4.331244755244756e-05,
      "loss": 0.6268789672851562,
      "step": 9950
    },
    {
      "epoch": 13.951714485654303,
      "grad_norm": 0.9961800575256348,
      "learning_rate": 4.3270489510489513e-05,
      "loss": 0.5416367340087891,
      "step": 9975
    },
    {
      "epoch": 13.98670398880336,
      "grad_norm": 1.0630197525024414,
      "learning_rate": 4.322853146853147e-05,
      "loss": 0.5761581039428711,
      "step": 10000
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.28574976325035095,
      "eval_mean_accuracy": 0.788117367129804,
      "eval_mean_iou": 0.6940763112511951,
      "eval_overall_accuracy": 0.8845528202478066,
      "eval_per_category_accuracy": [
        0.9130589249398857,
        0.8520831674707371,
        0.5564394137583186,
        0.5760300263761263,
        0.8400990380680612,
        0.9909936321656941
      ],
      "eval_per_category_iou": [
        0.7829009606398495,
        0.7247017619950663,
        0.4661166561257869,
        0.5007912135242812,
        0.7045517038307233,
        0.9853955713914632
      ],
      "eval_runtime": 29.6775,
      "eval_samples_per_second": 10.681,
      "eval_steps_per_second": 5.358,
      "step": 10010
    },
    {
      "epoch": 14.020993701889433,
      "grad_norm": 0.674103319644928,
      "learning_rate": 4.3186573426573426e-05,
      "loss": 0.5362611770629883,
      "step": 10025
    },
    {
      "epoch": 14.055983205038489,
      "grad_norm": 5.831978797912598,
      "learning_rate": 4.314461538461538e-05,
      "loss": 0.5790526962280274,
      "step": 10050
    },
    {
      "epoch": 14.090972708187543,
      "grad_norm": 0.44844093918800354,
      "learning_rate": 4.3102657342657345e-05,
      "loss": 0.5561718368530273,
      "step": 10075
    },
    {
      "epoch": 14.1259622113366,
      "grad_norm": 0.29734230041503906,
      "learning_rate": 4.30606993006993e-05,
      "loss": 0.6125726318359375,
      "step": 10100
    },
    {
      "epoch": 14.160951714485654,
      "grad_norm": 0.643371045589447,
      "learning_rate": 4.301874125874126e-05,
      "loss": 0.5911331558227539,
      "step": 10125
    },
    {
      "epoch": 14.19594121763471,
      "grad_norm": 3.1172730922698975,
      "learning_rate": 4.297678321678322e-05,
      "loss": 0.5529672241210938,
      "step": 10150
    },
    {
      "epoch": 14.230930720783764,
      "grad_norm": 1.1333017349243164,
      "learning_rate": 4.293482517482518e-05,
      "loss": 0.6131897354125977,
      "step": 10175
    },
    {
      "epoch": 14.26592022393282,
      "grad_norm": 0.34547728300094604,
      "learning_rate": 4.2892867132867133e-05,
      "loss": 0.5950219345092773,
      "step": 10200
    },
    {
      "epoch": 14.300909727081875,
      "grad_norm": 0.456594318151474,
      "learning_rate": 4.285090909090909e-05,
      "loss": 0.5891239166259765,
      "step": 10225
    },
    {
      "epoch": 14.33589923023093,
      "grad_norm": 0.5418087244033813,
      "learning_rate": 4.2808951048951046e-05,
      "loss": 0.5597427749633789,
      "step": 10250
    },
    {
      "epoch": 14.370888733379987,
      "grad_norm": 1.175406813621521,
      "learning_rate": 4.276699300699301e-05,
      "loss": 0.571081657409668,
      "step": 10275
    },
    {
      "epoch": 14.405878236529041,
      "grad_norm": 0.36199966073036194,
      "learning_rate": 4.2725034965034965e-05,
      "loss": 0.5572761535644531,
      "step": 10300
    },
    {
      "epoch": 14.440867739678097,
      "grad_norm": 0.27413469552993774,
      "learning_rate": 4.268307692307692e-05,
      "loss": 0.5577907180786132,
      "step": 10325
    },
    {
      "epoch": 14.475857242827152,
      "grad_norm": 0.743131697177887,
      "learning_rate": 4.2641118881118885e-05,
      "loss": 0.6201995849609375,
      "step": 10350
    },
    {
      "epoch": 14.510846745976208,
      "grad_norm": 0.39771029353141785,
      "learning_rate": 4.259916083916084e-05,
      "loss": 0.5711869430541993,
      "step": 10375
    },
    {
      "epoch": 14.545836249125262,
      "grad_norm": 0.4301448166370392,
      "learning_rate": 4.2557202797202804e-05,
      "loss": 0.5778006744384766,
      "step": 10400
    },
    {
      "epoch": 14.580825752274318,
      "grad_norm": 0.6901156306266785,
      "learning_rate": 4.251524475524475e-05,
      "loss": 0.5881895065307617,
      "step": 10425
    },
    {
      "epoch": 14.615815255423373,
      "grad_norm": 0.6577427983283997,
      "learning_rate": 4.247328671328671e-05,
      "loss": 0.5803816604614258,
      "step": 10450
    },
    {
      "epoch": 14.650804758572429,
      "grad_norm": 0.9887155294418335,
      "learning_rate": 4.243132867132867e-05,
      "loss": 0.5849685668945312,
      "step": 10475
    },
    {
      "epoch": 14.685794261721483,
      "grad_norm": 1.3320685625076294,
      "learning_rate": 4.238937062937063e-05,
      "loss": 0.6083088684082031,
      "step": 10500
    },
    {
      "epoch": 14.72078376487054,
      "grad_norm": 0.4855324923992157,
      "learning_rate": 4.2347412587412585e-05,
      "loss": 0.5410945510864258,
      "step": 10525
    },
    {
      "epoch": 14.755773268019594,
      "grad_norm": 0.5825875401496887,
      "learning_rate": 4.230545454545455e-05,
      "loss": 0.62746826171875,
      "step": 10550
    },
    {
      "epoch": 14.79076277116865,
      "grad_norm": 0.449971079826355,
      "learning_rate": 4.2263496503496504e-05,
      "loss": 0.5529629516601563,
      "step": 10575
    },
    {
      "epoch": 14.825752274317704,
      "grad_norm": 0.5838002562522888,
      "learning_rate": 4.222153846153847e-05,
      "loss": 0.582504768371582,
      "step": 10600
    },
    {
      "epoch": 14.86074177746676,
      "grad_norm": 0.4753659963607788,
      "learning_rate": 4.2179580419580424e-05,
      "loss": 0.6037428283691406,
      "step": 10625
    },
    {
      "epoch": 14.895731280615815,
      "grad_norm": 0.7028641700744629,
      "learning_rate": 4.213762237762237e-05,
      "loss": 0.5681732559204101,
      "step": 10650
    },
    {
      "epoch": 14.93072078376487,
      "grad_norm": 0.8587599992752075,
      "learning_rate": 4.2095664335664336e-05,
      "loss": 0.5700965881347656,
      "step": 10675
    },
    {
      "epoch": 14.965710286913925,
      "grad_norm": 0.6090745329856873,
      "learning_rate": 4.205370629370629e-05,
      "loss": 0.5756109619140625,
      "step": 10700
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.27595093846321106,
      "learning_rate": 4.201174825174825e-05,
      "loss": 0.5973335266113281,
      "step": 10725
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.28466176986694336,
      "eval_mean_accuracy": 0.7903004911693334,
      "eval_mean_iou": 0.6960885924650738,
      "eval_overall_accuracy": 0.8850460232996414,
      "eval_per_category_accuracy": [
        0.9081870102084181,
        0.8572560194578613,
        0.5715429603479892,
        0.5786319688163243,
        0.8348721132879516,
        0.9913128748974557
      ],
      "eval_per_category_iou": [
        0.7858271941288333,
        0.7257795661922265,
        0.47285884756936886,
        0.5023323908576127,
        0.7042997555903049,
        0.9854338004520962
      ],
      "eval_runtime": 29.3711,
      "eval_samples_per_second": 10.793,
      "eval_steps_per_second": 5.413,
      "step": 10725
    },
    {
      "epoch": 15.034989503149056,
      "grad_norm": 0.5446794033050537,
      "learning_rate": 4.196979020979021e-05,
      "loss": 0.5703859329223633,
      "step": 10750
    },
    {
      "epoch": 15.06997900629811,
      "grad_norm": 0.3754880130290985,
      "learning_rate": 4.192783216783217e-05,
      "loss": 0.5849104690551757,
      "step": 10775
    },
    {
      "epoch": 15.104968509447167,
      "grad_norm": 0.7208439111709595,
      "learning_rate": 4.188587412587413e-05,
      "loss": 0.5944292831420899,
      "step": 10800
    },
    {
      "epoch": 15.139958012596221,
      "grad_norm": 0.7473345398902893,
      "learning_rate": 4.184391608391609e-05,
      "loss": 0.6206257629394532,
      "step": 10825
    },
    {
      "epoch": 15.174947515745277,
      "grad_norm": 3.967470645904541,
      "learning_rate": 4.1801958041958044e-05,
      "loss": 0.5835567092895508,
      "step": 10850
    },
    {
      "epoch": 15.209937018894331,
      "grad_norm": 1.4364268779754639,
      "learning_rate": 4.176e-05,
      "loss": 0.6265860748291016,
      "step": 10875
    },
    {
      "epoch": 15.244926522043388,
      "grad_norm": 1.1365387439727783,
      "learning_rate": 4.1718041958041956e-05,
      "loss": 0.5972543716430664,
      "step": 10900
    },
    {
      "epoch": 15.279916025192442,
      "grad_norm": 0.3048664927482605,
      "learning_rate": 4.167608391608391e-05,
      "loss": 0.5668912124633789,
      "step": 10925
    },
    {
      "epoch": 15.314905528341498,
      "grad_norm": 1.140978217124939,
      "learning_rate": 4.163580419580419e-05,
      "loss": 0.5903224182128907,
      "step": 10950
    },
    {
      "epoch": 15.349895031490552,
      "grad_norm": 1.071484088897705,
      "learning_rate": 4.1593846153846154e-05,
      "loss": 0.5755949401855469,
      "step": 10975
    },
    {
      "epoch": 15.384884534639609,
      "grad_norm": 0.6780669093132019,
      "learning_rate": 4.155188811188811e-05,
      "loss": 0.5347472381591797,
      "step": 11000
    },
    {
      "epoch": 15.419874037788663,
      "grad_norm": 1.2632699012756348,
      "learning_rate": 4.1509930069930074e-05,
      "loss": 0.5997105026245118,
      "step": 11025
    },
    {
      "epoch": 15.454863540937719,
      "grad_norm": 0.6050742268562317,
      "learning_rate": 4.146797202797203e-05,
      "loss": 0.6256034088134765,
      "step": 11050
    },
    {
      "epoch": 15.489853044086773,
      "grad_norm": 0.3418700098991394,
      "learning_rate": 4.1426013986013986e-05,
      "loss": 0.5970918655395507,
      "step": 11075
    },
    {
      "epoch": 15.52484254723583,
      "grad_norm": 0.5535130500793457,
      "learning_rate": 4.138405594405595e-05,
      "loss": 0.5891696166992187,
      "step": 11100
    },
    {
      "epoch": 15.559832050384884,
      "grad_norm": 0.4049184322357178,
      "learning_rate": 4.1342097902097906e-05,
      "loss": 0.5499106979370117,
      "step": 11125
    },
    {
      "epoch": 15.59482155353394,
      "grad_norm": 0.6481803059577942,
      "learning_rate": 4.1300139860139855e-05,
      "loss": 0.5774156951904297,
      "step": 11150
    },
    {
      "epoch": 15.629811056682994,
      "grad_norm": 0.7053404450416565,
      "learning_rate": 4.125818181818182e-05,
      "loss": 0.5408637619018555,
      "step": 11175
    },
    {
      "epoch": 15.66480055983205,
      "grad_norm": 0.5007139444351196,
      "learning_rate": 4.1216223776223774e-05,
      "loss": 0.6096127319335938,
      "step": 11200
    },
    {
      "epoch": 15.699790062981105,
      "grad_norm": 0.4033370018005371,
      "learning_rate": 4.117426573426574e-05,
      "loss": 0.5956153869628906,
      "step": 11225
    },
    {
      "epoch": 15.734779566130161,
      "grad_norm": 0.410102516412735,
      "learning_rate": 4.1132307692307694e-05,
      "loss": 0.5363703536987304,
      "step": 11250
    },
    {
      "epoch": 15.769769069279217,
      "grad_norm": 0.2880478799343109,
      "learning_rate": 4.109034965034965e-05,
      "loss": 0.5681180953979492,
      "step": 11275
    },
    {
      "epoch": 15.804758572428272,
      "grad_norm": 0.600938618183136,
      "learning_rate": 4.104839160839161e-05,
      "loss": 0.5485551834106446,
      "step": 11300
    },
    {
      "epoch": 15.839748075577326,
      "grad_norm": 0.3838157653808594,
      "learning_rate": 4.100643356643357e-05,
      "loss": 0.6083915328979492,
      "step": 11325
    },
    {
      "epoch": 15.874737578726382,
      "grad_norm": 0.32576435804367065,
      "learning_rate": 4.0964475524475525e-05,
      "loss": 0.5600942230224609,
      "step": 11350
    },
    {
      "epoch": 15.909727081875438,
      "grad_norm": 3.1940150260925293,
      "learning_rate": 4.092251748251748e-05,
      "loss": 0.588097152709961,
      "step": 11375
    },
    {
      "epoch": 15.944716585024493,
      "grad_norm": 0.48482435941696167,
      "learning_rate": 4.088055944055944e-05,
      "loss": 0.55126220703125,
      "step": 11400
    },
    {
      "epoch": 15.979706088173549,
      "grad_norm": 0.4279298782348633,
      "learning_rate": 4.08386013986014e-05,
      "loss": 0.5590941238403321,
      "step": 11425
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.2849807143211365,
      "eval_mean_accuracy": 0.7896262466565647,
      "eval_mean_iou": 0.695754314728219,
      "eval_overall_accuracy": 0.8849249517880028,
      "eval_per_category_accuracy": [
        0.9057116708159013,
        0.862300087481233,
        0.5543793927902481,
        0.5886087237402972,
        0.8354656390832661,
        0.9912919660284426
      ],
      "eval_per_category_iou": [
        0.7872098414558882,
        0.726694732312063,
        0.4657305129708003,
        0.5062599549518455,
        0.7032100879592068,
        0.9854207587195103
      ],
      "eval_runtime": 29.652,
      "eval_samples_per_second": 10.691,
      "eval_steps_per_second": 5.362,
      "step": 11440
    },
    {
      "epoch": 16.01399580125962,
      "grad_norm": 0.27090176939964294,
      "learning_rate": 4.079664335664336e-05,
      "loss": 0.5856361007690429,
      "step": 11450
    },
    {
      "epoch": 16.048985304408678,
      "grad_norm": 0.4949880838394165,
      "learning_rate": 4.0754685314685314e-05,
      "loss": 0.5693071365356446,
      "step": 11475
    },
    {
      "epoch": 16.083974807557734,
      "grad_norm": 0.21663729846477509,
      "learning_rate": 4.0712727272727277e-05,
      "loss": 0.5835152053833008,
      "step": 11500
    },
    {
      "epoch": 16.118964310706787,
      "grad_norm": 1.048478126525879,
      "learning_rate": 4.067076923076923e-05,
      "loss": 0.5902505111694336,
      "step": 11525
    },
    {
      "epoch": 16.153953813855843,
      "grad_norm": 1.5834306478500366,
      "learning_rate": 4.062881118881119e-05,
      "loss": 0.6026797485351563,
      "step": 11550
    },
    {
      "epoch": 16.1889433170049,
      "grad_norm": 0.4041275084018707,
      "learning_rate": 4.058685314685315e-05,
      "loss": 0.5407492446899415,
      "step": 11575
    },
    {
      "epoch": 16.223932820153955,
      "grad_norm": 0.6000856161117554,
      "learning_rate": 4.05448951048951e-05,
      "loss": 0.5936841583251953,
      "step": 11600
    },
    {
      "epoch": 16.258922323303008,
      "grad_norm": 0.5243207812309265,
      "learning_rate": 4.0502937062937065e-05,
      "loss": 0.6170587539672852,
      "step": 11625
    },
    {
      "epoch": 16.293911826452064,
      "grad_norm": 1.8138461112976074,
      "learning_rate": 4.046097902097902e-05,
      "loss": 0.5839738082885743,
      "step": 11650
    },
    {
      "epoch": 16.32890132960112,
      "grad_norm": 0.5328688621520996,
      "learning_rate": 4.041902097902098e-05,
      "loss": 0.5411964797973633,
      "step": 11675
    },
    {
      "epoch": 16.363890832750176,
      "grad_norm": 1.5153920650482178,
      "learning_rate": 4.037706293706294e-05,
      "loss": 0.5961600494384766,
      "step": 11700
    },
    {
      "epoch": 16.39888033589923,
      "grad_norm": 0.4681224822998047,
      "learning_rate": 4.0335104895104896e-05,
      "loss": 0.5467628479003906,
      "step": 11725
    },
    {
      "epoch": 16.433869839048285,
      "grad_norm": 0.3419526517391205,
      "learning_rate": 4.029314685314685e-05,
      "loss": 0.5731541061401367,
      "step": 11750
    },
    {
      "epoch": 16.46885934219734,
      "grad_norm": 0.4688303768634796,
      "learning_rate": 4.0251188811188816e-05,
      "loss": 0.60703369140625,
      "step": 11775
    },
    {
      "epoch": 16.503848845346397,
      "grad_norm": 0.7544103860855103,
      "learning_rate": 4.0209230769230765e-05,
      "loss": 0.5736282730102539,
      "step": 11800
    },
    {
      "epoch": 16.538838348495453,
      "grad_norm": 0.5120725631713867,
      "learning_rate": 4.016727272727273e-05,
      "loss": 0.5661008071899414,
      "step": 11825
    },
    {
      "epoch": 16.573827851644506,
      "grad_norm": 1.0607306957244873,
      "learning_rate": 4.0125314685314685e-05,
      "loss": 0.5902888870239258,
      "step": 11850
    },
    {
      "epoch": 16.608817354793562,
      "grad_norm": 0.35561251640319824,
      "learning_rate": 4.008335664335664e-05,
      "loss": 0.556913185119629,
      "step": 11875
    },
    {
      "epoch": 16.643806857942618,
      "grad_norm": 0.402018278837204,
      "learning_rate": 4.0041398601398604e-05,
      "loss": 0.5687739944458008,
      "step": 11900
    },
    {
      "epoch": 16.678796361091674,
      "grad_norm": 0.38593319058418274,
      "learning_rate": 3.999944055944056e-05,
      "loss": 0.5560477447509765,
      "step": 11925
    },
    {
      "epoch": 16.713785864240727,
      "grad_norm": 0.4151080548763275,
      "learning_rate": 3.9957482517482516e-05,
      "loss": 0.5510196685791016,
      "step": 11950
    },
    {
      "epoch": 16.748775367389783,
      "grad_norm": 0.6411016583442688,
      "learning_rate": 3.991552447552448e-05,
      "loss": 0.5586101150512696,
      "step": 11975
    },
    {
      "epoch": 16.78376487053884,
      "grad_norm": 2.089343309402466,
      "learning_rate": 3.9873566433566436e-05,
      "loss": 0.589087142944336,
      "step": 12000
    },
    {
      "epoch": 16.818754373687895,
      "grad_norm": 0.4637204110622406,
      "learning_rate": 3.983160839160839e-05,
      "loss": 0.587513542175293,
      "step": 12025
    },
    {
      "epoch": 16.853743876836948,
      "grad_norm": 0.6184625029563904,
      "learning_rate": 3.978965034965035e-05,
      "loss": 0.5867880630493164,
      "step": 12050
    },
    {
      "epoch": 16.888733379986004,
      "grad_norm": 0.8577840924263,
      "learning_rate": 3.9747692307692305e-05,
      "loss": 0.5885434341430664,
      "step": 12075
    },
    {
      "epoch": 16.92372288313506,
      "grad_norm": 0.4700700044631958,
      "learning_rate": 3.970573426573427e-05,
      "loss": 0.5682327651977539,
      "step": 12100
    },
    {
      "epoch": 16.958712386284116,
      "grad_norm": 1.1890414953231812,
      "learning_rate": 3.9663776223776224e-05,
      "loss": 0.576200180053711,
      "step": 12125
    },
    {
      "epoch": 16.99370188943317,
      "grad_norm": 0.3914905786514282,
      "learning_rate": 3.962181818181819e-05,
      "loss": 0.6060043334960937,
      "step": 12150
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.28268322348594666,
      "eval_mean_accuracy": 0.7938342024855257,
      "eval_mean_iou": 0.6990353215920186,
      "eval_overall_accuracy": 0.8856768562966717,
      "eval_per_category_accuracy": [
        0.906573659844546,
        0.8681048465168384,
        0.5825534295616938,
        0.589159630060461,
        0.82542951019366,
        0.9911841387359556
      ],
      "eval_per_category_iou": [
        0.7879341220009428,
        0.7279412661505075,
        0.4784601936564461,
        0.5105802909397538,
        0.7038482884922732,
        0.9854477683121885
      ],
      "eval_runtime": 29.4698,
      "eval_samples_per_second": 10.757,
      "eval_steps_per_second": 5.395,
      "step": 12155
    },
    {
      "epoch": 17.027991602519243,
      "grad_norm": 0.38832518458366394,
      "learning_rate": 3.957986013986014e-05,
      "loss": 0.5247295761108398,
      "step": 12175
    },
    {
      "epoch": 17.0629811056683,
      "grad_norm": 1.7433068752288818,
      "learning_rate": 3.95379020979021e-05,
      "loss": 0.5777214813232422,
      "step": 12200
    },
    {
      "epoch": 17.097970608817356,
      "grad_norm": 0.4525081515312195,
      "learning_rate": 3.949594405594406e-05,
      "loss": 0.6013616943359374,
      "step": 12225
    },
    {
      "epoch": 17.13296011196641,
      "grad_norm": 0.3282190263271332,
      "learning_rate": 3.945398601398601e-05,
      "loss": 0.5837851715087891,
      "step": 12250
    },
    {
      "epoch": 17.167949615115464,
      "grad_norm": 1.5490589141845703,
      "learning_rate": 3.941202797202797e-05,
      "loss": 0.598760986328125,
      "step": 12275
    },
    {
      "epoch": 17.20293911826452,
      "grad_norm": 0.5492940545082092,
      "learning_rate": 3.937006993006993e-05,
      "loss": 0.5533448791503907,
      "step": 12300
    },
    {
      "epoch": 17.237928621413577,
      "grad_norm": 0.4650823771953583,
      "learning_rate": 3.932811188811189e-05,
      "loss": 0.5626675796508789,
      "step": 12325
    },
    {
      "epoch": 17.272918124562633,
      "grad_norm": 0.8715871572494507,
      "learning_rate": 3.928615384615385e-05,
      "loss": 0.549946060180664,
      "step": 12350
    },
    {
      "epoch": 17.307907627711685,
      "grad_norm": 0.5379483699798584,
      "learning_rate": 3.924419580419581e-05,
      "loss": 0.5717860794067383,
      "step": 12375
    },
    {
      "epoch": 17.34289713086074,
      "grad_norm": 0.292766809463501,
      "learning_rate": 3.920223776223776e-05,
      "loss": 0.5697929763793945,
      "step": 12400
    },
    {
      "epoch": 17.377886634009798,
      "grad_norm": 1.777762532234192,
      "learning_rate": 3.9160279720279726e-05,
      "loss": 0.5918403244018555,
      "step": 12425
    },
    {
      "epoch": 17.412876137158854,
      "grad_norm": 0.6031855940818787,
      "learning_rate": 3.911832167832168e-05,
      "loss": 0.5752162933349609,
      "step": 12450
    },
    {
      "epoch": 17.447865640307906,
      "grad_norm": 0.47786515951156616,
      "learning_rate": 3.907636363636363e-05,
      "loss": 0.638364486694336,
      "step": 12475
    },
    {
      "epoch": 17.482855143456963,
      "grad_norm": 0.7892830967903137,
      "learning_rate": 3.9034405594405595e-05,
      "loss": 0.5888410186767579,
      "step": 12500
    },
    {
      "epoch": 17.51784464660602,
      "grad_norm": 0.5593191981315613,
      "learning_rate": 3.899244755244755e-05,
      "loss": 0.5603724288940429,
      "step": 12525
    },
    {
      "epoch": 17.552834149755075,
      "grad_norm": 0.43443650007247925,
      "learning_rate": 3.8950489510489514e-05,
      "loss": 0.5749831008911133,
      "step": 12550
    },
    {
      "epoch": 17.587823652904127,
      "grad_norm": 0.2881307303905487,
      "learning_rate": 3.890853146853147e-05,
      "loss": 0.5780796051025391,
      "step": 12575
    },
    {
      "epoch": 17.622813156053184,
      "grad_norm": 0.33581292629241943,
      "learning_rate": 3.886657342657343e-05,
      "loss": 0.5910189056396484,
      "step": 12600
    },
    {
      "epoch": 17.65780265920224,
      "grad_norm": 0.7827540040016174,
      "learning_rate": 3.882461538461539e-05,
      "loss": 0.561390151977539,
      "step": 12625
    },
    {
      "epoch": 17.692792162351296,
      "grad_norm": 0.47959861159324646,
      "learning_rate": 3.8782657342657346e-05,
      "loss": 0.5210793685913085,
      "step": 12650
    },
    {
      "epoch": 17.72778166550035,
      "grad_norm": 0.3969901204109192,
      "learning_rate": 3.87406993006993e-05,
      "loss": 0.5502013778686523,
      "step": 12675
    },
    {
      "epoch": 17.762771168649405,
      "grad_norm": 0.46583911776542664,
      "learning_rate": 3.869874125874126e-05,
      "loss": 0.6038739776611328,
      "step": 12700
    },
    {
      "epoch": 17.79776067179846,
      "grad_norm": 0.2996329963207245,
      "learning_rate": 3.8656783216783215e-05,
      "loss": 0.5930381774902344,
      "step": 12725
    },
    {
      "epoch": 17.832750174947517,
      "grad_norm": 0.563439667224884,
      "learning_rate": 3.861482517482518e-05,
      "loss": 0.5622550201416016,
      "step": 12750
    },
    {
      "epoch": 17.86773967809657,
      "grad_norm": 0.7068238258361816,
      "learning_rate": 3.8572867132867134e-05,
      "loss": 0.6130443191528321,
      "step": 12775
    },
    {
      "epoch": 17.902729181245626,
      "grad_norm": 0.5304198265075684,
      "learning_rate": 3.853090909090909e-05,
      "loss": 0.5720171737670898,
      "step": 12800
    },
    {
      "epoch": 17.93771868439468,
      "grad_norm": 0.43907809257507324,
      "learning_rate": 3.8488951048951053e-05,
      "loss": 0.5473995208740234,
      "step": 12825
    },
    {
      "epoch": 17.972708187543738,
      "grad_norm": 0.6011764407157898,
      "learning_rate": 3.844699300699301e-05,
      "loss": 0.5562837600708008,
      "step": 12850
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.281806617975235,
      "eval_mean_accuracy": 0.7927607218431701,
      "eval_mean_iou": 0.6991495259045616,
      "eval_overall_accuracy": 0.885940383285378,
      "eval_per_category_accuracy": [
        0.9095676485958458,
        0.857585664007055,
        0.5775073067708011,
        0.5846562681447338,
        0.8362198344627451,
        0.9910276090778405
      ],
      "eval_per_category_iou": [
        0.7890109250776983,
        0.7269504601062853,
        0.47723566426126124,
        0.51028946804021,
        0.7059752538679026,
        0.9854353840740122
      ],
      "eval_runtime": 29.5312,
      "eval_samples_per_second": 10.734,
      "eval_steps_per_second": 5.384,
      "step": 12870
    },
    {
      "epoch": 18.006997900629813,
      "grad_norm": 0.6865703463554382,
      "learning_rate": 3.8405034965034966e-05,
      "loss": 0.5670574188232422,
      "step": 12875
    },
    {
      "epoch": 18.041987403778865,
      "grad_norm": 0.8860803842544556,
      "learning_rate": 3.836307692307693e-05,
      "loss": 0.593165168762207,
      "step": 12900
    },
    {
      "epoch": 18.07697690692792,
      "grad_norm": 0.608953058719635,
      "learning_rate": 3.832111888111888e-05,
      "loss": 0.523445053100586,
      "step": 12925
    },
    {
      "epoch": 18.111966410076977,
      "grad_norm": 1.0774084329605103,
      "learning_rate": 3.827916083916084e-05,
      "loss": 0.5527899932861328,
      "step": 12950
    },
    {
      "epoch": 18.146955913226034,
      "grad_norm": 0.38374990224838257,
      "learning_rate": 3.82372027972028e-05,
      "loss": 0.5898138427734375,
      "step": 12975
    },
    {
      "epoch": 18.181945416375086,
      "grad_norm": 0.3852977752685547,
      "learning_rate": 3.8195244755244754e-05,
      "loss": 0.5570750427246094,
      "step": 13000
    },
    {
      "epoch": 18.216934919524142,
      "grad_norm": 0.346250057220459,
      "learning_rate": 3.815328671328672e-05,
      "loss": 0.5890644073486329,
      "step": 13025
    },
    {
      "epoch": 18.2519244226732,
      "grad_norm": 0.43694910407066345,
      "learning_rate": 3.811132867132867e-05,
      "loss": 0.5554748916625977,
      "step": 13050
    },
    {
      "epoch": 18.286913925822255,
      "grad_norm": 0.3408145606517792,
      "learning_rate": 3.806937062937063e-05,
      "loss": 0.6513704681396484,
      "step": 13075
    },
    {
      "epoch": 18.321903428971307,
      "grad_norm": 0.8809049725532532,
      "learning_rate": 3.802741258741259e-05,
      "loss": 0.6176435089111328,
      "step": 13100
    },
    {
      "epoch": 18.356892932120363,
      "grad_norm": 0.7948509454727173,
      "learning_rate": 3.798545454545454e-05,
      "loss": 0.5218618392944336,
      "step": 13125
    },
    {
      "epoch": 18.39188243526942,
      "grad_norm": 0.5662228465080261,
      "learning_rate": 3.7943496503496505e-05,
      "loss": 0.5749195098876954,
      "step": 13150
    },
    {
      "epoch": 18.426871938418476,
      "grad_norm": 0.4033019244670868,
      "learning_rate": 3.790153846153846e-05,
      "loss": 0.5471828079223633,
      "step": 13175
    },
    {
      "epoch": 18.461861441567528,
      "grad_norm": 0.6131795644760132,
      "learning_rate": 3.785958041958042e-05,
      "loss": 0.5613280868530274,
      "step": 13200
    },
    {
      "epoch": 18.496850944716584,
      "grad_norm": 0.33702948689460754,
      "learning_rate": 3.781762237762238e-05,
      "loss": 0.5641806030273437,
      "step": 13225
    },
    {
      "epoch": 18.53184044786564,
      "grad_norm": 0.8297328352928162,
      "learning_rate": 3.777566433566434e-05,
      "loss": 0.5922935485839844,
      "step": 13250
    },
    {
      "epoch": 18.566829951014697,
      "grad_norm": 0.297387033700943,
      "learning_rate": 3.773370629370629e-05,
      "loss": 0.6101275253295898,
      "step": 13275
    },
    {
      "epoch": 18.60181945416375,
      "grad_norm": 0.4003995656967163,
      "learning_rate": 3.7691748251748256e-05,
      "loss": 0.5707713317871094,
      "step": 13300
    },
    {
      "epoch": 18.636808957312805,
      "grad_norm": 0.5174612998962402,
      "learning_rate": 3.764979020979021e-05,
      "loss": 0.579169692993164,
      "step": 13325
    },
    {
      "epoch": 18.67179846046186,
      "grad_norm": 0.3972893953323364,
      "learning_rate": 3.760783216783217e-05,
      "loss": 0.6025301361083985,
      "step": 13350
    },
    {
      "epoch": 18.706787963610918,
      "grad_norm": 0.33145174384117126,
      "learning_rate": 3.7565874125874125e-05,
      "loss": 0.558909912109375,
      "step": 13375
    },
    {
      "epoch": 18.741777466759974,
      "grad_norm": 0.4641472101211548,
      "learning_rate": 3.752391608391608e-05,
      "loss": 0.5644430160522461,
      "step": 13400
    },
    {
      "epoch": 18.776766969909026,
      "grad_norm": 0.3849767744541168,
      "learning_rate": 3.7481958041958044e-05,
      "loss": 0.6011354064941407,
      "step": 13425
    },
    {
      "epoch": 18.811756473058082,
      "grad_norm": 0.27265027165412903,
      "learning_rate": 3.744e-05,
      "loss": 0.5357382202148437,
      "step": 13450
    },
    {
      "epoch": 18.84674597620714,
      "grad_norm": 1.2814220190048218,
      "learning_rate": 3.739804195804196e-05,
      "loss": 0.6005776977539062,
      "step": 13475
    },
    {
      "epoch": 18.881735479356195,
      "grad_norm": 0.38380327820777893,
      "learning_rate": 3.735608391608392e-05,
      "loss": 0.5060615921020508,
      "step": 13500
    },
    {
      "epoch": 18.916724982505247,
      "grad_norm": 0.5868950486183167,
      "learning_rate": 3.7314125874125876e-05,
      "loss": 0.571112937927246,
      "step": 13525
    },
    {
      "epoch": 18.951714485654303,
      "grad_norm": 0.579684317111969,
      "learning_rate": 3.727216783216784e-05,
      "loss": 0.5582056427001953,
      "step": 13550
    },
    {
      "epoch": 18.98670398880336,
      "grad_norm": 0.7814422845840454,
      "learning_rate": 3.723020979020979e-05,
      "loss": 0.5632222747802734,
      "step": 13575
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.2817405164241791,
      "eval_mean_accuracy": 0.7885950785609532,
      "eval_mean_iou": 0.6976960370903046,
      "eval_overall_accuracy": 0.8859460872806585,
      "eval_per_category_accuracy": [
        0.9068406222503439,
        0.8612979633407938,
        0.5568253105433395,
        0.5754513765722133,
        0.8394223933511342,
        0.9917328053078949
      ],
      "eval_per_category_iou": [
        0.7883555270505089,
        0.7268631505503467,
        0.46987646980022124,
        0.5089085191082104,
        0.7066787751690187,
        0.985493780863521
      ],
      "eval_runtime": 29.5954,
      "eval_samples_per_second": 10.711,
      "eval_steps_per_second": 5.372,
      "step": 13585
    },
    {
      "epoch": 19.020993701889434,
      "grad_norm": 0.29739174246788025,
      "learning_rate": 3.7188251748251745e-05,
      "loss": 0.5636970520019531,
      "step": 13600
    },
    {
      "epoch": 19.055983205038487,
      "grad_norm": 0.8977854251861572,
      "learning_rate": 3.7147972027972024e-05,
      "loss": 0.5651485824584961,
      "step": 13625
    },
    {
      "epoch": 19.090972708187543,
      "grad_norm": 0.43669426441192627,
      "learning_rate": 3.710601398601399e-05,
      "loss": 0.5534846115112305,
      "step": 13650
    },
    {
      "epoch": 19.1259622113366,
      "grad_norm": 0.8601847887039185,
      "learning_rate": 3.706405594405594e-05,
      "loss": 0.5592232131958008,
      "step": 13675
    },
    {
      "epoch": 19.160951714485655,
      "grad_norm": 0.41885891556739807,
      "learning_rate": 3.70220979020979e-05,
      "loss": 0.567527961730957,
      "step": 13700
    },
    {
      "epoch": 19.195941217634708,
      "grad_norm": 0.6519232392311096,
      "learning_rate": 3.698013986013986e-05,
      "loss": 0.5684947967529297,
      "step": 13725
    },
    {
      "epoch": 19.230930720783764,
      "grad_norm": 4.330150127410889,
      "learning_rate": 3.693818181818182e-05,
      "loss": 0.5488273620605468,
      "step": 13750
    },
    {
      "epoch": 19.26592022393282,
      "grad_norm": 0.37561169266700745,
      "learning_rate": 3.689622377622378e-05,
      "loss": 0.5405956649780274,
      "step": 13775
    },
    {
      "epoch": 19.300909727081876,
      "grad_norm": 0.40798985958099365,
      "learning_rate": 3.685426573426574e-05,
      "loss": 0.5901790237426758,
      "step": 13800
    },
    {
      "epoch": 19.33589923023093,
      "grad_norm": 0.4712441861629486,
      "learning_rate": 3.6812307692307694e-05,
      "loss": 0.5491345977783203,
      "step": 13825
    },
    {
      "epoch": 19.370888733379985,
      "grad_norm": 0.3724922835826874,
      "learning_rate": 3.677034965034965e-05,
      "loss": 0.5884130859375,
      "step": 13850
    },
    {
      "epoch": 19.40587823652904,
      "grad_norm": 0.364777535200119,
      "learning_rate": 3.672839160839161e-05,
      "loss": 0.5707622528076172,
      "step": 13875
    },
    {
      "epoch": 19.440867739678097,
      "grad_norm": 0.48949384689331055,
      "learning_rate": 3.668643356643356e-05,
      "loss": 0.5635111999511718,
      "step": 13900
    },
    {
      "epoch": 19.475857242827153,
      "grad_norm": 0.6178781986236572,
      "learning_rate": 3.6644475524475526e-05,
      "loss": 0.5288748168945312,
      "step": 13925
    },
    {
      "epoch": 19.510846745976206,
      "grad_norm": 0.5285311341285706,
      "learning_rate": 3.660251748251748e-05,
      "loss": 0.5662454223632812,
      "step": 13950
    },
    {
      "epoch": 19.545836249125262,
      "grad_norm": 0.665776789188385,
      "learning_rate": 3.6560559440559445e-05,
      "loss": 0.5748434066772461,
      "step": 13975
    },
    {
      "epoch": 19.58082575227432,
      "grad_norm": 0.5187867879867554,
      "learning_rate": 3.65186013986014e-05,
      "loss": 0.612128562927246,
      "step": 14000
    },
    {
      "epoch": 19.615815255423374,
      "grad_norm": 0.48636409640312195,
      "learning_rate": 3.647664335664336e-05,
      "loss": 0.5748835372924804,
      "step": 14025
    },
    {
      "epoch": 19.650804758572427,
      "grad_norm": 0.3370387554168701,
      "learning_rate": 3.643468531468532e-05,
      "loss": 0.5546784210205078,
      "step": 14050
    },
    {
      "epoch": 19.685794261721483,
      "grad_norm": 0.6854890584945679,
      "learning_rate": 3.639272727272727e-05,
      "loss": 0.5756926727294922,
      "step": 14075
    },
    {
      "epoch": 19.72078376487054,
      "grad_norm": 1.1551048755645752,
      "learning_rate": 3.635076923076923e-05,
      "loss": 0.561377944946289,
      "step": 14100
    },
    {
      "epoch": 19.755773268019595,
      "grad_norm": 0.6682814955711365,
      "learning_rate": 3.630881118881119e-05,
      "loss": 0.5815625381469727,
      "step": 14125
    },
    {
      "epoch": 19.790762771168648,
      "grad_norm": 0.24459001421928406,
      "learning_rate": 3.6266853146853146e-05,
      "loss": 0.6089716720581054,
      "step": 14150
    },
    {
      "epoch": 19.825752274317704,
      "grad_norm": 1.0141772031784058,
      "learning_rate": 3.622489510489511e-05,
      "loss": 0.5701715850830078,
      "step": 14175
    },
    {
      "epoch": 19.86074177746676,
      "grad_norm": 0.3189202845096588,
      "learning_rate": 3.6182937062937065e-05,
      "loss": 0.5705080032348633,
      "step": 14200
    },
    {
      "epoch": 19.895731280615816,
      "grad_norm": 2.2886672019958496,
      "learning_rate": 3.614097902097902e-05,
      "loss": 0.592991828918457,
      "step": 14225
    },
    {
      "epoch": 19.93072078376487,
      "grad_norm": 0.27567756175994873,
      "learning_rate": 3.6099020979020985e-05,
      "loss": 0.6036178970336914,
      "step": 14250
    },
    {
      "epoch": 19.965710286913925,
      "grad_norm": 0.6173643469810486,
      "learning_rate": 3.605706293706294e-05,
      "loss": 0.5505611038208008,
      "step": 14275
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.3950236141681671,
      "learning_rate": 3.601510489510489e-05,
      "loss": 0.6018867874145508,
      "step": 14300
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.28028586506843567,
      "eval_mean_accuracy": 0.7936854173658228,
      "eval_mean_iou": 0.6999440146443884,
      "eval_overall_accuracy": 0.886261515812919,
      "eval_per_category_accuracy": [
        0.9096183678727631,
        0.8618572637924269,
        0.5646780690446314,
        0.5985636802127529,
        0.8356264605290413,
        0.9917686627433214
      ],
      "eval_per_category_iou": [
        0.7894621655462754,
        0.7277678839807755,
        0.473995181150546,
        0.5166145004874545,
        0.7063332426080463,
        0.985491114093232
      ],
      "eval_runtime": 29.5791,
      "eval_samples_per_second": 10.717,
      "eval_steps_per_second": 5.375,
      "step": 14300
    },
    {
      "epoch": 20.034989503149056,
      "grad_norm": 0.5171719193458557,
      "learning_rate": 3.5973146853146853e-05,
      "loss": 0.5818869400024415,
      "step": 14325
    },
    {
      "epoch": 20.069979006298112,
      "grad_norm": 3.6870615482330322,
      "learning_rate": 3.593118881118881e-05,
      "loss": 0.5958518981933594,
      "step": 14350
    },
    {
      "epoch": 20.104968509447165,
      "grad_norm": 0.44435837864875793,
      "learning_rate": 3.588923076923077e-05,
      "loss": 0.5709382247924805,
      "step": 14375
    },
    {
      "epoch": 20.13995801259622,
      "grad_norm": 0.4322948157787323,
      "learning_rate": 3.584727272727273e-05,
      "loss": 0.5946118545532226,
      "step": 14400
    },
    {
      "epoch": 20.174947515745277,
      "grad_norm": 0.32745516300201416,
      "learning_rate": 3.5805314685314685e-05,
      "loss": 0.5235737609863281,
      "step": 14425
    },
    {
      "epoch": 20.209937018894333,
      "grad_norm": 0.5420677065849304,
      "learning_rate": 3.576335664335665e-05,
      "loss": 0.5657201385498047,
      "step": 14450
    },
    {
      "epoch": 20.244926522043386,
      "grad_norm": 1.9381800889968872,
      "learning_rate": 3.5721398601398605e-05,
      "loss": 0.5573284530639648,
      "step": 14475
    },
    {
      "epoch": 20.279916025192442,
      "grad_norm": 0.5331240892410278,
      "learning_rate": 3.5679440559440554e-05,
      "loss": 0.6099758911132812,
      "step": 14500
    },
    {
      "epoch": 20.314905528341498,
      "grad_norm": 0.6529404520988464,
      "learning_rate": 3.563748251748252e-05,
      "loss": 0.5873197174072265,
      "step": 14525
    },
    {
      "epoch": 20.349895031490554,
      "grad_norm": 0.6769615411758423,
      "learning_rate": 3.5595524475524473e-05,
      "loss": 0.5540350723266602,
      "step": 14550
    },
    {
      "epoch": 20.384884534639607,
      "grad_norm": 0.3948163092136383,
      "learning_rate": 3.5553566433566436e-05,
      "loss": 0.5979755401611329,
      "step": 14575
    },
    {
      "epoch": 20.419874037788663,
      "grad_norm": 0.2569188177585602,
      "learning_rate": 3.551160839160839e-05,
      "loss": 0.5739065170288086,
      "step": 14600
    },
    {
      "epoch": 20.45486354093772,
      "grad_norm": 0.4513324201107025,
      "learning_rate": 3.546965034965035e-05,
      "loss": 0.5842715072631836,
      "step": 14625
    },
    {
      "epoch": 20.489853044086775,
      "grad_norm": 1.1545759439468384,
      "learning_rate": 3.542769230769231e-05,
      "loss": 0.576529769897461,
      "step": 14650
    },
    {
      "epoch": 20.524842547235828,
      "grad_norm": 0.4825012683868408,
      "learning_rate": 3.538573426573427e-05,
      "loss": 0.5478938293457031,
      "step": 14675
    },
    {
      "epoch": 20.559832050384884,
      "grad_norm": 0.33799055218696594,
      "learning_rate": 3.5343776223776225e-05,
      "loss": 0.5700534820556641,
      "step": 14700
    },
    {
      "epoch": 20.59482155353394,
      "grad_norm": 0.34855183959007263,
      "learning_rate": 3.530181818181818e-05,
      "loss": 0.5744764328002929,
      "step": 14725
    },
    {
      "epoch": 20.629811056682996,
      "grad_norm": 0.3644197881221771,
      "learning_rate": 3.525986013986014e-05,
      "loss": 0.5273204421997071,
      "step": 14750
    },
    {
      "epoch": 20.66480055983205,
      "grad_norm": 1.25227689743042,
      "learning_rate": 3.52179020979021e-05,
      "loss": 0.5987345504760743,
      "step": 14775
    },
    {
      "epoch": 20.699790062981105,
      "grad_norm": 0.48086562752723694,
      "learning_rate": 3.5175944055944056e-05,
      "loss": 0.6074153137207031,
      "step": 14800
    },
    {
      "epoch": 20.73477956613016,
      "grad_norm": 1.39542818069458,
      "learning_rate": 3.513398601398601e-05,
      "loss": 0.5884015274047851,
      "step": 14825
    },
    {
      "epoch": 20.769769069279217,
      "grad_norm": 0.5815971493721008,
      "learning_rate": 3.5092027972027976e-05,
      "loss": 0.5675613021850586,
      "step": 14850
    },
    {
      "epoch": 20.80475857242827,
      "grad_norm": 0.3003806173801422,
      "learning_rate": 3.505006993006993e-05,
      "loss": 0.5638297653198242,
      "step": 14875
    },
    {
      "epoch": 20.839748075577326,
      "grad_norm": 4.969858646392822,
      "learning_rate": 3.500811188811189e-05,
      "loss": 0.5589989471435547,
      "step": 14900
    },
    {
      "epoch": 20.874737578726382,
      "grad_norm": 0.3755652606487274,
      "learning_rate": 3.496615384615385e-05,
      "loss": 0.5528964233398438,
      "step": 14925
    },
    {
      "epoch": 20.909727081875438,
      "grad_norm": 0.5639874339103699,
      "learning_rate": 3.49241958041958e-05,
      "loss": 0.5898004913330078,
      "step": 14950
    },
    {
      "epoch": 20.944716585024494,
      "grad_norm": 0.4691806137561798,
      "learning_rate": 3.4882237762237764e-05,
      "loss": 0.5260329818725586,
      "step": 14975
    },
    {
      "epoch": 20.979706088173547,
      "grad_norm": 0.4565242528915405,
      "learning_rate": 3.484027972027972e-05,
      "loss": 0.5167295455932617,
      "step": 15000
    },
    {
      "epoch": 21.0,
      "eval_loss": 0.2799393832683563,
      "eval_mean_accuracy": 0.7924596691854022,
      "eval_mean_iou": 0.6998484621270439,
      "eval_overall_accuracy": 0.8863901806178529,
      "eval_per_category_accuracy": [
        0.9072040109520223,
        0.8619891687210074,
        0.5628641220098133,
        0.5929882308178581,
        0.8380566512651996,
        0.9916558313465127
      ],
      "eval_per_category_iou": [
        0.7897679860569723,
        0.7279909045883419,
        0.4726274066907818,
        0.5159415117258455,
        0.7072309591960574,
        0.985532004504264
      ],
      "eval_runtime": 29.3673,
      "eval_samples_per_second": 10.794,
      "eval_steps_per_second": 5.414,
      "step": 15015
    },
    {
      "epoch": 21.01399580125962,
      "grad_norm": 0.3807743489742279,
      "learning_rate": 3.4798321678321676e-05,
      "loss": 0.5571826553344726,
      "step": 15025
    },
    {
      "epoch": 21.048985304408678,
      "grad_norm": 0.7470332384109497,
      "learning_rate": 3.475636363636364e-05,
      "loss": 0.585569190979004,
      "step": 15050
    },
    {
      "epoch": 21.083974807557734,
      "grad_norm": 0.8917853236198425,
      "learning_rate": 3.4714405594405596e-05,
      "loss": 0.5872898483276368,
      "step": 15075
    },
    {
      "epoch": 21.118964310706787,
      "grad_norm": 0.3033148944377899,
      "learning_rate": 3.467244755244755e-05,
      "loss": 0.6146905136108398,
      "step": 15100
    },
    {
      "epoch": 21.153953813855843,
      "grad_norm": 0.30546045303344727,
      "learning_rate": 3.4630489510489515e-05,
      "loss": 0.5398509216308593,
      "step": 15125
    },
    {
      "epoch": 21.1889433170049,
      "grad_norm": 0.48614171147346497,
      "learning_rate": 3.458853146853147e-05,
      "loss": 0.5278720474243164,
      "step": 15150
    },
    {
      "epoch": 21.223932820153955,
      "grad_norm": 0.411734014749527,
      "learning_rate": 3.454657342657343e-05,
      "loss": 0.5434687042236328,
      "step": 15175
    },
    {
      "epoch": 21.258922323303008,
      "grad_norm": 0.21944908797740936,
      "learning_rate": 3.4504615384615384e-05,
      "loss": 0.568606071472168,
      "step": 15200
    },
    {
      "epoch": 21.293911826452064,
      "grad_norm": 0.16119323670864105,
      "learning_rate": 3.446265734265734e-05,
      "loss": 0.5284667205810547,
      "step": 15225
    },
    {
      "epoch": 21.32890132960112,
      "grad_norm": 0.5001763105392456,
      "learning_rate": 3.44206993006993e-05,
      "loss": 0.5837395477294922,
      "step": 15250
    },
    {
      "epoch": 21.363890832750176,
      "grad_norm": 0.6098446846008301,
      "learning_rate": 3.437874125874126e-05,
      "loss": 0.552098274230957,
      "step": 15275
    },
    {
      "epoch": 21.39888033589923,
      "grad_norm": 0.45089292526245117,
      "learning_rate": 3.4336783216783216e-05,
      "loss": 0.5515509796142578,
      "step": 15300
    },
    {
      "epoch": 21.433869839048285,
      "grad_norm": 0.4414089620113373,
      "learning_rate": 3.429482517482518e-05,
      "loss": 0.6024850463867187,
      "step": 15325
    },
    {
      "epoch": 21.46885934219734,
      "grad_norm": 1.5493768453598022,
      "learning_rate": 3.4252867132867135e-05,
      "loss": 0.5866277313232422,
      "step": 15350
    },
    {
      "epoch": 21.503848845346397,
      "grad_norm": 0.4873278737068176,
      "learning_rate": 3.42109090909091e-05,
      "loss": 0.5644242477416992,
      "step": 15375
    },
    {
      "epoch": 21.538838348495453,
      "grad_norm": 2.6445720195770264,
      "learning_rate": 3.416895104895105e-05,
      "loss": 0.5708258819580078,
      "step": 15400
    },
    {
      "epoch": 21.573827851644506,
      "grad_norm": 0.6796731352806091,
      "learning_rate": 3.4126993006993004e-05,
      "loss": 0.5733988189697266,
      "step": 15425
    },
    {
      "epoch": 21.608817354793562,
      "grad_norm": 0.37853944301605225,
      "learning_rate": 3.408503496503497e-05,
      "loss": 0.5389081192016602,
      "step": 15450
    },
    {
      "epoch": 21.643806857942618,
      "grad_norm": 0.5386379957199097,
      "learning_rate": 3.404307692307692e-05,
      "loss": 0.5924673080444336,
      "step": 15475
    },
    {
      "epoch": 21.678796361091674,
      "grad_norm": 0.378481388092041,
      "learning_rate": 3.400111888111888e-05,
      "loss": 0.5475682067871094,
      "step": 15500
    },
    {
      "epoch": 21.713785864240727,
      "grad_norm": 1.807753086090088,
      "learning_rate": 3.395916083916084e-05,
      "loss": 0.5553632354736329,
      "step": 15525
    },
    {
      "epoch": 21.748775367389783,
      "grad_norm": 0.5982566475868225,
      "learning_rate": 3.39172027972028e-05,
      "loss": 0.5811807632446289,
      "step": 15550
    },
    {
      "epoch": 21.78376487053884,
      "grad_norm": 0.9326989054679871,
      "learning_rate": 3.387524475524476e-05,
      "loss": 0.5704860687255859,
      "step": 15575
    },
    {
      "epoch": 21.818754373687895,
      "grad_norm": 0.5344288945198059,
      "learning_rate": 3.383328671328672e-05,
      "loss": 0.5512649917602539,
      "step": 15600
    },
    {
      "epoch": 21.853743876836948,
      "grad_norm": 0.35058146715164185,
      "learning_rate": 3.379132867132867e-05,
      "loss": 0.5468215560913086,
      "step": 15625
    },
    {
      "epoch": 21.888733379986004,
      "grad_norm": 0.3155107796192169,
      "learning_rate": 3.374937062937063e-05,
      "loss": 0.5424984359741211,
      "step": 15650
    },
    {
      "epoch": 21.92372288313506,
      "grad_norm": 0.4132280647754669,
      "learning_rate": 3.3707412587412587e-05,
      "loss": 0.5942603302001953,
      "step": 15675
    },
    {
      "epoch": 21.958712386284116,
      "grad_norm": 1.1549105644226074,
      "learning_rate": 3.366545454545454e-05,
      "loss": 0.5591619873046875,
      "step": 15700
    },
    {
      "epoch": 21.99370188943317,
      "grad_norm": 0.38220086693763733,
      "learning_rate": 3.3623496503496506e-05,
      "loss": 0.5614230346679687,
      "step": 15725
    },
    {
      "epoch": 22.0,
      "eval_loss": 0.2788948118686676,
      "eval_mean_accuracy": 0.7951040427195538,
      "eval_mean_iou": 0.7014156664567582,
      "eval_overall_accuracy": 0.8866081598805329,
      "eval_per_category_accuracy": [
        0.911226825318156,
        0.8574159541837653,
        0.5697361562430189,
        0.6015996100058856,
        0.8392947789775745,
        0.9913509315889217
      ],
      "eval_per_category_iou": [
        0.7898364589906178,
        0.7280552648088758,
        0.47623525632672425,
        0.5212567951346321,
        0.7075875694949928,
        0.9855226539847065
      ],
      "eval_runtime": 29.9923,
      "eval_samples_per_second": 10.569,
      "eval_steps_per_second": 5.301,
      "step": 15730
    },
    {
      "epoch": 22.027991602519243,
      "grad_norm": 2.030172109603882,
      "learning_rate": 3.358153846153846e-05,
      "loss": 0.5854429626464843,
      "step": 15750
    },
    {
      "epoch": 22.0629811056683,
      "grad_norm": 0.3777494430541992,
      "learning_rate": 3.3539580419580425e-05,
      "loss": 0.5803034973144531,
      "step": 15775
    },
    {
      "epoch": 22.097970608817356,
      "grad_norm": 0.3315829932689667,
      "learning_rate": 3.349762237762238e-05,
      "loss": 0.6090323638916015,
      "step": 15800
    },
    {
      "epoch": 22.13296011196641,
      "grad_norm": 1.1404227018356323,
      "learning_rate": 3.345566433566433e-05,
      "loss": 0.5602816772460938,
      "step": 15825
    },
    {
      "epoch": 22.167949615115464,
      "grad_norm": 0.4964146912097931,
      "learning_rate": 3.3413706293706294e-05,
      "loss": 0.565955810546875,
      "step": 15850
    },
    {
      "epoch": 22.20293911826452,
      "grad_norm": 0.6614242196083069,
      "learning_rate": 3.337174825174825e-05,
      "loss": 0.5410957336425781,
      "step": 15875
    },
    {
      "epoch": 22.237928621413577,
      "grad_norm": 0.3926127552986145,
      "learning_rate": 3.3329790209790207e-05,
      "loss": 0.5108465957641601,
      "step": 15900
    },
    {
      "epoch": 22.272918124562633,
      "grad_norm": 0.46244052052497864,
      "learning_rate": 3.328783216783217e-05,
      "loss": 0.4878252029418945,
      "step": 15925
    },
    {
      "epoch": 22.307907627711685,
      "grad_norm": 0.7350902557373047,
      "learning_rate": 3.3245874125874126e-05,
      "loss": 0.5983535766601562,
      "step": 15950
    },
    {
      "epoch": 22.34289713086074,
      "grad_norm": 1.1975431442260742,
      "learning_rate": 3.320391608391609e-05,
      "loss": 0.5867250061035156,
      "step": 15975
    },
    {
      "epoch": 22.377886634009798,
      "grad_norm": 0.9520395398139954,
      "learning_rate": 3.3161958041958045e-05,
      "loss": 0.5561068725585937,
      "step": 16000
    },
    {
      "epoch": 22.412876137158854,
      "grad_norm": 0.4124087989330292,
      "learning_rate": 3.312e-05,
      "loss": 0.5450659561157226,
      "step": 16025
    },
    {
      "epoch": 22.447865640307906,
      "grad_norm": 0.4317605495452881,
      "learning_rate": 3.307804195804196e-05,
      "loss": 0.5418532562255859,
      "step": 16050
    },
    {
      "epoch": 22.482855143456963,
      "grad_norm": 2.385840654373169,
      "learning_rate": 3.3036083916083914e-05,
      "loss": 0.5288919830322265,
      "step": 16075
    },
    {
      "epoch": 22.51784464660602,
      "grad_norm": 0.4479973018169403,
      "learning_rate": 3.299412587412587e-05,
      "loss": 0.5964275741577149,
      "step": 16100
    },
    {
      "epoch": 22.552834149755075,
      "grad_norm": 0.44738006591796875,
      "learning_rate": 3.295216783216783e-05,
      "loss": 0.5191995620727539,
      "step": 16125
    },
    {
      "epoch": 22.587823652904127,
      "grad_norm": 0.7048130631446838,
      "learning_rate": 3.291020979020979e-05,
      "loss": 0.5349803924560547,
      "step": 16150
    },
    {
      "epoch": 22.622813156053184,
      "grad_norm": 0.3139331042766571,
      "learning_rate": 3.286825174825175e-05,
      "loss": 0.6219949722290039,
      "step": 16175
    },
    {
      "epoch": 22.65780265920224,
      "grad_norm": 0.40937337279319763,
      "learning_rate": 3.282629370629371e-05,
      "loss": 0.6087961196899414,
      "step": 16200
    },
    {
      "epoch": 22.692792162351296,
      "grad_norm": 0.6007043123245239,
      "learning_rate": 3.2784335664335665e-05,
      "loss": 0.5596650314331054,
      "step": 16225
    },
    {
      "epoch": 22.72778166550035,
      "grad_norm": 0.29498037695884705,
      "learning_rate": 3.274237762237763e-05,
      "loss": 0.555514907836914,
      "step": 16250
    },
    {
      "epoch": 22.762771168649405,
      "grad_norm": 0.917547881603241,
      "learning_rate": 3.270041958041958e-05,
      "loss": 0.5930567550659179,
      "step": 16275
    },
    {
      "epoch": 22.79776067179846,
      "grad_norm": 0.7907842397689819,
      "learning_rate": 3.2658461538461534e-05,
      "loss": 0.5613677215576172,
      "step": 16300
    },
    {
      "epoch": 22.832750174947517,
      "grad_norm": 0.2896110415458679,
      "learning_rate": 3.26165034965035e-05,
      "loss": 0.6035576248168946,
      "step": 16325
    },
    {
      "epoch": 22.86773967809657,
      "grad_norm": 0.4866832196712494,
      "learning_rate": 3.257454545454545e-05,
      "loss": 0.5409349822998046,
      "step": 16350
    },
    {
      "epoch": 22.902729181245626,
      "grad_norm": 0.6143618822097778,
      "learning_rate": 3.2532587412587416e-05,
      "loss": 0.576870994567871,
      "step": 16375
    },
    {
      "epoch": 22.93771868439468,
      "grad_norm": 0.28665703535079956,
      "learning_rate": 3.249062937062937e-05,
      "loss": 0.5957161331176758,
      "step": 16400
    },
    {
      "epoch": 22.972708187543738,
      "grad_norm": 0.3836901783943176,
      "learning_rate": 3.244867132867133e-05,
      "loss": 0.5675749206542968,
      "step": 16425
    },
    {
      "epoch": 23.0,
      "eval_loss": 0.2785665690898895,
      "eval_mean_accuracy": 0.7966671050236162,
      "eval_mean_iou": 0.7024944210244587,
      "eval_overall_accuracy": 0.8868379923823504,
      "eval_per_category_accuracy": [
        0.9102786731884894,
        0.8586289495515468,
        0.5774749850132403,
        0.6050368294746772,
        0.8371992406109534,
        0.99138395230279
      ],
      "eval_per_category_iou": [
        0.7908629054257429,
        0.7285773425209293,
        0.4797663232014736,
        0.5227284595188506,
        0.7074839468134742,
        0.9855475486662819
      ],
      "eval_runtime": 28.437,
      "eval_samples_per_second": 11.147,
      "eval_steps_per_second": 5.591,
      "step": 16445
    },
    {
      "epoch": 23.006997900629813,
      "grad_norm": 0.537223219871521,
      "learning_rate": 3.240671328671329e-05,
      "loss": 0.577944450378418,
      "step": 16450
    },
    {
      "epoch": 23.041987403778865,
      "grad_norm": 2.723468065261841,
      "learning_rate": 3.236475524475525e-05,
      "loss": 0.6181957626342773,
      "step": 16475
    },
    {
      "epoch": 23.07697690692792,
      "grad_norm": 0.45068711042404175,
      "learning_rate": 3.23227972027972e-05,
      "loss": 0.5289567565917969,
      "step": 16500
    },
    {
      "epoch": 23.111966410076977,
      "grad_norm": 0.30388593673706055,
      "learning_rate": 3.228083916083916e-05,
      "loss": 0.543092041015625,
      "step": 16525
    },
    {
      "epoch": 23.146955913226034,
      "grad_norm": 0.6885831356048584,
      "learning_rate": 3.223888111888112e-05,
      "loss": 0.5873026275634765,
      "step": 16550
    },
    {
      "epoch": 23.181945416375086,
      "grad_norm": 0.47321876883506775,
      "learning_rate": 3.219692307692308e-05,
      "loss": 0.5494138336181641,
      "step": 16575
    },
    {
      "epoch": 23.216934919524142,
      "grad_norm": 0.3723224401473999,
      "learning_rate": 3.2154965034965036e-05,
      "loss": 0.5847332382202148,
      "step": 16600
    },
    {
      "epoch": 23.2519244226732,
      "grad_norm": 0.7309121489524841,
      "learning_rate": 3.211300699300699e-05,
      "loss": 0.5464167404174805,
      "step": 16625
    },
    {
      "epoch": 23.286913925822255,
      "grad_norm": 0.3070921301841736,
      "learning_rate": 3.2071048951048955e-05,
      "loss": 0.5674298858642578,
      "step": 16650
    },
    {
      "epoch": 23.321903428971307,
      "grad_norm": 0.496019184589386,
      "learning_rate": 3.202909090909091e-05,
      "loss": 0.5496016693115234,
      "step": 16675
    },
    {
      "epoch": 23.356892932120363,
      "grad_norm": 0.2505526840686798,
      "learning_rate": 3.198713286713287e-05,
      "loss": 0.5446410751342774,
      "step": 16700
    },
    {
      "epoch": 23.39188243526942,
      "grad_norm": 0.6181175112724304,
      "learning_rate": 3.1945174825174824e-05,
      "loss": 0.529984130859375,
      "step": 16725
    },
    {
      "epoch": 23.426871938418476,
      "grad_norm": 0.5987983345985413,
      "learning_rate": 3.190321678321678e-05,
      "loss": 0.584999771118164,
      "step": 16750
    },
    {
      "epoch": 23.461861441567528,
      "grad_norm": 0.5591376423835754,
      "learning_rate": 3.186293706293706e-05,
      "loss": 0.6425630950927734,
      "step": 16775
    },
    {
      "epoch": 23.496850944716584,
      "grad_norm": 0.4561966061592102,
      "learning_rate": 3.182097902097902e-05,
      "loss": 0.5677362823486328,
      "step": 16800
    },
    {
      "epoch": 23.53184044786564,
      "grad_norm": 0.3122384548187256,
      "learning_rate": 3.177902097902098e-05,
      "loss": 0.5677817916870117,
      "step": 16825
    },
    {
      "epoch": 23.566829951014697,
      "grad_norm": 0.8882079720497131,
      "learning_rate": 3.1737062937062935e-05,
      "loss": 0.5281692123413086,
      "step": 16850
    },
    {
      "epoch": 23.60181945416375,
      "grad_norm": 0.6044714450836182,
      "learning_rate": 3.16951048951049e-05,
      "loss": 0.5783490371704102,
      "step": 16875
    },
    {
      "epoch": 23.636808957312805,
      "grad_norm": 0.5700604915618896,
      "learning_rate": 3.1653146853146854e-05,
      "loss": 0.5621581268310547,
      "step": 16900
    },
    {
      "epoch": 23.67179846046186,
      "grad_norm": 0.684579610824585,
      "learning_rate": 3.161118881118881e-05,
      "loss": 0.6061162567138672,
      "step": 16925
    },
    {
      "epoch": 23.706787963610918,
      "grad_norm": 0.30952492356300354,
      "learning_rate": 3.1569230769230773e-05,
      "loss": 0.5754029083251954,
      "step": 16950
    },
    {
      "epoch": 23.741777466759974,
      "grad_norm": 0.344820499420166,
      "learning_rate": 3.152727272727273e-05,
      "loss": 0.5599665069580078,
      "step": 16975
    },
    {
      "epoch": 23.776766969909026,
      "grad_norm": 3.6808362007141113,
      "learning_rate": 3.1485314685314686e-05,
      "loss": 0.5403440093994141,
      "step": 17000
    },
    {
      "epoch": 23.811756473058082,
      "grad_norm": 0.7369239926338196,
      "learning_rate": 3.144335664335664e-05,
      "loss": 0.5473520278930664,
      "step": 17025
    },
    {
      "epoch": 23.84674597620714,
      "grad_norm": 0.5437964797019958,
      "learning_rate": 3.14013986013986e-05,
      "loss": 0.5822623062133789,
      "step": 17050
    },
    {
      "epoch": 23.881735479356195,
      "grad_norm": 0.31687992811203003,
      "learning_rate": 3.135944055944056e-05,
      "loss": 0.5628582000732422,
      "step": 17075
    },
    {
      "epoch": 23.916724982505247,
      "grad_norm": 0.4586983323097229,
      "learning_rate": 3.131748251748252e-05,
      "loss": 0.5742436981201172,
      "step": 17100
    },
    {
      "epoch": 23.951714485654303,
      "grad_norm": 0.5213825702667236,
      "learning_rate": 3.1275524475524474e-05,
      "loss": 0.5246884536743164,
      "step": 17125
    },
    {
      "epoch": 23.98670398880336,
      "grad_norm": 1.8079255819320679,
      "learning_rate": 3.123356643356644e-05,
      "loss": 0.5623202133178711,
      "step": 17150
    },
    {
      "epoch": 24.0,
      "eval_loss": 0.27780261635780334,
      "eval_mean_accuracy": 0.79620273952101,
      "eval_mean_iou": 0.7027762989704932,
      "eval_overall_accuracy": 0.8870801955743542,
      "eval_per_category_accuracy": [
        0.9097477318637711,
        0.8636659512394588,
        0.5734095864904339,
        0.6039379893504656,
        0.8346450801798933,
        0.9918100980020365
      ],
      "eval_per_category_iou": [
        0.7912007162246343,
        0.7290338400201929,
        0.4798204301297,
        0.5234006158644342,
        0.7076372542939612,
        0.9855649372900366
      ],
      "eval_runtime": 28.5943,
      "eval_samples_per_second": 11.086,
      "eval_steps_per_second": 5.561,
      "step": 17160
    },
    {
      "epoch": 24.020993701889434,
      "grad_norm": 0.49638158082962036,
      "learning_rate": 3.1191608391608393e-05,
      "loss": 0.5546018600463867,
      "step": 17175
    },
    {
      "epoch": 24.055983205038487,
      "grad_norm": 0.37870630621910095,
      "learning_rate": 3.1149650349650356e-05,
      "loss": 0.584622688293457,
      "step": 17200
    },
    {
      "epoch": 24.090972708187543,
      "grad_norm": 0.750354528427124,
      "learning_rate": 3.1107692307692306e-05,
      "loss": 0.5727369689941406,
      "step": 17225
    },
    {
      "epoch": 24.1259622113366,
      "grad_norm": 0.5410414934158325,
      "learning_rate": 3.106573426573426e-05,
      "loss": 0.5572652816772461,
      "step": 17250
    },
    {
      "epoch": 24.160951714485655,
      "grad_norm": 0.4911649525165558,
      "learning_rate": 3.1023776223776225e-05,
      "loss": 0.592033920288086,
      "step": 17275
    },
    {
      "epoch": 24.195941217634708,
      "grad_norm": 0.23285941779613495,
      "learning_rate": 3.098181818181818e-05,
      "loss": 0.5170295333862305,
      "step": 17300
    },
    {
      "epoch": 24.230930720783764,
      "grad_norm": 0.4869098663330078,
      "learning_rate": 3.093986013986014e-05,
      "loss": 0.5712868881225586,
      "step": 17325
    },
    {
      "epoch": 24.26592022393282,
      "grad_norm": 0.3893488049507141,
      "learning_rate": 3.08979020979021e-05,
      "loss": 0.5505056762695313,
      "step": 17350
    },
    {
      "epoch": 24.300909727081876,
      "grad_norm": 3.4205338954925537,
      "learning_rate": 3.085594405594406e-05,
      "loss": 0.505828742980957,
      "step": 17375
    },
    {
      "epoch": 24.33589923023093,
      "grad_norm": 0.4278641939163208,
      "learning_rate": 3.081398601398602e-05,
      "loss": 0.5562249755859375,
      "step": 17400
    },
    {
      "epoch": 24.370888733379985,
      "grad_norm": 0.7006653547286987,
      "learning_rate": 3.077202797202797e-05,
      "loss": 0.5707674026489258,
      "step": 17425
    },
    {
      "epoch": 24.40587823652904,
      "grad_norm": 0.4066488742828369,
      "learning_rate": 3.0730069930069926e-05,
      "loss": 0.579607162475586,
      "step": 17450
    },
    {
      "epoch": 24.440867739678097,
      "grad_norm": 0.5957182049751282,
      "learning_rate": 3.068811188811189e-05,
      "loss": 0.5597539901733398,
      "step": 17475
    },
    {
      "epoch": 24.475857242827153,
      "grad_norm": 0.32763752341270447,
      "learning_rate": 3.0646153846153845e-05,
      "loss": 0.5396363067626954,
      "step": 17500
    },
    {
      "epoch": 24.510846745976206,
      "grad_norm": 0.47591304779052734,
      "learning_rate": 3.06041958041958e-05,
      "loss": 0.5722558975219727,
      "step": 17525
    },
    {
      "epoch": 24.545836249125262,
      "grad_norm": 1.0978981256484985,
      "learning_rate": 3.0562237762237764e-05,
      "loss": 0.5991012573242187,
      "step": 17550
    },
    {
      "epoch": 24.58082575227432,
      "grad_norm": 0.3963519036769867,
      "learning_rate": 3.052027972027972e-05,
      "loss": 0.6005728912353515,
      "step": 17575
    },
    {
      "epoch": 24.615815255423374,
      "grad_norm": 0.5959064364433289,
      "learning_rate": 3.047832167832168e-05,
      "loss": 0.5170811080932617,
      "step": 17600
    },
    {
      "epoch": 24.650804758572427,
      "grad_norm": 0.3720378279685974,
      "learning_rate": 3.043636363636364e-05,
      "loss": 0.5533251190185546,
      "step": 17625
    },
    {
      "epoch": 24.685794261721483,
      "grad_norm": 0.31696584820747375,
      "learning_rate": 3.0394405594405593e-05,
      "loss": 0.565619125366211,
      "step": 17650
    },
    {
      "epoch": 24.72078376487054,
      "grad_norm": 0.5716512799263,
      "learning_rate": 3.0352447552447553e-05,
      "loss": 0.5439013671875,
      "step": 17675
    },
    {
      "epoch": 24.755773268019595,
      "grad_norm": 0.8153748512268066,
      "learning_rate": 3.031048951048951e-05,
      "loss": 0.5335529708862304,
      "step": 17700
    },
    {
      "epoch": 24.790762771168648,
      "grad_norm": 12.914260864257812,
      "learning_rate": 3.026853146853147e-05,
      "loss": 0.6103602600097656,
      "step": 17725
    },
    {
      "epoch": 24.825752274317704,
      "grad_norm": 0.5792195796966553,
      "learning_rate": 3.0226573426573428e-05,
      "loss": 0.6341778182983399,
      "step": 17750
    },
    {
      "epoch": 24.86074177746676,
      "grad_norm": 0.44542619585990906,
      "learning_rate": 3.0184615384615384e-05,
      "loss": 0.5441202163696289,
      "step": 17775
    },
    {
      "epoch": 24.895731280615816,
      "grad_norm": 0.4239818751811981,
      "learning_rate": 3.0142657342657344e-05,
      "loss": 0.5794084167480469,
      "step": 17800
    },
    {
      "epoch": 24.93072078376487,
      "grad_norm": 0.5524073243141174,
      "learning_rate": 3.0100699300699304e-05,
      "loss": 0.5686959838867187,
      "step": 17825
    },
    {
      "epoch": 24.965710286913925,
      "grad_norm": 1.0662273168563843,
      "learning_rate": 3.0058741258741263e-05,
      "loss": 0.551431884765625,
      "step": 17850
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.9059902429580688,
      "learning_rate": 3.0016783216783216e-05,
      "loss": 0.5461558532714844,
      "step": 17875
    },
    {
      "epoch": 25.0,
      "eval_loss": 0.27721336483955383,
      "eval_mean_accuracy": 0.7994636252384874,
      "eval_mean_iou": 0.7044146391123297,
      "eval_overall_accuracy": 0.8871546002216519,
      "eval_per_category_accuracy": [
        0.9056218081911512,
        0.8585882592365248,
        0.589274390828621,
        0.6154505442677008,
        0.8361198082818033,
        0.9917269406251229
      ],
      "eval_per_category_iou": [
        0.791629255638151,
        0.7286325235869459,
        0.4852250510530575,
        0.527492153106147,
        0.7079334312795221,
        0.9855754200101552
      ],
      "eval_runtime": 28.5093,
      "eval_samples_per_second": 11.119,
      "eval_steps_per_second": 5.577,
      "step": 17875
    },
    {
      "epoch": 25.034989503149056,
      "grad_norm": 0.44541728496551514,
      "learning_rate": 2.9974825174825176e-05,
      "loss": 0.5462105178833008,
      "step": 17900
    },
    {
      "epoch": 25.069979006298112,
      "grad_norm": 0.2674436569213867,
      "learning_rate": 2.9932867132867132e-05,
      "loss": 0.5397047805786133,
      "step": 17925
    },
    {
      "epoch": 25.104968509447165,
      "grad_norm": 0.3658898174762726,
      "learning_rate": 2.9890909090909092e-05,
      "loss": 0.5218799209594727,
      "step": 17950
    },
    {
      "epoch": 25.13995801259622,
      "grad_norm": 2.766433000564575,
      "learning_rate": 2.984895104895105e-05,
      "loss": 0.5849544906616211,
      "step": 17975
    },
    {
      "epoch": 25.174947515745277,
      "grad_norm": 0.40384677052497864,
      "learning_rate": 2.9806993006993008e-05,
      "loss": 0.5604112243652344,
      "step": 18000
    },
    {
      "epoch": 25.209937018894333,
      "grad_norm": 0.6153871417045593,
      "learning_rate": 2.9765034965034967e-05,
      "loss": 0.5585441589355469,
      "step": 18025
    },
    {
      "epoch": 25.244926522043386,
      "grad_norm": 0.4403422772884369,
      "learning_rate": 2.9723076923076924e-05,
      "loss": 0.5563193893432617,
      "step": 18050
    },
    {
      "epoch": 25.279916025192442,
      "grad_norm": 1.5490360260009766,
      "learning_rate": 2.9681118881118883e-05,
      "loss": 0.5783559799194335,
      "step": 18075
    },
    {
      "epoch": 25.314905528341498,
      "grad_norm": 0.5505168437957764,
      "learning_rate": 2.963916083916084e-05,
      "loss": 0.5840619659423828,
      "step": 18100
    },
    {
      "epoch": 25.349895031490554,
      "grad_norm": 0.2942952513694763,
      "learning_rate": 2.95972027972028e-05,
      "loss": 0.6026663208007812,
      "step": 18125
    },
    {
      "epoch": 25.384884534639607,
      "grad_norm": 3.490443706512451,
      "learning_rate": 2.9555244755244755e-05,
      "loss": 0.5473076629638672,
      "step": 18150
    },
    {
      "epoch": 25.419874037788663,
      "grad_norm": 0.4838099777698517,
      "learning_rate": 2.9513286713286715e-05,
      "loss": 0.5451699066162109,
      "step": 18175
    },
    {
      "epoch": 25.45486354093772,
      "grad_norm": 0.2646855413913727,
      "learning_rate": 2.947132867132867e-05,
      "loss": 0.5275799179077149,
      "step": 18200
    },
    {
      "epoch": 25.489853044086775,
      "grad_norm": 0.2940528988838196,
      "learning_rate": 2.942937062937063e-05,
      "loss": 0.5282382583618164,
      "step": 18225
    },
    {
      "epoch": 25.524842547235828,
      "grad_norm": 0.22760014235973358,
      "learning_rate": 2.9387412587412587e-05,
      "loss": 0.588048210144043,
      "step": 18250
    },
    {
      "epoch": 25.559832050384884,
      "grad_norm": 0.6411617398262024,
      "learning_rate": 2.9345454545454547e-05,
      "loss": 0.5799164199829101,
      "step": 18275
    },
    {
      "epoch": 25.59482155353394,
      "grad_norm": 0.3649356961250305,
      "learning_rate": 2.9303496503496503e-05,
      "loss": 0.5454291915893554,
      "step": 18300
    },
    {
      "epoch": 25.629811056682996,
      "grad_norm": 0.8052265048027039,
      "learning_rate": 2.9261538461538463e-05,
      "loss": 0.6217594528198243,
      "step": 18325
    },
    {
      "epoch": 25.66480055983205,
      "grad_norm": 0.45395785570144653,
      "learning_rate": 2.9219580419580423e-05,
      "loss": 0.611798095703125,
      "step": 18350
    },
    {
      "epoch": 25.699790062981105,
      "grad_norm": 1.0482121706008911,
      "learning_rate": 2.917762237762238e-05,
      "loss": 0.5588851547241211,
      "step": 18375
    },
    {
      "epoch": 25.73477956613016,
      "grad_norm": 0.40785524249076843,
      "learning_rate": 2.9135664335664335e-05,
      "loss": 0.5577195358276367,
      "step": 18400
    },
    {
      "epoch": 25.769769069279217,
      "grad_norm": 0.40602317452430725,
      "learning_rate": 2.9093706293706295e-05,
      "loss": 0.5700567626953125,
      "step": 18425
    },
    {
      "epoch": 25.80475857242827,
      "grad_norm": 0.3175012767314911,
      "learning_rate": 2.9051748251748254e-05,
      "loss": 0.5757289505004883,
      "step": 18450
    },
    {
      "epoch": 25.839748075577326,
      "grad_norm": 0.7791935205459595,
      "learning_rate": 2.900979020979021e-05,
      "loss": 0.5258735275268555,
      "step": 18475
    },
    {
      "epoch": 25.874737578726382,
      "grad_norm": 0.7135423421859741,
      "learning_rate": 2.8967832167832167e-05,
      "loss": 0.6136598587036133,
      "step": 18500
    },
    {
      "epoch": 25.909727081875438,
      "grad_norm": 0.8931930661201477,
      "learning_rate": 2.8925874125874127e-05,
      "loss": 0.5301474380493164,
      "step": 18525
    },
    {
      "epoch": 25.944716585024494,
      "grad_norm": 0.35491156578063965,
      "learning_rate": 2.8883916083916086e-05,
      "loss": 0.5413616943359375,
      "step": 18550
    },
    {
      "epoch": 25.979706088173547,
      "grad_norm": 0.4139886498451233,
      "learning_rate": 2.8841958041958046e-05,
      "loss": 0.541998405456543,
      "step": 18575
    },
    {
      "epoch": 26.0,
      "eval_loss": 0.27637916803359985,
      "eval_mean_accuracy": 0.7983806704378832,
      "eval_mean_iou": 0.7045230777985795,
      "eval_overall_accuracy": 0.8874278625103201,
      "eval_per_category_accuracy": [
        0.9085414484494639,
        0.8622537440978433,
        0.5875200872580311,
        0.6064061685654439,
        0.8338294820891379,
        0.9917330921673783
      ],
      "eval_per_category_iou": [
        0.7920063276581308,
        0.7295374633561741,
        0.4856679005877917,
        0.526272658480377,
        0.7080810435515099,
        0.985573073157493
      ],
      "eval_runtime": 28.4636,
      "eval_samples_per_second": 11.137,
      "eval_steps_per_second": 5.586,
      "step": 18590
    },
    {
      "epoch": 26.01399580125962,
      "grad_norm": 0.5699747204780579,
      "learning_rate": 2.88e-05,
      "loss": 0.5618587112426758,
      "step": 18600
    },
    {
      "epoch": 26.048985304408678,
      "grad_norm": 0.43168407678604126,
      "learning_rate": 2.875804195804196e-05,
      "loss": 0.5746206283569336,
      "step": 18625
    },
    {
      "epoch": 26.083974807557734,
      "grad_norm": 0.4077194929122925,
      "learning_rate": 2.8716083916083918e-05,
      "loss": 0.547454833984375,
      "step": 18650
    },
    {
      "epoch": 26.118964310706787,
      "grad_norm": 0.5129786133766174,
      "learning_rate": 2.8674125874125878e-05,
      "loss": 0.5426398468017578,
      "step": 18675
    },
    {
      "epoch": 26.153953813855843,
      "grad_norm": 0.6982755064964294,
      "learning_rate": 2.863216783216783e-05,
      "loss": 0.5892229843139648,
      "step": 18700
    },
    {
      "epoch": 26.1889433170049,
      "grad_norm": 1.4428209066390991,
      "learning_rate": 2.859020979020979e-05,
      "loss": 0.5728329467773438,
      "step": 18725
    },
    {
      "epoch": 26.223932820153955,
      "grad_norm": 0.4085122346878052,
      "learning_rate": 2.854825174825175e-05,
      "loss": 0.5466020202636719,
      "step": 18750
    },
    {
      "epoch": 26.258922323303008,
      "grad_norm": 0.4091956317424774,
      "learning_rate": 2.850629370629371e-05,
      "loss": 0.5694407653808594,
      "step": 18775
    },
    {
      "epoch": 26.293911826452064,
      "grad_norm": 0.3895551264286041,
      "learning_rate": 2.8464335664335666e-05,
      "loss": 0.5661583328247071,
      "step": 18800
    },
    {
      "epoch": 26.32890132960112,
      "grad_norm": 0.4605601727962494,
      "learning_rate": 2.8422377622377622e-05,
      "loss": 0.552859992980957,
      "step": 18825
    },
    {
      "epoch": 26.363890832750176,
      "grad_norm": 0.30992287397384644,
      "learning_rate": 2.838041958041958e-05,
      "loss": 0.5698759078979492,
      "step": 18850
    },
    {
      "epoch": 26.39888033589923,
      "grad_norm": 0.24274572730064392,
      "learning_rate": 2.833846153846154e-05,
      "loss": 0.5571882629394531,
      "step": 18875
    },
    {
      "epoch": 26.433869839048285,
      "grad_norm": 1.021414875984192,
      "learning_rate": 2.8296503496503498e-05,
      "loss": 0.5800272750854493,
      "step": 18900
    },
    {
      "epoch": 26.46885934219734,
      "grad_norm": 0.2576630115509033,
      "learning_rate": 2.8254545454545454e-05,
      "loss": 0.5867700958251953,
      "step": 18925
    },
    {
      "epoch": 26.503848845346397,
      "grad_norm": 0.4780209958553314,
      "learning_rate": 2.8212587412587413e-05,
      "loss": 0.5783476638793945,
      "step": 18950
    },
    {
      "epoch": 26.538838348495453,
      "grad_norm": 1.1921865940093994,
      "learning_rate": 2.8170629370629373e-05,
      "loss": 0.5335665893554687,
      "step": 18975
    },
    {
      "epoch": 26.573827851644506,
      "grad_norm": 0.24230727553367615,
      "learning_rate": 2.812867132867133e-05,
      "loss": 0.5330426788330078,
      "step": 19000
    },
    {
      "epoch": 26.608817354793562,
      "grad_norm": 0.5476073622703552,
      "learning_rate": 2.8086713286713286e-05,
      "loss": 0.5086902999877929,
      "step": 19025
    },
    {
      "epoch": 26.643806857942618,
      "grad_norm": 1.2929476499557495,
      "learning_rate": 2.8044755244755245e-05,
      "loss": 0.5644321060180664,
      "step": 19050
    },
    {
      "epoch": 26.678796361091674,
      "grad_norm": 0.35012221336364746,
      "learning_rate": 2.8002797202797205e-05,
      "loss": 0.554288101196289,
      "step": 19075
    },
    {
      "epoch": 26.713785864240727,
      "grad_norm": 0.3605135977268219,
      "learning_rate": 2.796083916083916e-05,
      "loss": 0.578389892578125,
      "step": 19100
    },
    {
      "epoch": 26.748775367389783,
      "grad_norm": 1.0425587892532349,
      "learning_rate": 2.792055944055944e-05,
      "loss": 0.6064880752563476,
      "step": 19125
    },
    {
      "epoch": 26.78376487053884,
      "grad_norm": 0.2697213292121887,
      "learning_rate": 2.78786013986014e-05,
      "loss": 0.5727441787719727,
      "step": 19150
    },
    {
      "epoch": 26.818754373687895,
      "grad_norm": 0.235168918967247,
      "learning_rate": 2.783664335664336e-05,
      "loss": 0.5853600311279297,
      "step": 19175
    },
    {
      "epoch": 26.853743876836948,
      "grad_norm": 0.6382473111152649,
      "learning_rate": 2.7794685314685316e-05,
      "loss": 0.56558837890625,
      "step": 19200
    },
    {
      "epoch": 26.888733379986004,
      "grad_norm": 7.96755838394165,
      "learning_rate": 2.7752727272727272e-05,
      "loss": 0.5528057861328125,
      "step": 19225
    },
    {
      "epoch": 26.92372288313506,
      "grad_norm": 0.2965751588344574,
      "learning_rate": 2.771076923076923e-05,
      "loss": 0.55046142578125,
      "step": 19250
    },
    {
      "epoch": 26.958712386284116,
      "grad_norm": 2.8204779624938965,
      "learning_rate": 2.766881118881119e-05,
      "loss": 0.5560384750366211,
      "step": 19275
    },
    {
      "epoch": 26.99370188943317,
      "grad_norm": 0.19212861359119415,
      "learning_rate": 2.7626853146853147e-05,
      "loss": 0.5526444244384766,
      "step": 19300
    },
    {
      "epoch": 27.0,
      "eval_loss": 0.2763121724128723,
      "eval_mean_accuracy": 0.7985769567544058,
      "eval_mean_iou": 0.7048975176118928,
      "eval_overall_accuracy": 0.8875483925034195,
      "eval_per_category_accuracy": [
        0.9063117096967023,
        0.8704153615537646,
        0.5841171954785969,
        0.6097294415831225,
        0.8293144339196474,
        0.9915735982946012
      ],
      "eval_per_category_iou": [
        0.7921750495735268,
        0.7302024400298675,
        0.4856703027095936,
        0.5280483901554969,
        0.7076896919670499,
        0.9855992312358223
      ],
      "eval_runtime": 28.7136,
      "eval_samples_per_second": 11.04,
      "eval_steps_per_second": 5.537,
      "step": 19305
    },
    {
      "epoch": 27.027991602519243,
      "grad_norm": 0.4702194035053253,
      "learning_rate": 2.7584895104895104e-05,
      "loss": 0.5641994476318359,
      "step": 19325
    },
    {
      "epoch": 27.0629811056683,
      "grad_norm": 0.9248610734939575,
      "learning_rate": 2.7542937062937063e-05,
      "loss": 0.5618873977661133,
      "step": 19350
    },
    {
      "epoch": 27.097970608817356,
      "grad_norm": 0.2937209904193878,
      "learning_rate": 2.7500979020979023e-05,
      "loss": 0.5581065750122071,
      "step": 19375
    },
    {
      "epoch": 27.13296011196641,
      "grad_norm": 0.693684995174408,
      "learning_rate": 2.7459020979020983e-05,
      "loss": 0.5442930984497071,
      "step": 19400
    },
    {
      "epoch": 27.167949615115464,
      "grad_norm": 0.47299817204475403,
      "learning_rate": 2.7417062937062936e-05,
      "loss": 0.5631307601928711,
      "step": 19425
    },
    {
      "epoch": 27.20293911826452,
      "grad_norm": 0.5829184055328369,
      "learning_rate": 2.7375104895104895e-05,
      "loss": 0.5607353210449219,
      "step": 19450
    },
    {
      "epoch": 27.237928621413577,
      "grad_norm": 0.5704228281974792,
      "learning_rate": 2.7333146853146855e-05,
      "loss": 0.5449817276000977,
      "step": 19475
    },
    {
      "epoch": 27.272918124562633,
      "grad_norm": 0.6070555448532104,
      "learning_rate": 2.7291188811188815e-05,
      "loss": 0.5683136367797852,
      "step": 19500
    },
    {
      "epoch": 27.307907627711685,
      "grad_norm": 0.339466392993927,
      "learning_rate": 2.7249230769230767e-05,
      "loss": 0.5498379516601563,
      "step": 19525
    },
    {
      "epoch": 27.34289713086074,
      "grad_norm": 0.5034247636795044,
      "learning_rate": 2.7207272727272727e-05,
      "loss": 0.557393684387207,
      "step": 19550
    },
    {
      "epoch": 27.377886634009798,
      "grad_norm": 1.483312964439392,
      "learning_rate": 2.7165314685314687e-05,
      "loss": 0.5672755813598633,
      "step": 19575
    },
    {
      "epoch": 27.412876137158854,
      "grad_norm": 0.2823740243911743,
      "learning_rate": 2.7123356643356646e-05,
      "loss": 0.56723876953125,
      "step": 19600
    },
    {
      "epoch": 27.447865640307906,
      "grad_norm": 0.6775507926940918,
      "learning_rate": 2.7081398601398603e-05,
      "loss": 0.5908587646484375,
      "step": 19625
    },
    {
      "epoch": 27.482855143456963,
      "grad_norm": 0.30459389090538025,
      "learning_rate": 2.703944055944056e-05,
      "loss": 0.5206967544555664,
      "step": 19650
    },
    {
      "epoch": 27.51784464660602,
      "grad_norm": 0.2678200900554657,
      "learning_rate": 2.699748251748252e-05,
      "loss": 0.5436857604980468,
      "step": 19675
    },
    {
      "epoch": 27.552834149755075,
      "grad_norm": 0.42973941564559937,
      "learning_rate": 2.6955524475524478e-05,
      "loss": 0.5162619400024414,
      "step": 19700
    },
    {
      "epoch": 27.587823652904127,
      "grad_norm": 0.398637056350708,
      "learning_rate": 2.6913566433566434e-05,
      "loss": 0.585906867980957,
      "step": 19725
    },
    {
      "epoch": 27.622813156053184,
      "grad_norm": 0.42291685938835144,
      "learning_rate": 2.687160839160839e-05,
      "loss": 0.6111789321899415,
      "step": 19750
    },
    {
      "epoch": 27.65780265920224,
      "grad_norm": 0.28755947947502136,
      "learning_rate": 2.682965034965035e-05,
      "loss": 0.5585378265380859,
      "step": 19775
    },
    {
      "epoch": 27.692792162351296,
      "grad_norm": 0.4236230254173279,
      "learning_rate": 2.678769230769231e-05,
      "loss": 0.5431413269042968,
      "step": 19800
    },
    {
      "epoch": 27.72778166550035,
      "grad_norm": 0.9677793979644775,
      "learning_rate": 2.6745734265734266e-05,
      "loss": 0.5637481307983399,
      "step": 19825
    },
    {
      "epoch": 27.762771168649405,
      "grad_norm": 0.29962459206581116,
      "learning_rate": 2.6703776223776223e-05,
      "loss": 0.5603358459472656,
      "step": 19850
    },
    {
      "epoch": 27.79776067179846,
      "grad_norm": 0.2452961951494217,
      "learning_rate": 2.6661818181818182e-05,
      "loss": 0.5716180801391602,
      "step": 19875
    },
    {
      "epoch": 27.832750174947517,
      "grad_norm": 0.6374670267105103,
      "learning_rate": 2.6619860139860142e-05,
      "loss": 0.5668450546264648,
      "step": 19900
    },
    {
      "epoch": 27.86773967809657,
      "grad_norm": 0.3121291995048523,
      "learning_rate": 2.6577902097902098e-05,
      "loss": 0.5331582641601562,
      "step": 19925
    },
    {
      "epoch": 27.902729181245626,
      "grad_norm": 6.235665798187256,
      "learning_rate": 2.6535944055944058e-05,
      "loss": 0.501629638671875,
      "step": 19950
    },
    {
      "epoch": 27.93771868439468,
      "grad_norm": 1.4498238563537598,
      "learning_rate": 2.6493986013986014e-05,
      "loss": 0.5953236389160156,
      "step": 19975
    },
    {
      "epoch": 27.972708187543738,
      "grad_norm": 0.3689371347427368,
      "learning_rate": 2.6452027972027974e-05,
      "loss": 0.6068524551391602,
      "step": 20000
    },
    {
      "epoch": 28.0,
      "eval_loss": 0.27608683705329895,
      "eval_mean_accuracy": 0.798945541613779,
      "eval_mean_iou": 0.7052015564159362,
      "eval_overall_accuracy": 0.8875026883387039,
      "eval_per_category_accuracy": [
        0.9057602419822669,
        0.8688807890364392,
        0.5915447710807057,
        0.6071522701105578,
        0.8288638605205776,
        0.9914713169521269
      ],
      "eval_per_category_iou": [
        0.7921283114067085,
        0.7296055386303585,
        0.4884069193426489,
        0.5278111075604006,
        0.7076482024382955,
        0.9856092591172056
      ],
      "eval_runtime": 28.6021,
      "eval_samples_per_second": 11.083,
      "eval_steps_per_second": 5.559,
      "step": 20020
    },
    {
      "epoch": 28.006997900629813,
      "grad_norm": 0.5427653789520264,
      "learning_rate": 2.641006993006993e-05,
      "loss": 0.5478656005859375,
      "step": 20025
    },
    {
      "epoch": 28.041987403778865,
      "grad_norm": 0.6036920547485352,
      "learning_rate": 2.636811188811189e-05,
      "loss": 0.5687040710449218,
      "step": 20050
    },
    {
      "epoch": 28.07697690692792,
      "grad_norm": 0.6490834951400757,
      "learning_rate": 2.6326153846153846e-05,
      "loss": 0.5621305847167969,
      "step": 20075
    },
    {
      "epoch": 28.111966410076977,
      "grad_norm": 0.31567999720573425,
      "learning_rate": 2.6284195804195806e-05,
      "loss": 0.5490665435791016,
      "step": 20100
    },
    {
      "epoch": 28.146955913226034,
      "grad_norm": 0.2910226583480835,
      "learning_rate": 2.6242237762237762e-05,
      "loss": 0.5329369735717774,
      "step": 20125
    },
    {
      "epoch": 28.181945416375086,
      "grad_norm": 1.6330597400665283,
      "learning_rate": 2.620027972027972e-05,
      "loss": 0.5239618682861328,
      "step": 20150
    },
    {
      "epoch": 28.216934919524142,
      "grad_norm": 0.32334476709365845,
      "learning_rate": 2.615832167832168e-05,
      "loss": 0.5453892517089843,
      "step": 20175
    },
    {
      "epoch": 28.2519244226732,
      "grad_norm": 0.41741955280303955,
      "learning_rate": 2.6116363636363637e-05,
      "loss": 0.5428451919555664,
      "step": 20200
    },
    {
      "epoch": 28.286913925822255,
      "grad_norm": 0.5640422105789185,
      "learning_rate": 2.6074405594405594e-05,
      "loss": 0.5471539306640625,
      "step": 20225
    },
    {
      "epoch": 28.321903428971307,
      "grad_norm": 0.3190552890300751,
      "learning_rate": 2.6032447552447553e-05,
      "loss": 0.5532890701293945,
      "step": 20250
    },
    {
      "epoch": 28.356892932120363,
      "grad_norm": 0.3735542297363281,
      "learning_rate": 2.5990489510489513e-05,
      "loss": 0.5812590789794921,
      "step": 20275
    },
    {
      "epoch": 28.39188243526942,
      "grad_norm": 0.3888707160949707,
      "learning_rate": 2.594853146853147e-05,
      "loss": 0.5607368469238281,
      "step": 20300
    },
    {
      "epoch": 28.426871938418476,
      "grad_norm": 0.5668398141860962,
      "learning_rate": 2.5906573426573425e-05,
      "loss": 0.6029677963256836,
      "step": 20325
    },
    {
      "epoch": 28.461861441567528,
      "grad_norm": 0.435889333486557,
      "learning_rate": 2.5864615384615385e-05,
      "loss": 0.578839225769043,
      "step": 20350
    },
    {
      "epoch": 28.496850944716584,
      "grad_norm": 0.3474434018135071,
      "learning_rate": 2.5822657342657345e-05,
      "loss": 0.5311388778686523,
      "step": 20375
    },
    {
      "epoch": 28.53184044786564,
      "grad_norm": 0.23600012063980103,
      "learning_rate": 2.57806993006993e-05,
      "loss": 0.5583956527709961,
      "step": 20400
    },
    {
      "epoch": 28.566829951014697,
      "grad_norm": 0.7389581203460693,
      "learning_rate": 2.5738741258741257e-05,
      "loss": 0.5797943115234375,
      "step": 20425
    },
    {
      "epoch": 28.60181945416375,
      "grad_norm": 0.6362802982330322,
      "learning_rate": 2.5696783216783217e-05,
      "loss": 0.5895120239257813,
      "step": 20450
    },
    {
      "epoch": 28.636808957312805,
      "grad_norm": 0.5735580921173096,
      "learning_rate": 2.5654825174825177e-05,
      "loss": 0.5814105224609375,
      "step": 20475
    },
    {
      "epoch": 28.67179846046186,
      "grad_norm": 0.5275495648384094,
      "learning_rate": 2.5612867132867136e-05,
      "loss": 0.5238895797729493,
      "step": 20500
    },
    {
      "epoch": 28.706787963610918,
      "grad_norm": 0.3497440218925476,
      "learning_rate": 2.557090909090909e-05,
      "loss": 0.5857555770874023,
      "step": 20525
    },
    {
      "epoch": 28.741777466759974,
      "grad_norm": 0.4207511246204376,
      "learning_rate": 2.552895104895105e-05,
      "loss": 0.5281560897827149,
      "step": 20550
    },
    {
      "epoch": 28.776766969909026,
      "grad_norm": 0.6932316422462463,
      "learning_rate": 2.548699300699301e-05,
      "loss": 0.5450083541870118,
      "step": 20575
    },
    {
      "epoch": 28.811756473058082,
      "grad_norm": 0.36496615409851074,
      "learning_rate": 2.5445034965034968e-05,
      "loss": 0.5317324829101563,
      "step": 20600
    },
    {
      "epoch": 28.84674597620714,
      "grad_norm": 0.4345487356185913,
      "learning_rate": 2.540307692307692e-05,
      "loss": 0.545192985534668,
      "step": 20625
    },
    {
      "epoch": 28.881735479356195,
      "grad_norm": 0.24738864600658417,
      "learning_rate": 2.536111888111888e-05,
      "loss": 0.5659274291992188,
      "step": 20650
    },
    {
      "epoch": 28.916724982505247,
      "grad_norm": 0.5090660452842712,
      "learning_rate": 2.531916083916084e-05,
      "loss": 0.5581476974487305,
      "step": 20675
    },
    {
      "epoch": 28.951714485654303,
      "grad_norm": 0.5259671807289124,
      "learning_rate": 2.52772027972028e-05,
      "loss": 0.5865964126586914,
      "step": 20700
    },
    {
      "epoch": 28.98670398880336,
      "grad_norm": 0.435123473405838,
      "learning_rate": 2.5235244755244756e-05,
      "loss": 0.5970751953125,
      "step": 20725
    },
    {
      "epoch": 29.0,
      "eval_loss": 0.27573350071907043,
      "eval_mean_accuracy": 0.7988187993622691,
      "eval_mean_iou": 0.7053003486297488,
      "eval_overall_accuracy": 0.8876147586088451,
      "eval_per_category_accuracy": [
        0.9055823068248933,
        0.8634304067241364,
        0.5883342026874202,
        0.609416336552238,
        0.8343172615828394,
        0.9918322818020869
      ],
      "eval_per_category_iou": [
        0.7924228537021336,
        0.7295093320612567,
        0.48670535328350223,
        0.528933260177466,
        0.7086342282521234,
        0.98559706430201
      ],
      "eval_runtime": 28.5349,
      "eval_samples_per_second": 11.109,
      "eval_steps_per_second": 5.572,
      "step": 20735
    },
    {
      "epoch": 29.020993701889434,
      "grad_norm": 0.3304479718208313,
      "learning_rate": 2.5193286713286712e-05,
      "loss": 0.5932826995849609,
      "step": 20750
    },
    {
      "epoch": 29.055983205038487,
      "grad_norm": 0.45679807662963867,
      "learning_rate": 2.5151328671328672e-05,
      "loss": 0.5082065963745117,
      "step": 20775
    },
    {
      "epoch": 29.090972708187543,
      "grad_norm": 0.39890721440315247,
      "learning_rate": 2.5109370629370632e-05,
      "loss": 0.5941749572753906,
      "step": 20800
    },
    {
      "epoch": 29.1259622113366,
      "grad_norm": 0.44187846779823303,
      "learning_rate": 2.5067412587412588e-05,
      "loss": 0.5421488189697266,
      "step": 20825
    },
    {
      "epoch": 29.160951714485655,
      "grad_norm": 0.5369681119918823,
      "learning_rate": 2.5025454545454544e-05,
      "loss": 0.5246715545654297,
      "step": 20850
    },
    {
      "epoch": 29.195941217634708,
      "grad_norm": 0.35071998834609985,
      "learning_rate": 2.4983496503496504e-05,
      "loss": 0.5856154632568359,
      "step": 20875
    },
    {
      "epoch": 29.230930720783764,
      "grad_norm": 0.582924485206604,
      "learning_rate": 2.4941538461538464e-05,
      "loss": 0.5249696350097657,
      "step": 20900
    },
    {
      "epoch": 29.26592022393282,
      "grad_norm": 0.30479755997657776,
      "learning_rate": 2.489958041958042e-05,
      "loss": 0.5957972717285156,
      "step": 20925
    },
    {
      "epoch": 29.300909727081876,
      "grad_norm": 0.42611783742904663,
      "learning_rate": 2.485762237762238e-05,
      "loss": 0.5459188842773437,
      "step": 20950
    },
    {
      "epoch": 29.33589923023093,
      "grad_norm": 0.8069387674331665,
      "learning_rate": 2.4815664335664336e-05,
      "loss": 0.5403317642211914,
      "step": 20975
    },
    {
      "epoch": 29.370888733379985,
      "grad_norm": 0.27654173970222473,
      "learning_rate": 2.4773706293706295e-05,
      "loss": 0.558052978515625,
      "step": 21000
    },
    {
      "epoch": 29.40587823652904,
      "grad_norm": 0.34049299359321594,
      "learning_rate": 2.473174825174825e-05,
      "loss": 0.5733448028564453,
      "step": 21025
    },
    {
      "epoch": 29.440867739678097,
      "grad_norm": 0.8396814465522766,
      "learning_rate": 2.468979020979021e-05,
      "loss": 0.5769051742553711,
      "step": 21050
    },
    {
      "epoch": 29.475857242827153,
      "grad_norm": 0.5453730821609497,
      "learning_rate": 2.4647832167832168e-05,
      "loss": 0.5735078430175782,
      "step": 21075
    },
    {
      "epoch": 29.510846745976206,
      "grad_norm": 0.9404042959213257,
      "learning_rate": 2.4605874125874127e-05,
      "loss": 0.5548689270019531,
      "step": 21100
    },
    {
      "epoch": 29.545836249125262,
      "grad_norm": 0.32274046540260315,
      "learning_rate": 2.4563916083916083e-05,
      "loss": 0.561642723083496,
      "step": 21125
    },
    {
      "epoch": 29.58082575227432,
      "grad_norm": 0.2853066027164459,
      "learning_rate": 2.4521958041958043e-05,
      "loss": 0.5778185653686524,
      "step": 21150
    },
    {
      "epoch": 29.615815255423374,
      "grad_norm": 0.5385484099388123,
      "learning_rate": 2.448e-05,
      "loss": 0.5781294631958008,
      "step": 21175
    },
    {
      "epoch": 29.650804758572427,
      "grad_norm": 0.2927623391151428,
      "learning_rate": 2.443804195804196e-05,
      "loss": 0.5350847625732422,
      "step": 21200
    },
    {
      "epoch": 29.685794261721483,
      "grad_norm": 1.110632061958313,
      "learning_rate": 2.4396083916083915e-05,
      "loss": 0.5270456314086914,
      "step": 21225
    },
    {
      "epoch": 29.72078376487054,
      "grad_norm": 0.37848013639450073,
      "learning_rate": 2.4354125874125875e-05,
      "loss": 0.5281268310546875,
      "step": 21250
    },
    {
      "epoch": 29.755773268019595,
      "grad_norm": 0.3435743451118469,
      "learning_rate": 2.4312167832167835e-05,
      "loss": 0.5360283660888672,
      "step": 21275
    },
    {
      "epoch": 29.790762771168648,
      "grad_norm": 0.4284641742706299,
      "learning_rate": 2.427020979020979e-05,
      "loss": 0.5720414352416993,
      "step": 21300
    },
    {
      "epoch": 29.825752274317704,
      "grad_norm": 1.3095487356185913,
      "learning_rate": 2.4228251748251747e-05,
      "loss": 0.565132942199707,
      "step": 21325
    },
    {
      "epoch": 29.86074177746676,
      "grad_norm": 0.9977959394454956,
      "learning_rate": 2.4186293706293707e-05,
      "loss": 0.5892480850219727,
      "step": 21350
    },
    {
      "epoch": 29.895731280615816,
      "grad_norm": 0.4125765264034271,
      "learning_rate": 2.4144335664335666e-05,
      "loss": 0.5962371444702148,
      "step": 21375
    },
    {
      "epoch": 29.93072078376487,
      "grad_norm": 0.3769916594028473,
      "learning_rate": 2.4102377622377623e-05,
      "loss": 0.5293597793579101,
      "step": 21400
    },
    {
      "epoch": 29.965710286913925,
      "grad_norm": 0.3402467966079712,
      "learning_rate": 2.406041958041958e-05,
      "loss": 0.5578003692626953,
      "step": 21425
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.5822579860687256,
      "learning_rate": 2.401846153846154e-05,
      "loss": 0.564356689453125,
      "step": 21450
    },
    {
      "epoch": 30.0,
      "eval_loss": 0.27523642778396606,
      "eval_mean_accuracy": 0.7983256248368084,
      "eval_mean_iou": 0.7051420483533386,
      "eval_overall_accuracy": 0.8877099214668154,
      "eval_per_category_accuracy": [
        0.9083416741681472,
        0.8631108317029728,
        0.5768999791605022,
        0.6126296264736248,
        0.8375480667308265,
        0.9914235707847768
      ],
      "eval_per_category_iou": [
        0.7926941097699832,
        0.7295913895062445,
        0.48242971662654144,
        0.5309933786209325,
        0.7095430207589652,
        0.9856006748373649
      ],
      "eval_runtime": 28.5951,
      "eval_samples_per_second": 11.086,
      "eval_steps_per_second": 5.56,
      "step": 21450
    },
    {
      "epoch": 30.034989503149056,
      "grad_norm": 0.36207789182662964,
      "learning_rate": 2.3976503496503498e-05,
      "loss": 0.584332275390625,
      "step": 21475
    },
    {
      "epoch": 30.069979006298112,
      "grad_norm": 0.5444872379302979,
      "learning_rate": 2.3934545454545458e-05,
      "loss": 0.5871762466430664,
      "step": 21500
    },
    {
      "epoch": 30.104968509447165,
      "grad_norm": 0.2733660638332367,
      "learning_rate": 2.389258741258741e-05,
      "loss": 0.5441046524047851,
      "step": 21525
    },
    {
      "epoch": 30.13995801259622,
      "grad_norm": 0.48889708518981934,
      "learning_rate": 2.3852307692307693e-05,
      "loss": 0.5701368713378906,
      "step": 21550
    },
    {
      "epoch": 30.174947515745277,
      "grad_norm": 0.3731662631034851,
      "learning_rate": 2.381034965034965e-05,
      "loss": 0.5727007293701172,
      "step": 21575
    },
    {
      "epoch": 30.209937018894333,
      "grad_norm": 0.5052016973495483,
      "learning_rate": 2.376839160839161e-05,
      "loss": 0.5660466384887696,
      "step": 21600
    },
    {
      "epoch": 30.244926522043386,
      "grad_norm": 0.35137060284614563,
      "learning_rate": 2.372643356643357e-05,
      "loss": 0.5345661926269532,
      "step": 21625
    },
    {
      "epoch": 30.279916025192442,
      "grad_norm": 0.39141786098480225,
      "learning_rate": 2.3684475524475525e-05,
      "loss": 0.5500066375732422,
      "step": 21650
    },
    {
      "epoch": 30.314905528341498,
      "grad_norm": 0.40508216619491577,
      "learning_rate": 2.364251748251748e-05,
      "loss": 0.546144790649414,
      "step": 21675
    },
    {
      "epoch": 30.349895031490554,
      "grad_norm": 0.3396546542644501,
      "learning_rate": 2.360055944055944e-05,
      "loss": 0.5368259429931641,
      "step": 21700
    },
    {
      "epoch": 30.384884534639607,
      "grad_norm": 0.32610592246055603,
      "learning_rate": 2.35586013986014e-05,
      "loss": 0.5663583374023438,
      "step": 21725
    },
    {
      "epoch": 30.419874037788663,
      "grad_norm": 0.31599974632263184,
      "learning_rate": 2.3516643356643357e-05,
      "loss": 0.5602803421020508,
      "step": 21750
    },
    {
      "epoch": 30.45486354093772,
      "grad_norm": 0.44674479961395264,
      "learning_rate": 2.3474685314685316e-05,
      "loss": 0.5678319549560547,
      "step": 21775
    },
    {
      "epoch": 30.489853044086775,
      "grad_norm": 0.25273144245147705,
      "learning_rate": 2.3432727272727273e-05,
      "loss": 0.5574960327148437,
      "step": 21800
    },
    {
      "epoch": 30.524842547235828,
      "grad_norm": 0.8382542133331299,
      "learning_rate": 2.3390769230769232e-05,
      "loss": 0.521292953491211,
      "step": 21825
    },
    {
      "epoch": 30.559832050384884,
      "grad_norm": 0.424873411655426,
      "learning_rate": 2.334881118881119e-05,
      "loss": 0.5556831359863281,
      "step": 21850
    },
    {
      "epoch": 30.59482155353394,
      "grad_norm": 0.3210710287094116,
      "learning_rate": 2.3306853146853148e-05,
      "loss": 0.5656859207153321,
      "step": 21875
    },
    {
      "epoch": 30.629811056682996,
      "grad_norm": 0.4933934807777405,
      "learning_rate": 2.3264895104895104e-05,
      "loss": 0.5692356109619141,
      "step": 21900
    },
    {
      "epoch": 30.66480055983205,
      "grad_norm": 0.27612030506134033,
      "learning_rate": 2.3222937062937064e-05,
      "loss": 0.5684342193603515,
      "step": 21925
    },
    {
      "epoch": 30.699790062981105,
      "grad_norm": 0.643890917301178,
      "learning_rate": 2.318097902097902e-05,
      "loss": 0.5584994888305664,
      "step": 21950
    },
    {
      "epoch": 30.73477956613016,
      "grad_norm": 0.4311735928058624,
      "learning_rate": 2.313902097902098e-05,
      "loss": 0.5287529754638672,
      "step": 21975
    },
    {
      "epoch": 30.769769069279217,
      "grad_norm": 0.2877378761768341,
      "learning_rate": 2.3097062937062936e-05,
      "loss": 0.5624726867675781,
      "step": 22000
    },
    {
      "epoch": 30.80475857242827,
      "grad_norm": 0.3479079604148865,
      "learning_rate": 2.3055104895104896e-05,
      "loss": 0.557985954284668,
      "step": 22025
    },
    {
      "epoch": 30.839748075577326,
      "grad_norm": 0.5739034414291382,
      "learning_rate": 2.3013146853146852e-05,
      "loss": 0.5436611175537109,
      "step": 22050
    },
    {
      "epoch": 30.874737578726382,
      "grad_norm": 0.22222039103507996,
      "learning_rate": 2.2971188811188812e-05,
      "loss": 0.5509006881713867,
      "step": 22075
    },
    {
      "epoch": 30.909727081875438,
      "grad_norm": 0.6796809434890747,
      "learning_rate": 2.292923076923077e-05,
      "loss": 0.5815204620361328,
      "step": 22100
    },
    {
      "epoch": 30.944716585024494,
      "grad_norm": 0.3037959933280945,
      "learning_rate": 2.2887272727272728e-05,
      "loss": 0.5502165985107422,
      "step": 22125
    },
    {
      "epoch": 30.979706088173547,
      "grad_norm": 0.2282564640045166,
      "learning_rate": 2.2845314685314684e-05,
      "loss": 0.5814559936523438,
      "step": 22150
    },
    {
      "epoch": 31.0,
      "eval_loss": 0.27438491582870483,
      "eval_mean_accuracy": 0.7998842937629908,
      "eval_mean_iou": 0.7063521271828336,
      "eval_overall_accuracy": 0.8879143988672491,
      "eval_per_category_accuracy": [
        0.908232359208956,
        0.8592075646534363,
        0.586246681439436,
        0.6153138085263652,
        0.8386411666049434,
        0.9916641821448076
      ],
      "eval_per_category_iou": [
        0.793392274070152,
        0.7292423826387647,
        0.48697589557220206,
        0.533062139637571,
        0.7098160949886049,
        0.9856239761897075
      ],
      "eval_runtime": 28.4751,
      "eval_samples_per_second": 11.133,
      "eval_steps_per_second": 5.584,
      "step": 22165
    },
    {
      "epoch": 31.01399580125962,
      "grad_norm": 0.3139038681983948,
      "learning_rate": 2.2803356643356644e-05,
      "loss": 0.4980224609375,
      "step": 22175
    },
    {
      "epoch": 31.048985304408678,
      "grad_norm": 0.5460123419761658,
      "learning_rate": 2.2761398601398603e-05,
      "loss": 0.5465504455566407,
      "step": 22200
    },
    {
      "epoch": 31.083974807557734,
      "grad_norm": 0.5698044300079346,
      "learning_rate": 2.271944055944056e-05,
      "loss": 0.5574913024902344,
      "step": 22225
    },
    {
      "epoch": 31.118964310706787,
      "grad_norm": 0.5575146675109863,
      "learning_rate": 2.2677482517482516e-05,
      "loss": 0.5790927886962891,
      "step": 22250
    },
    {
      "epoch": 31.153953813855843,
      "grad_norm": 0.4787241518497467,
      "learning_rate": 2.2635524475524476e-05,
      "loss": 0.5783464813232422,
      "step": 22275
    },
    {
      "epoch": 31.1889433170049,
      "grad_norm": 0.49003270268440247,
      "learning_rate": 2.2593566433566435e-05,
      "loss": 0.5946933364868164,
      "step": 22300
    },
    {
      "epoch": 31.223932820153955,
      "grad_norm": 19.744319915771484,
      "learning_rate": 2.2551608391608395e-05,
      "loss": 0.5224814605712891,
      "step": 22325
    },
    {
      "epoch": 31.258922323303008,
      "grad_norm": 1.3007798194885254,
      "learning_rate": 2.2509650349650348e-05,
      "loss": 0.5311840438842773,
      "step": 22350
    },
    {
      "epoch": 31.293911826452064,
      "grad_norm": 1.0366899967193604,
      "learning_rate": 2.2467692307692307e-05,
      "loss": 0.579521484375,
      "step": 22375
    },
    {
      "epoch": 31.32890132960112,
      "grad_norm": 1.4286805391311646,
      "learning_rate": 2.2425734265734267e-05,
      "loss": 0.5445110702514648,
      "step": 22400
    },
    {
      "epoch": 31.363890832750176,
      "grad_norm": 0.5907592177391052,
      "learning_rate": 2.2383776223776227e-05,
      "loss": 0.5653628540039063,
      "step": 22425
    },
    {
      "epoch": 31.39888033589923,
      "grad_norm": 0.6361119151115417,
      "learning_rate": 2.234181818181818e-05,
      "loss": 0.5313717651367188,
      "step": 22450
    },
    {
      "epoch": 31.433869839048285,
      "grad_norm": 0.20120316743850708,
      "learning_rate": 2.229986013986014e-05,
      "loss": 0.5464255905151367,
      "step": 22475
    },
    {
      "epoch": 31.46885934219734,
      "grad_norm": 0.34030434489250183,
      "learning_rate": 2.22579020979021e-05,
      "loss": 0.559080810546875,
      "step": 22500
    },
    {
      "epoch": 31.503848845346397,
      "grad_norm": 0.25613731145858765,
      "learning_rate": 2.221594405594406e-05,
      "loss": 0.5089873886108398,
      "step": 22525
    },
    {
      "epoch": 31.538838348495453,
      "grad_norm": 0.34377872943878174,
      "learning_rate": 2.217398601398601e-05,
      "loss": 0.5482257843017578,
      "step": 22550
    },
    {
      "epoch": 31.573827851644506,
      "grad_norm": 0.18768425285816193,
      "learning_rate": 2.213202797202797e-05,
      "loss": 0.5702604675292968,
      "step": 22575
    },
    {
      "epoch": 31.608817354793562,
      "grad_norm": 0.32184162735939026,
      "learning_rate": 2.209006993006993e-05,
      "loss": 0.5766143798828125,
      "step": 22600
    },
    {
      "epoch": 31.643806857942618,
      "grad_norm": 0.3446452021598816,
      "learning_rate": 2.204811188811189e-05,
      "loss": 0.5634146499633789,
      "step": 22625
    },
    {
      "epoch": 31.678796361091674,
      "grad_norm": 0.6269354820251465,
      "learning_rate": 2.2006153846153847e-05,
      "loss": 0.5501881408691406,
      "step": 22650
    },
    {
      "epoch": 31.713785864240727,
      "grad_norm": 0.6523043513298035,
      "learning_rate": 2.1964195804195803e-05,
      "loss": 0.5822119903564453,
      "step": 22675
    },
    {
      "epoch": 31.748775367389783,
      "grad_norm": 0.41479188203811646,
      "learning_rate": 2.1922237762237762e-05,
      "loss": 0.5910014343261719,
      "step": 22700
    },
    {
      "epoch": 31.78376487053884,
      "grad_norm": 0.38615646958351135,
      "learning_rate": 2.1880279720279722e-05,
      "loss": 0.570546989440918,
      "step": 22725
    },
    {
      "epoch": 31.818754373687895,
      "grad_norm": 0.5989712476730347,
      "learning_rate": 2.183832167832168e-05,
      "loss": 0.5534345626831054,
      "step": 22750
    },
    {
      "epoch": 31.853743876836948,
      "grad_norm": 0.44538554549217224,
      "learning_rate": 2.1796363636363635e-05,
      "loss": 0.5501872634887696,
      "step": 22775
    },
    {
      "epoch": 31.888733379986004,
      "grad_norm": 2.1694817543029785,
      "learning_rate": 2.1754405594405594e-05,
      "loss": 0.5670825958251953,
      "step": 22800
    },
    {
      "epoch": 31.92372288313506,
      "grad_norm": 0.27947989106178284,
      "learning_rate": 2.1712447552447554e-05,
      "loss": 0.5327102661132812,
      "step": 22825
    },
    {
      "epoch": 31.958712386284116,
      "grad_norm": 0.4941674768924713,
      "learning_rate": 2.167048951048951e-05,
      "loss": 0.5477827835083008,
      "step": 22850
    },
    {
      "epoch": 31.99370188943317,
      "grad_norm": 0.38476476073265076,
      "learning_rate": 2.162853146853147e-05,
      "loss": 0.5773221206665039,
      "step": 22875
    },
    {
      "epoch": 32.0,
      "eval_loss": 0.27423644065856934,
      "eval_mean_accuracy": 0.7989930240560618,
      "eval_mean_iou": 0.7060894432946094,
      "eval_overall_accuracy": 0.887957383405523,
      "eval_per_category_accuracy": [
        0.9086161549608763,
        0.864433060839735,
        0.5874474079468309,
        0.6076090467537152,
        0.834133660895453,
        0.9917188129397596
      ],
      "eval_per_category_iou": [
        0.7932166661023439,
        0.7301742491233526,
        0.4871684943230326,
        0.5310589101301175,
        0.7092969865428209,
        0.9856213535459895
      ],
      "eval_runtime": 29.3716,
      "eval_samples_per_second": 10.793,
      "eval_steps_per_second": 5.413,
      "step": 22880
    },
    {
      "epoch": 32.02799160251924,
      "grad_norm": 0.30583396553993225,
      "learning_rate": 2.1586573426573426e-05,
      "loss": 0.5692038726806641,
      "step": 22900
    },
    {
      "epoch": 32.0629811056683,
      "grad_norm": 0.29988712072372437,
      "learning_rate": 2.1544615384615386e-05,
      "loss": 0.5364009857177734,
      "step": 22925
    },
    {
      "epoch": 32.097970608817356,
      "grad_norm": 0.6098504662513733,
      "learning_rate": 2.1502657342657342e-05,
      "loss": 0.5646559143066406,
      "step": 22950
    },
    {
      "epoch": 32.13296011196641,
      "grad_norm": 0.48126018047332764,
      "learning_rate": 2.1460699300699302e-05,
      "loss": 0.5498849868774414,
      "step": 22975
    },
    {
      "epoch": 32.16794961511547,
      "grad_norm": 0.3882738947868347,
      "learning_rate": 2.1418741258741258e-05,
      "loss": 0.5580150985717773,
      "step": 23000
    },
    {
      "epoch": 32.202939118264524,
      "grad_norm": 1.0574934482574463,
      "learning_rate": 2.1376783216783218e-05,
      "loss": 0.600354118347168,
      "step": 23025
    },
    {
      "epoch": 32.23792862141357,
      "grad_norm": 0.3067411482334137,
      "learning_rate": 2.1334825174825174e-05,
      "loss": 0.592960090637207,
      "step": 23050
    },
    {
      "epoch": 32.27291812456263,
      "grad_norm": 0.49896034598350525,
      "learning_rate": 2.1292867132867134e-05,
      "loss": 0.5783460998535156,
      "step": 23075
    },
    {
      "epoch": 32.307907627711685,
      "grad_norm": 0.3055238723754883,
      "learning_rate": 2.1250909090909093e-05,
      "loss": 0.5005291748046875,
      "step": 23100
    },
    {
      "epoch": 32.34289713086074,
      "grad_norm": 0.7315818667411804,
      "learning_rate": 2.120895104895105e-05,
      "loss": 0.5551213073730469,
      "step": 23125
    },
    {
      "epoch": 32.3778866340098,
      "grad_norm": 0.30401039123535156,
      "learning_rate": 2.1166993006993006e-05,
      "loss": 0.5296757888793945,
      "step": 23150
    },
    {
      "epoch": 32.412876137158854,
      "grad_norm": 0.8137747049331665,
      "learning_rate": 2.1125034965034965e-05,
      "loss": 0.5378876495361328,
      "step": 23175
    },
    {
      "epoch": 32.44786564030791,
      "grad_norm": 0.5998925566673279,
      "learning_rate": 2.1083076923076925e-05,
      "loss": 0.5454076385498047,
      "step": 23200
    },
    {
      "epoch": 32.482855143456966,
      "grad_norm": 0.8062509894371033,
      "learning_rate": 2.104111888111888e-05,
      "loss": 0.5275088882446289,
      "step": 23225
    },
    {
      "epoch": 32.517844646606015,
      "grad_norm": 0.2137850970029831,
      "learning_rate": 2.0999160839160838e-05,
      "loss": 0.5581155395507813,
      "step": 23250
    },
    {
      "epoch": 32.55283414975507,
      "grad_norm": 0.5145680904388428,
      "learning_rate": 2.0957202797202797e-05,
      "loss": 0.5625768280029297,
      "step": 23275
    },
    {
      "epoch": 32.58782365290413,
      "grad_norm": 0.3507727086544037,
      "learning_rate": 2.0915244755244757e-05,
      "loss": 0.5651405715942383,
      "step": 23300
    },
    {
      "epoch": 32.62281315605318,
      "grad_norm": 0.5814825296401978,
      "learning_rate": 2.0873286713286713e-05,
      "loss": 0.5657780456542969,
      "step": 23325
    },
    {
      "epoch": 32.65780265920224,
      "grad_norm": 0.4342256784439087,
      "learning_rate": 2.083132867132867e-05,
      "loss": 0.5190388107299805,
      "step": 23350
    },
    {
      "epoch": 32.692792162351296,
      "grad_norm": 0.2186085432767868,
      "learning_rate": 2.078937062937063e-05,
      "loss": 0.5565209197998047,
      "step": 23375
    },
    {
      "epoch": 32.72778166550035,
      "grad_norm": 0.3661503791809082,
      "learning_rate": 2.074741258741259e-05,
      "loss": 0.5727587509155273,
      "step": 23400
    },
    {
      "epoch": 32.76277116864941,
      "grad_norm": 0.3394048511981964,
      "learning_rate": 2.070545454545455e-05,
      "loss": 0.546078872680664,
      "step": 23425
    },
    {
      "epoch": 32.79776067179846,
      "grad_norm": 0.531112551689148,
      "learning_rate": 2.06634965034965e-05,
      "loss": 0.5687648773193359,
      "step": 23450
    },
    {
      "epoch": 32.83275017494751,
      "grad_norm": 0.624960720539093,
      "learning_rate": 2.062153846153846e-05,
      "loss": 0.5672933197021485,
      "step": 23475
    },
    {
      "epoch": 32.86773967809657,
      "grad_norm": 0.4091227650642395,
      "learning_rate": 2.057958041958042e-05,
      "loss": 0.5983660888671875,
      "step": 23500
    },
    {
      "epoch": 32.902729181245626,
      "grad_norm": 0.25095686316490173,
      "learning_rate": 2.053762237762238e-05,
      "loss": 0.5061288452148438,
      "step": 23525
    },
    {
      "epoch": 32.93771868439468,
      "grad_norm": 0.30846816301345825,
      "learning_rate": 2.0495664335664333e-05,
      "loss": 0.5483459091186523,
      "step": 23550
    },
    {
      "epoch": 32.97270818754374,
      "grad_norm": 0.4056906998157501,
      "learning_rate": 2.0453706293706293e-05,
      "loss": 0.5660167312622071,
      "step": 23575
    },
    {
      "epoch": 33.0,
      "eval_loss": 0.2743242084980011,
      "eval_mean_accuracy": 0.7971446605662026,
      "eval_mean_iou": 0.7054011527727672,
      "eval_overall_accuracy": 0.8879673473466458,
      "eval_per_category_accuracy": [
        0.9079552529477746,
        0.8634657972875636,
        0.5734285152545303,
        0.6069273497244477,
        0.8395015132503305,
        0.9915895349325686
      ],
      "eval_per_category_iou": [
        0.7936844674829359,
        0.7301309835798487,
        0.48179101709781547,
        0.5311788645029479,
        0.7099815935384951,
        0.9856399904345596
      ],
      "eval_runtime": 28.3267,
      "eval_samples_per_second": 11.191,
      "eval_steps_per_second": 5.613,
      "step": 23595
    },
    {
      "epoch": 33.00699790062981,
      "grad_norm": 3.999279022216797,
      "learning_rate": 2.0411748251748252e-05,
      "loss": 0.5420209884643554,
      "step": 23600
    },
    {
      "epoch": 33.04198740377887,
      "grad_norm": 0.22628821432590485,
      "learning_rate": 2.0369790209790212e-05,
      "loss": 0.556269645690918,
      "step": 23625
    },
    {
      "epoch": 33.076976906927925,
      "grad_norm": 0.20207479596138,
      "learning_rate": 2.0327832167832168e-05,
      "loss": 0.5307004547119141,
      "step": 23650
    },
    {
      "epoch": 33.111966410076974,
      "grad_norm": 0.5885300636291504,
      "learning_rate": 2.0285874125874125e-05,
      "loss": 0.5538796615600586,
      "step": 23675
    },
    {
      "epoch": 33.14695591322603,
      "grad_norm": 0.44522354006767273,
      "learning_rate": 2.0243916083916084e-05,
      "loss": 0.5772222518920899,
      "step": 23700
    },
    {
      "epoch": 33.181945416375086,
      "grad_norm": 0.336529940366745,
      "learning_rate": 2.0201958041958044e-05,
      "loss": 0.52755615234375,
      "step": 23725
    },
    {
      "epoch": 33.21693491952414,
      "grad_norm": 0.8990999460220337,
      "learning_rate": 2.016e-05,
      "loss": 0.549723014831543,
      "step": 23750
    },
    {
      "epoch": 33.2519244226732,
      "grad_norm": 0.6468929648399353,
      "learning_rate": 2.0118041958041956e-05,
      "loss": 0.5489052963256836,
      "step": 23775
    },
    {
      "epoch": 33.286913925822255,
      "grad_norm": 0.44572174549102783,
      "learning_rate": 2.0076083916083916e-05,
      "loss": 0.5356383895874024,
      "step": 23800
    },
    {
      "epoch": 33.32190342897131,
      "grad_norm": 0.3974120318889618,
      "learning_rate": 2.0034125874125876e-05,
      "loss": 0.5534901809692383,
      "step": 23825
    },
    {
      "epoch": 33.35689293212037,
      "grad_norm": 0.2805021405220032,
      "learning_rate": 1.9992167832167832e-05,
      "loss": 0.5769909286499023,
      "step": 23850
    },
    {
      "epoch": 33.391882435269416,
      "grad_norm": 0.6499508619308472,
      "learning_rate": 1.9950209790209788e-05,
      "loss": 0.5589429855346679,
      "step": 23875
    },
    {
      "epoch": 33.42687193841847,
      "grad_norm": 0.28553879261016846,
      "learning_rate": 1.9908251748251748e-05,
      "loss": 0.5615642547607422,
      "step": 23900
    },
    {
      "epoch": 33.46186144156753,
      "grad_norm": 0.29837775230407715,
      "learning_rate": 1.9866293706293708e-05,
      "loss": 0.5647669982910156,
      "step": 23925
    },
    {
      "epoch": 33.496850944716584,
      "grad_norm": 0.7699263095855713,
      "learning_rate": 1.9824335664335667e-05,
      "loss": 0.5117103958129883,
      "step": 23950
    },
    {
      "epoch": 33.53184044786564,
      "grad_norm": 0.34569835662841797,
      "learning_rate": 1.9782377622377623e-05,
      "loss": 0.5876744079589844,
      "step": 23975
    },
    {
      "epoch": 33.5668299510147,
      "grad_norm": 0.41686081886291504,
      "learning_rate": 1.974041958041958e-05,
      "loss": 0.5308391571044921,
      "step": 24000
    },
    {
      "epoch": 33.60181945416375,
      "grad_norm": 4.936129570007324,
      "learning_rate": 1.969846153846154e-05,
      "loss": 0.5875923538208008,
      "step": 24025
    },
    {
      "epoch": 33.63680895731281,
      "grad_norm": 0.1805439591407776,
      "learning_rate": 1.96565034965035e-05,
      "loss": 0.5604516220092773,
      "step": 24050
    },
    {
      "epoch": 33.67179846046186,
      "grad_norm": 0.428250789642334,
      "learning_rate": 1.9614545454545455e-05,
      "loss": 0.5415015411376953,
      "step": 24075
    },
    {
      "epoch": 33.706787963610914,
      "grad_norm": 0.3700007200241089,
      "learning_rate": 1.957258741258741e-05,
      "loss": 0.5301493453979492,
      "step": 24100
    },
    {
      "epoch": 33.74177746675997,
      "grad_norm": 0.7829534411430359,
      "learning_rate": 1.953062937062937e-05,
      "loss": 0.5568152618408203,
      "step": 24125
    },
    {
      "epoch": 33.776766969909026,
      "grad_norm": 0.5319550037384033,
      "learning_rate": 1.948867132867133e-05,
      "loss": 0.6005807876586914,
      "step": 24150
    },
    {
      "epoch": 33.81175647305808,
      "grad_norm": 0.7958935499191284,
      "learning_rate": 1.9446713286713287e-05,
      "loss": 0.5425471878051757,
      "step": 24175
    },
    {
      "epoch": 33.84674597620714,
      "grad_norm": 0.2549867331981659,
      "learning_rate": 1.9404755244755247e-05,
      "loss": 0.5755572509765625,
      "step": 24200
    },
    {
      "epoch": 33.881735479356195,
      "grad_norm": 0.3041759431362152,
      "learning_rate": 1.9362797202797203e-05,
      "loss": 0.5665170288085938,
      "step": 24225
    },
    {
      "epoch": 33.91672498250525,
      "grad_norm": 0.3061508536338806,
      "learning_rate": 1.9320839160839163e-05,
      "loss": 0.5407370376586914,
      "step": 24250
    },
    {
      "epoch": 33.95171448565431,
      "grad_norm": 1.2853922843933105,
      "learning_rate": 1.927888111888112e-05,
      "loss": 0.5326737213134766,
      "step": 24275
    },
    {
      "epoch": 33.986703988803356,
      "grad_norm": 1.4799855947494507,
      "learning_rate": 1.923692307692308e-05,
      "loss": 0.5840225982666015,
      "step": 24300
    },
    {
      "epoch": 34.0,
      "eval_loss": 0.2739303410053253,
      "eval_mean_accuracy": 0.8012085373342543,
      "eval_mean_iou": 0.7069909404231144,
      "eval_overall_accuracy": 0.8880959279153625,
      "eval_per_category_accuracy": [
        0.9106128237187684,
        0.8608527253207057,
        0.582190033005693,
        0.624183796616484,
        0.8378667736312359,
        0.9915450717126397
      ],
      "eval_per_category_iou": [
        0.7937832780647288,
        0.7296715935052918,
        0.48602617229480377,
        0.5366594397045606,
        0.7101584332567309,
        0.9856467257125714
      ],
      "eval_runtime": 29.1732,
      "eval_samples_per_second": 10.866,
      "eval_steps_per_second": 5.45,
      "step": 24310
    },
    {
      "epoch": 34.02099370188943,
      "grad_norm": 0.2857816219329834,
      "learning_rate": 1.9194965034965035e-05,
      "loss": 0.557416877746582,
      "step": 24325
    },
    {
      "epoch": 34.05598320503849,
      "grad_norm": 1.5109052658081055,
      "learning_rate": 1.9153006993006994e-05,
      "loss": 0.5872513198852539,
      "step": 24350
    },
    {
      "epoch": 34.09097270818754,
      "grad_norm": 0.27676230669021606,
      "learning_rate": 1.911104895104895e-05,
      "loss": 0.524859504699707,
      "step": 24375
    },
    {
      "epoch": 34.1259622113366,
      "grad_norm": 1.0203312635421753,
      "learning_rate": 1.906909090909091e-05,
      "loss": 0.5829229354858398,
      "step": 24400
    },
    {
      "epoch": 34.160951714485655,
      "grad_norm": 0.4721177816390991,
      "learning_rate": 1.902713286713287e-05,
      "loss": 0.6181466674804688,
      "step": 24425
    },
    {
      "epoch": 34.19594121763471,
      "grad_norm": 1.0004726648330688,
      "learning_rate": 1.8985174825174826e-05,
      "loss": 0.5596876907348632,
      "step": 24450
    },
    {
      "epoch": 34.23093072078377,
      "grad_norm": 0.5681036114692688,
      "learning_rate": 1.8943216783216783e-05,
      "loss": 0.5285513305664062,
      "step": 24475
    },
    {
      "epoch": 34.26592022393282,
      "grad_norm": 0.32707247138023376,
      "learning_rate": 1.8901258741258742e-05,
      "loss": 0.5088045883178711,
      "step": 24500
    },
    {
      "epoch": 34.30090972708187,
      "grad_norm": 0.21926943957805634,
      "learning_rate": 1.8859300699300702e-05,
      "loss": 0.5677482986450195,
      "step": 24525
    },
    {
      "epoch": 34.33589923023093,
      "grad_norm": 1.1424314975738525,
      "learning_rate": 1.8817342657342658e-05,
      "loss": 0.5504303359985352,
      "step": 24550
    },
    {
      "epoch": 34.370888733379985,
      "grad_norm": 0.5726478099822998,
      "learning_rate": 1.8775384615384614e-05,
      "loss": 0.5579337692260742,
      "step": 24575
    },
    {
      "epoch": 34.40587823652904,
      "grad_norm": 0.3984464704990387,
      "learning_rate": 1.8733426573426574e-05,
      "loss": 0.519151725769043,
      "step": 24600
    },
    {
      "epoch": 34.4408677396781,
      "grad_norm": 0.5878159403800964,
      "learning_rate": 1.8691468531468534e-05,
      "loss": 0.5534082412719726,
      "step": 24625
    },
    {
      "epoch": 34.47585724282715,
      "grad_norm": 0.9320991635322571,
      "learning_rate": 1.864951048951049e-05,
      "loss": 0.5606792068481445,
      "step": 24650
    },
    {
      "epoch": 34.51084674597621,
      "grad_norm": 0.2879731059074402,
      "learning_rate": 1.8607552447552446e-05,
      "loss": 0.599442253112793,
      "step": 24675
    },
    {
      "epoch": 34.545836249125266,
      "grad_norm": 0.2964906692504883,
      "learning_rate": 1.8565594405594406e-05,
      "loss": 0.4909446334838867,
      "step": 24700
    },
    {
      "epoch": 34.580825752274315,
      "grad_norm": 0.4286372661590576,
      "learning_rate": 1.8523636363636366e-05,
      "loss": 0.5464422988891602,
      "step": 24725
    },
    {
      "epoch": 34.61581525542337,
      "grad_norm": 0.8316099643707275,
      "learning_rate": 1.8481678321678325e-05,
      "loss": 0.5174919128417969,
      "step": 24750
    },
    {
      "epoch": 34.65080475857243,
      "grad_norm": 0.5738633871078491,
      "learning_rate": 1.8439720279720278e-05,
      "loss": 0.5539305114746094,
      "step": 24775
    },
    {
      "epoch": 34.68579426172148,
      "grad_norm": 0.24361717700958252,
      "learning_rate": 1.8397762237762238e-05,
      "loss": 0.5354230880737305,
      "step": 24800
    },
    {
      "epoch": 34.72078376487054,
      "grad_norm": 0.46137186884880066,
      "learning_rate": 1.8355804195804197e-05,
      "loss": 0.5839168167114258,
      "step": 24825
    },
    {
      "epoch": 34.755773268019595,
      "grad_norm": 0.6495220065116882,
      "learning_rate": 1.8313846153846157e-05,
      "loss": 0.5878294372558593,
      "step": 24850
    },
    {
      "epoch": 34.79076277116865,
      "grad_norm": 0.27551713585853577,
      "learning_rate": 1.827188811188811e-05,
      "loss": 0.5657148742675782,
      "step": 24875
    },
    {
      "epoch": 34.82575227431771,
      "grad_norm": 0.3195043206214905,
      "learning_rate": 1.822993006993007e-05,
      "loss": 0.5226958084106446,
      "step": 24900
    },
    {
      "epoch": 34.86074177746676,
      "grad_norm": 0.2185465693473816,
      "learning_rate": 1.818797202797203e-05,
      "loss": 0.5258316040039063,
      "step": 24925
    },
    {
      "epoch": 34.89573128061581,
      "grad_norm": 0.3909280598163605,
      "learning_rate": 1.814601398601399e-05,
      "loss": 0.5444454956054687,
      "step": 24950
    },
    {
      "epoch": 34.93072078376487,
      "grad_norm": 0.48590919375419617,
      "learning_rate": 1.8105734265734268e-05,
      "loss": 0.5961825942993164,
      "step": 24975
    },
    {
      "epoch": 34.965710286913925,
      "grad_norm": 0.30374929308891296,
      "learning_rate": 1.8063776223776224e-05,
      "loss": 0.5685686111450196,
      "step": 25000
    },
    {
      "epoch": 35.0,
      "grad_norm": 4.065425395965576,
      "learning_rate": 1.8021818181818184e-05,
      "loss": 0.5603963088989258,
      "step": 25025
    },
    {
      "epoch": 35.0,
      "eval_loss": 0.27356112003326416,
      "eval_mean_accuracy": 0.8006344636849859,
      "eval_mean_iou": 0.7072528057255828,
      "eval_overall_accuracy": 0.8882374327265502,
      "eval_per_category_accuracy": [
        0.9100614753438081,
        0.8632805415262625,
        0.5854220301885216,
        0.6175303147101896,
        0.8358564296282408,
        0.9916559907128923
      ],
      "eval_per_category_iou": [
        0.7943479969637819,
        0.7304295454468454,
        0.48760357744226057,
        0.5357172021059418,
        0.7097533422091544,
        0.9856651701855131
      ],
      "eval_runtime": 28.5921,
      "eval_samples_per_second": 11.087,
      "eval_steps_per_second": 5.561,
      "step": 25025
    },
    {
      "epoch": 35.034989503149056,
      "grad_norm": 0.42793336510658264,
      "learning_rate": 1.797986013986014e-05,
      "loss": 0.5379037475585937,
      "step": 25050
    },
    {
      "epoch": 35.06997900629811,
      "grad_norm": 0.36640533804893494,
      "learning_rate": 1.79379020979021e-05,
      "loss": 0.5345656204223633,
      "step": 25075
    },
    {
      "epoch": 35.10496850944717,
      "grad_norm": 0.4010487198829651,
      "learning_rate": 1.7895944055944056e-05,
      "loss": 0.588860969543457,
      "step": 25100
    },
    {
      "epoch": 35.139958012596225,
      "grad_norm": 0.19800351560115814,
      "learning_rate": 1.7853986013986015e-05,
      "loss": 0.5654586791992188,
      "step": 25125
    },
    {
      "epoch": 35.174947515745274,
      "grad_norm": 5.987521648406982,
      "learning_rate": 1.7812027972027972e-05,
      "loss": 0.54027587890625,
      "step": 25150
    },
    {
      "epoch": 35.20993701889433,
      "grad_norm": 0.4732823073863983,
      "learning_rate": 1.777006993006993e-05,
      "loss": 0.5690345764160156,
      "step": 25175
    },
    {
      "epoch": 35.244926522043386,
      "grad_norm": 0.4602213203907013,
      "learning_rate": 1.7728111888111888e-05,
      "loss": 0.5509614562988281,
      "step": 25200
    },
    {
      "epoch": 35.27991602519244,
      "grad_norm": 0.6537181735038757,
      "learning_rate": 1.7686153846153847e-05,
      "loss": 0.5431735229492187,
      "step": 25225
    },
    {
      "epoch": 35.3149055283415,
      "grad_norm": 0.9739879369735718,
      "learning_rate": 1.7644195804195807e-05,
      "loss": 0.4844797897338867,
      "step": 25250
    },
    {
      "epoch": 35.349895031490554,
      "grad_norm": 0.8712482452392578,
      "learning_rate": 1.7602237762237763e-05,
      "loss": 0.5271651077270508,
      "step": 25275
    },
    {
      "epoch": 35.38488453463961,
      "grad_norm": 0.37052592635154724,
      "learning_rate": 1.756027972027972e-05,
      "loss": 0.566275634765625,
      "step": 25300
    },
    {
      "epoch": 35.41987403778867,
      "grad_norm": 0.44461631774902344,
      "learning_rate": 1.751832167832168e-05,
      "loss": 0.5412392425537109,
      "step": 25325
    },
    {
      "epoch": 35.454863540937716,
      "grad_norm": 0.2747931480407715,
      "learning_rate": 1.747636363636364e-05,
      "loss": 0.5406349182128907,
      "step": 25350
    },
    {
      "epoch": 35.48985304408677,
      "grad_norm": 0.17144344747066498,
      "learning_rate": 1.7434405594405595e-05,
      "loss": 0.5952048873901368,
      "step": 25375
    },
    {
      "epoch": 35.52484254723583,
      "grad_norm": 0.6528550386428833,
      "learning_rate": 1.739244755244755e-05,
      "loss": 0.5964047241210938,
      "step": 25400
    },
    {
      "epoch": 35.559832050384884,
      "grad_norm": 1.0508445501327515,
      "learning_rate": 1.735048951048951e-05,
      "loss": 0.56157470703125,
      "step": 25425
    },
    {
      "epoch": 35.59482155353394,
      "grad_norm": 0.47067990899086,
      "learning_rate": 1.730853146853147e-05,
      "loss": 0.5719429779052735,
      "step": 25450
    },
    {
      "epoch": 35.629811056682996,
      "grad_norm": 0.41489046812057495,
      "learning_rate": 1.7266573426573427e-05,
      "loss": 0.5358321380615234,
      "step": 25475
    },
    {
      "epoch": 35.66480055983205,
      "grad_norm": 0.2845735549926758,
      "learning_rate": 1.7224615384615383e-05,
      "loss": 0.5294618606567383,
      "step": 25500
    },
    {
      "epoch": 35.69979006298111,
      "grad_norm": 24.46171760559082,
      "learning_rate": 1.7182657342657343e-05,
      "loss": 0.6260689926147461,
      "step": 25525
    },
    {
      "epoch": 35.73477956613016,
      "grad_norm": 0.3592627942562103,
      "learning_rate": 1.7140699300699302e-05,
      "loss": 0.5556261825561524,
      "step": 25550
    },
    {
      "epoch": 35.769769069279214,
      "grad_norm": 0.30530691146850586,
      "learning_rate": 1.7098741258741262e-05,
      "loss": 0.5627299499511719,
      "step": 25575
    },
    {
      "epoch": 35.80475857242827,
      "grad_norm": 0.43776997923851013,
      "learning_rate": 1.7056783216783215e-05,
      "loss": 0.5509344100952148,
      "step": 25600
    },
    {
      "epoch": 35.839748075577326,
      "grad_norm": 0.7216874361038208,
      "learning_rate": 1.7014825174825175e-05,
      "loss": 0.5510917663574219,
      "step": 25625
    },
    {
      "epoch": 35.87473757872638,
      "grad_norm": 0.21126706898212433,
      "learning_rate": 1.6972867132867134e-05,
      "loss": 0.530989875793457,
      "step": 25650
    },
    {
      "epoch": 35.90972708187544,
      "grad_norm": 0.4198342263698578,
      "learning_rate": 1.6930909090909094e-05,
      "loss": 0.5248215866088867,
      "step": 25675
    },
    {
      "epoch": 35.944716585024494,
      "grad_norm": 0.7442470192909241,
      "learning_rate": 1.6888951048951047e-05,
      "loss": 0.6166294097900391,
      "step": 25700
    },
    {
      "epoch": 35.97970608817355,
      "grad_norm": 0.33656296133995056,
      "learning_rate": 1.6846993006993006e-05,
      "loss": 0.5256989669799804,
      "step": 25725
    },
    {
      "epoch": 36.0,
      "eval_loss": 0.2731800973415375,
      "eval_mean_accuracy": 0.8015711425554026,
      "eval_mean_iou": 0.7075671137937446,
      "eval_overall_accuracy": 0.8882706578973716,
      "eval_per_category_accuracy": [
        0.9116909365368185,
        0.8598954135243065,
        0.5879104483742067,
        0.6208714228245641,
        0.8375110124978056,
        0.9915476215747145
      ],
      "eval_per_category_iou": [
        0.7937251300983071,
        0.7297952925354404,
        0.4888407181638411,
        0.536891981047202,
        0.7105025802221774,
        0.9856469806954994
      ],
      "eval_runtime": 28.5286,
      "eval_samples_per_second": 11.112,
      "eval_steps_per_second": 5.573,
      "step": 25740
    },
    {
      "epoch": 36.013995801259625,
      "grad_norm": 0.2765161693096161,
      "learning_rate": 1.6805034965034966e-05,
      "loss": 0.5225089645385742,
      "step": 25750
    },
    {
      "epoch": 36.048985304408674,
      "grad_norm": 0.4741378128528595,
      "learning_rate": 1.6763076923076926e-05,
      "loss": 0.5583065414428711,
      "step": 25775
    },
    {
      "epoch": 36.08397480755773,
      "grad_norm": 0.31060853600502014,
      "learning_rate": 1.6721118881118882e-05,
      "loss": 0.5827767562866211,
      "step": 25800
    },
    {
      "epoch": 36.11896431070679,
      "grad_norm": 0.39866313338279724,
      "learning_rate": 1.6679160839160838e-05,
      "loss": 0.5605519485473632,
      "step": 25825
    },
    {
      "epoch": 36.15395381385584,
      "grad_norm": 0.355376273393631,
      "learning_rate": 1.6637202797202798e-05,
      "loss": 0.537090721130371,
      "step": 25850
    },
    {
      "epoch": 36.1889433170049,
      "grad_norm": 0.3181392550468445,
      "learning_rate": 1.6595244755244758e-05,
      "loss": 0.5336606597900391,
      "step": 25875
    },
    {
      "epoch": 36.223932820153955,
      "grad_norm": 0.48487287759780884,
      "learning_rate": 1.6553286713286714e-05,
      "loss": 0.5267918395996094,
      "step": 25900
    },
    {
      "epoch": 36.25892232330301,
      "grad_norm": 0.29594913125038147,
      "learning_rate": 1.651132867132867e-05,
      "loss": 0.539884376525879,
      "step": 25925
    },
    {
      "epoch": 36.29391182645207,
      "grad_norm": 0.5177866220474243,
      "learning_rate": 1.646937062937063e-05,
      "loss": 0.5239764785766602,
      "step": 25950
    },
    {
      "epoch": 36.328901329601116,
      "grad_norm": 0.5625590085983276,
      "learning_rate": 1.642741258741259e-05,
      "loss": 0.5811712646484375,
      "step": 25975
    },
    {
      "epoch": 36.36389083275017,
      "grad_norm": 0.9544861316680908,
      "learning_rate": 1.6385454545454546e-05,
      "loss": 0.529088249206543,
      "step": 26000
    },
    {
      "epoch": 36.39888033589923,
      "grad_norm": 0.2830820083618164,
      "learning_rate": 1.6343496503496502e-05,
      "loss": 0.5328363037109375,
      "step": 26025
    },
    {
      "epoch": 36.433869839048285,
      "grad_norm": 0.4278663992881775,
      "learning_rate": 1.630153846153846e-05,
      "loss": 0.5281327056884766,
      "step": 26050
    },
    {
      "epoch": 36.46885934219734,
      "grad_norm": 0.3389231860637665,
      "learning_rate": 1.625958041958042e-05,
      "loss": 0.5625727462768555,
      "step": 26075
    },
    {
      "epoch": 36.5038488453464,
      "grad_norm": 0.5263375043869019,
      "learning_rate": 1.6217622377622377e-05,
      "loss": 0.574017562866211,
      "step": 26100
    },
    {
      "epoch": 36.53883834849545,
      "grad_norm": 0.6719267964363098,
      "learning_rate": 1.6175664335664337e-05,
      "loss": 0.5753024291992187,
      "step": 26125
    },
    {
      "epoch": 36.57382785164451,
      "grad_norm": 0.5064644813537598,
      "learning_rate": 1.6133706293706293e-05,
      "loss": 0.5434907150268554,
      "step": 26150
    },
    {
      "epoch": 36.608817354793565,
      "grad_norm": 0.9021269679069519,
      "learning_rate": 1.6091748251748253e-05,
      "loss": 0.587193717956543,
      "step": 26175
    },
    {
      "epoch": 36.643806857942614,
      "grad_norm": 0.5936464071273804,
      "learning_rate": 1.604979020979021e-05,
      "loss": 0.5362215042114258,
      "step": 26200
    },
    {
      "epoch": 36.67879636109167,
      "grad_norm": 0.375592976808548,
      "learning_rate": 1.600783216783217e-05,
      "loss": 0.6012363815307618,
      "step": 26225
    },
    {
      "epoch": 36.71378586424073,
      "grad_norm": 0.5683634877204895,
      "learning_rate": 1.5965874125874125e-05,
      "loss": 0.5725658798217773,
      "step": 26250
    },
    {
      "epoch": 36.74877536738978,
      "grad_norm": 0.4380497336387634,
      "learning_rate": 1.5923916083916085e-05,
      "loss": 0.5348167037963867,
      "step": 26275
    },
    {
      "epoch": 36.78376487053884,
      "grad_norm": 0.44479912519454956,
      "learning_rate": 1.588195804195804e-05,
      "loss": 0.5649966430664063,
      "step": 26300
    },
    {
      "epoch": 36.818754373687895,
      "grad_norm": 0.5147042274475098,
      "learning_rate": 1.584e-05,
      "loss": 0.5713308334350586,
      "step": 26325
    },
    {
      "epoch": 36.85374387683695,
      "grad_norm": 0.9418609738349915,
      "learning_rate": 1.579804195804196e-05,
      "loss": 0.5273113632202149,
      "step": 26350
    },
    {
      "epoch": 36.88873337998601,
      "grad_norm": 0.4915006160736084,
      "learning_rate": 1.5756083916083917e-05,
      "loss": 0.5785356903076172,
      "step": 26375
    },
    {
      "epoch": 36.923722883135056,
      "grad_norm": 0.4322187304496765,
      "learning_rate": 1.5714125874125873e-05,
      "loss": 0.5290404510498047,
      "step": 26400
    },
    {
      "epoch": 36.95871238628411,
      "grad_norm": 0.5530886054039001,
      "learning_rate": 1.5672167832167833e-05,
      "loss": 0.5627269744873047,
      "step": 26425
    },
    {
      "epoch": 36.99370188943317,
      "grad_norm": 0.42856112122535706,
      "learning_rate": 1.5630209790209792e-05,
      "loss": 0.5531104278564453,
      "step": 26450
    },
    {
      "epoch": 37.0,
      "eval_loss": 0.2731762230396271,
      "eval_mean_accuracy": 0.8017074118680537,
      "eval_mean_iou": 0.707924181305991,
      "eval_overall_accuracy": 0.8883640277272895,
      "eval_per_category_accuracy": [
        0.9095732575511755,
        0.8650504819005239,
        0.5882840236052402,
        0.6214282741769598,
        0.8345726421915696,
        0.9913357917828527
      ],
      "eval_per_category_iou": [
        0.7943917002617041,
        0.7306774221001118,
        0.4889315169333929,
        0.5378034380715792,
        0.7100989268300667,
        0.9856420836390911
      ],
      "eval_runtime": 28.3832,
      "eval_samples_per_second": 11.169,
      "eval_steps_per_second": 5.602,
      "step": 26455
    },
    {
      "epoch": 37.02799160251924,
      "grad_norm": 0.15605469048023224,
      "learning_rate": 1.558825174825175e-05,
      "loss": 0.5270060729980469,
      "step": 26475
    },
    {
      "epoch": 37.0629811056683,
      "grad_norm": 0.3066793978214264,
      "learning_rate": 1.5546293706293705e-05,
      "loss": 0.5433125686645508,
      "step": 26500
    },
    {
      "epoch": 37.097970608817356,
      "grad_norm": 0.6645271182060242,
      "learning_rate": 1.5504335664335664e-05,
      "loss": 0.5533585357666015,
      "step": 26525
    },
    {
      "epoch": 37.13296011196641,
      "grad_norm": 0.4399210810661316,
      "learning_rate": 1.5462377622377624e-05,
      "loss": 0.5896754837036133,
      "step": 26550
    },
    {
      "epoch": 37.16794961511547,
      "grad_norm": 1.5239875316619873,
      "learning_rate": 1.5420419580419584e-05,
      "loss": 0.5662447738647461,
      "step": 26575
    },
    {
      "epoch": 37.202939118264524,
      "grad_norm": 0.3702499270439148,
      "learning_rate": 1.5378461538461537e-05,
      "loss": 0.598724479675293,
      "step": 26600
    },
    {
      "epoch": 37.23792862141357,
      "grad_norm": 1.0067038536071777,
      "learning_rate": 1.5336503496503496e-05,
      "loss": 0.5803433609008789,
      "step": 26625
    },
    {
      "epoch": 37.27291812456263,
      "grad_norm": 0.6575670838356018,
      "learning_rate": 1.5294545454545456e-05,
      "loss": 0.5561841583251953,
      "step": 26650
    },
    {
      "epoch": 37.307907627711685,
      "grad_norm": 0.3371434211730957,
      "learning_rate": 1.5252587412587414e-05,
      "loss": 0.5972276306152344,
      "step": 26675
    },
    {
      "epoch": 37.34289713086074,
      "grad_norm": 0.9918142557144165,
      "learning_rate": 1.521062937062937e-05,
      "loss": 0.5401960754394531,
      "step": 26700
    },
    {
      "epoch": 37.3778866340098,
      "grad_norm": 0.34265726804733276,
      "learning_rate": 1.5168671328671328e-05,
      "loss": 0.5783722686767578,
      "step": 26725
    },
    {
      "epoch": 37.412876137158854,
      "grad_norm": 0.6178107261657715,
      "learning_rate": 1.5126713286713288e-05,
      "loss": 0.546451301574707,
      "step": 26750
    },
    {
      "epoch": 37.44786564030791,
      "grad_norm": 0.16797541081905365,
      "learning_rate": 1.5084755244755246e-05,
      "loss": 0.5923558807373047,
      "step": 26775
    },
    {
      "epoch": 37.482855143456966,
      "grad_norm": 0.35894155502319336,
      "learning_rate": 1.5042797202797202e-05,
      "loss": 0.5808601379394531,
      "step": 26800
    },
    {
      "epoch": 37.517844646606015,
      "grad_norm": 0.3918348550796509,
      "learning_rate": 1.500083916083916e-05,
      "loss": 0.5775636291503906,
      "step": 26825
    },
    {
      "epoch": 37.55283414975507,
      "grad_norm": 0.9932084083557129,
      "learning_rate": 1.495888111888112e-05,
      "loss": 0.5301155471801757,
      "step": 26850
    },
    {
      "epoch": 37.58782365290413,
      "grad_norm": 0.279774010181427,
      "learning_rate": 1.4916923076923078e-05,
      "loss": 0.5468653869628907,
      "step": 26875
    },
    {
      "epoch": 37.62281315605318,
      "grad_norm": 0.6306579113006592,
      "learning_rate": 1.4874965034965036e-05,
      "loss": 0.5825395202636718,
      "step": 26900
    },
    {
      "epoch": 37.65780265920224,
      "grad_norm": 0.21579037606716156,
      "learning_rate": 1.4833006993006993e-05,
      "loss": 0.543082389831543,
      "step": 26925
    },
    {
      "epoch": 37.692792162351296,
      "grad_norm": 0.3588569462299347,
      "learning_rate": 1.4791048951048951e-05,
      "loss": 0.5013355636596679,
      "step": 26950
    },
    {
      "epoch": 37.72778166550035,
      "grad_norm": 1.202262282371521,
      "learning_rate": 1.474909090909091e-05,
      "loss": 0.5317768096923828,
      "step": 26975
    },
    {
      "epoch": 37.76277116864941,
      "grad_norm": 0.19987702369689941,
      "learning_rate": 1.4707132867132867e-05,
      "loss": 0.5339896392822265,
      "step": 27000
    },
    {
      "epoch": 37.79776067179846,
      "grad_norm": 0.5506195425987244,
      "learning_rate": 1.4665174825174825e-05,
      "loss": 0.5127936172485351,
      "step": 27025
    },
    {
      "epoch": 37.83275017494751,
      "grad_norm": 0.3336109519004822,
      "learning_rate": 1.4623216783216783e-05,
      "loss": 0.5253910827636719,
      "step": 27050
    },
    {
      "epoch": 37.86773967809657,
      "grad_norm": 0.8141899108886719,
      "learning_rate": 1.4581258741258741e-05,
      "loss": 0.5360615539550782,
      "step": 27075
    },
    {
      "epoch": 37.902729181245626,
      "grad_norm": 0.5702736973762512,
      "learning_rate": 1.4539300699300701e-05,
      "loss": 0.5230062103271484,
      "step": 27100
    },
    {
      "epoch": 37.93771868439468,
      "grad_norm": 0.3344576060771942,
      "learning_rate": 1.4497342657342657e-05,
      "loss": 0.5498506546020507,
      "step": 27125
    },
    {
      "epoch": 37.97270818754374,
      "grad_norm": 1.2178922891616821,
      "learning_rate": 1.4455384615384617e-05,
      "loss": 0.5434710311889649,
      "step": 27150
    },
    {
      "epoch": 38.0,
      "eval_loss": 0.2728460133075714,
      "eval_mean_accuracy": 0.800794484566461,
      "eval_mean_iou": 0.7075694349658642,
      "eval_overall_accuracy": 0.8884004418406201,
      "eval_per_category_accuracy": [
        0.9103114915441418,
        0.8629859931098518,
        0.5827989677752077,
        0.6194911845080386,
        0.837847335345061,
        0.9913319351164647
      ],
      "eval_per_category_iou": [
        0.794408750506665,
        0.7303580149019255,
        0.48739379463321003,
        0.536940135398927,
        0.7106617663159943,
        0.9856541480384631
      ],
      "eval_runtime": 28.6681,
      "eval_samples_per_second": 11.058,
      "eval_steps_per_second": 5.546,
      "step": 27170
    },
    {
      "epoch": 38.00699790062981,
      "grad_norm": 0.3860485255718231,
      "learning_rate": 1.4413426573426573e-05,
      "loss": 0.5073854827880859,
      "step": 27175
    },
    {
      "epoch": 38.04198740377887,
      "grad_norm": 1.1299817562103271,
      "learning_rate": 1.4371468531468533e-05,
      "loss": 0.5820503234863281,
      "step": 27200
    },
    {
      "epoch": 38.076976906927925,
      "grad_norm": 0.2598164975643158,
      "learning_rate": 1.4329510489510489e-05,
      "loss": 0.5633200073242187,
      "step": 27225
    },
    {
      "epoch": 38.111966410076974,
      "grad_norm": 1.0319688320159912,
      "learning_rate": 1.4287552447552449e-05,
      "loss": 0.5784479141235351,
      "step": 27250
    },
    {
      "epoch": 38.14695591322603,
      "grad_norm": 0.603119432926178,
      "learning_rate": 1.4245594405594405e-05,
      "loss": 0.5376014709472656,
      "step": 27275
    },
    {
      "epoch": 38.181945416375086,
      "grad_norm": 0.508566677570343,
      "learning_rate": 1.4203636363636365e-05,
      "loss": 0.527803726196289,
      "step": 27300
    },
    {
      "epoch": 38.21693491952414,
      "grad_norm": 0.2724117636680603,
      "learning_rate": 1.416167832167832e-05,
      "loss": 0.5201736831665039,
      "step": 27325
    },
    {
      "epoch": 38.2519244226732,
      "grad_norm": 0.23337464034557343,
      "learning_rate": 1.411972027972028e-05,
      "loss": 0.5793320465087891,
      "step": 27350
    },
    {
      "epoch": 38.286913925822255,
      "grad_norm": 0.25578588247299194,
      "learning_rate": 1.4077762237762238e-05,
      "loss": 0.5190819931030274,
      "step": 27375
    },
    {
      "epoch": 38.32190342897131,
      "grad_norm": 0.24206675589084625,
      "learning_rate": 1.4035804195804196e-05,
      "loss": 0.5431418228149414,
      "step": 27400
    },
    {
      "epoch": 38.35689293212037,
      "grad_norm": 0.6440945863723755,
      "learning_rate": 1.3993846153846154e-05,
      "loss": 0.5573529815673828,
      "step": 27425
    },
    {
      "epoch": 38.391882435269416,
      "grad_norm": 0.33632129430770874,
      "learning_rate": 1.3951888111888112e-05,
      "loss": 0.5907889556884766,
      "step": 27450
    },
    {
      "epoch": 38.42687193841847,
      "grad_norm": 3.276531219482422,
      "learning_rate": 1.390993006993007e-05,
      "loss": 0.579190559387207,
      "step": 27475
    },
    {
      "epoch": 38.46186144156753,
      "grad_norm": 0.4256376028060913,
      "learning_rate": 1.3867972027972028e-05,
      "loss": 0.5661272811889648,
      "step": 27500
    },
    {
      "epoch": 38.496850944716584,
      "grad_norm": 0.3247172236442566,
      "learning_rate": 1.3826013986013986e-05,
      "loss": 0.5826711654663086,
      "step": 27525
    },
    {
      "epoch": 38.53184044786564,
      "grad_norm": 0.8531012535095215,
      "learning_rate": 1.3784055944055944e-05,
      "loss": 0.5655769348144531,
      "step": 27550
    },
    {
      "epoch": 38.5668299510147,
      "grad_norm": 0.40852585434913635,
      "learning_rate": 1.3742097902097902e-05,
      "loss": 0.5407911682128906,
      "step": 27575
    },
    {
      "epoch": 38.60181945416375,
      "grad_norm": 0.4053815007209778,
      "learning_rate": 1.370013986013986e-05,
      "loss": 0.5448401260375977,
      "step": 27600
    },
    {
      "epoch": 38.63680895731281,
      "grad_norm": 0.23475001752376556,
      "learning_rate": 1.3658181818181818e-05,
      "loss": 0.5533454513549805,
      "step": 27625
    },
    {
      "epoch": 38.67179846046186,
      "grad_norm": 0.23301924765110016,
      "learning_rate": 1.3616223776223778e-05,
      "loss": 0.5002790832519531,
      "step": 27650
    },
    {
      "epoch": 38.706787963610914,
      "grad_norm": 0.4776623547077179,
      "learning_rate": 1.3574265734265734e-05,
      "loss": 0.5915674209594727,
      "step": 27675
    },
    {
      "epoch": 38.74177746675997,
      "grad_norm": 0.42178863286972046,
      "learning_rate": 1.3532307692307694e-05,
      "loss": 0.5624475479125977,
      "step": 27700
    },
    {
      "epoch": 38.776766969909026,
      "grad_norm": 0.29733365774154663,
      "learning_rate": 1.349034965034965e-05,
      "loss": 0.5226676177978515,
      "step": 27725
    },
    {
      "epoch": 38.81175647305808,
      "grad_norm": 0.3086947500705719,
      "learning_rate": 1.344839160839161e-05,
      "loss": 0.5132038497924805,
      "step": 27750
    },
    {
      "epoch": 38.84674597620714,
      "grad_norm": 1.1015111207962036,
      "learning_rate": 1.3406433566433566e-05,
      "loss": 0.5806161117553711,
      "step": 27775
    },
    {
      "epoch": 38.881735479356195,
      "grad_norm": 0.5008842945098877,
      "learning_rate": 1.3364475524475525e-05,
      "loss": 0.5594169616699218,
      "step": 27800
    },
    {
      "epoch": 38.91672498250525,
      "grad_norm": 0.36817726492881775,
      "learning_rate": 1.3322517482517482e-05,
      "loss": 0.5609522628784179,
      "step": 27825
    },
    {
      "epoch": 38.95171448565431,
      "grad_norm": 0.4318847358226776,
      "learning_rate": 1.3280559440559441e-05,
      "loss": 0.5355537414550782,
      "step": 27850
    },
    {
      "epoch": 38.986703988803356,
      "grad_norm": 0.745273232460022,
      "learning_rate": 1.3238601398601398e-05,
      "loss": 0.5355491638183594,
      "step": 27875
    },
    {
      "epoch": 39.0,
      "eval_loss": 0.2729739844799042,
      "eval_mean_accuracy": 0.8009404744384487,
      "eval_mean_iou": 0.7076635908206886,
      "eval_overall_accuracy": 0.888350814674931,
      "eval_per_category_accuracy": [
        0.9096240961675679,
        0.8611358498281232,
        0.58414612434448,
        0.6205335468260463,
        0.8385071740437325,
        0.9916960554207422
      ],
      "eval_per_category_iou": [
        0.7940431522680448,
        0.7302544134740662,
        0.4871720598572671,
        0.5381768340365561,
        0.7106557922821054,
        0.9856792930060927
      ],
      "eval_runtime": 28.4449,
      "eval_samples_per_second": 11.144,
      "eval_steps_per_second": 5.59,
      "step": 27885
    },
    {
      "epoch": 39.02099370188943,
      "grad_norm": 0.272566020488739,
      "learning_rate": 1.3196643356643357e-05,
      "loss": 0.5446853256225586,
      "step": 27900
    },
    {
      "epoch": 39.05598320503849,
      "grad_norm": 0.33249565958976746,
      "learning_rate": 1.3154685314685315e-05,
      "loss": 0.5303179168701172,
      "step": 27925
    },
    {
      "epoch": 39.09097270818754,
      "grad_norm": 0.27872541546821594,
      "learning_rate": 1.3112727272727273e-05,
      "loss": 0.5541572952270508,
      "step": 27950
    },
    {
      "epoch": 39.1259622113366,
      "grad_norm": 0.3374061584472656,
      "learning_rate": 1.3070769230769231e-05,
      "loss": 0.570179557800293,
      "step": 27975
    },
    {
      "epoch": 39.160951714485655,
      "grad_norm": 0.3379777669906616,
      "learning_rate": 1.3028811188811189e-05,
      "loss": 0.5335811614990235,
      "step": 28000
    },
    {
      "epoch": 39.19594121763471,
      "grad_norm": 1.1384391784667969,
      "learning_rate": 1.2986853146853147e-05,
      "loss": 0.5481626892089844,
      "step": 28025
    },
    {
      "epoch": 39.23093072078377,
      "grad_norm": 0.2843698263168335,
      "learning_rate": 1.2944895104895105e-05,
      "loss": 0.5447800827026367,
      "step": 28050
    },
    {
      "epoch": 39.26592022393282,
      "grad_norm": 0.3099095821380615,
      "learning_rate": 1.2902937062937063e-05,
      "loss": 0.5464514541625977,
      "step": 28075
    },
    {
      "epoch": 39.30090972708187,
      "grad_norm": 0.3294955790042877,
      "learning_rate": 1.2860979020979021e-05,
      "loss": 0.5333234405517578,
      "step": 28100
    },
    {
      "epoch": 39.33589923023093,
      "grad_norm": 0.8162382245063782,
      "learning_rate": 1.2819020979020979e-05,
      "loss": 0.5389409255981445,
      "step": 28125
    },
    {
      "epoch": 39.370888733379985,
      "grad_norm": 0.27725860476493835,
      "learning_rate": 1.2777062937062937e-05,
      "loss": 0.5313837432861328,
      "step": 28150
    },
    {
      "epoch": 39.40587823652904,
      "grad_norm": 0.3758627474308014,
      "learning_rate": 1.2735104895104895e-05,
      "loss": 0.5669450759887695,
      "step": 28175
    },
    {
      "epoch": 39.4408677396781,
      "grad_norm": 0.22956354916095734,
      "learning_rate": 1.2693146853146854e-05,
      "loss": 0.5131680297851563,
      "step": 28200
    },
    {
      "epoch": 39.47585724282715,
      "grad_norm": 0.2393501251935959,
      "learning_rate": 1.265118881118881e-05,
      "loss": 0.5249968338012695,
      "step": 28225
    },
    {
      "epoch": 39.51084674597621,
      "grad_norm": 0.27007895708084106,
      "learning_rate": 1.260923076923077e-05,
      "loss": 0.5295429229736328,
      "step": 28250
    },
    {
      "epoch": 39.545836249125266,
      "grad_norm": 0.3772808313369751,
      "learning_rate": 1.2567272727272727e-05,
      "loss": 0.5476557159423828,
      "step": 28275
    },
    {
      "epoch": 39.580825752274315,
      "grad_norm": 0.6256835460662842,
      "learning_rate": 1.2525314685314686e-05,
      "loss": 0.5213619232177734,
      "step": 28300
    },
    {
      "epoch": 39.61581525542337,
      "grad_norm": 0.45847436785697937,
      "learning_rate": 1.2483356643356643e-05,
      "loss": 0.5561267471313477,
      "step": 28325
    },
    {
      "epoch": 39.65080475857243,
      "grad_norm": 0.37153303623199463,
      "learning_rate": 1.2441398601398602e-05,
      "loss": 0.576710205078125,
      "step": 28350
    },
    {
      "epoch": 39.68579426172148,
      "grad_norm": 0.77175372838974,
      "learning_rate": 1.2399440559440558e-05,
      "loss": 0.5559063720703125,
      "step": 28375
    },
    {
      "epoch": 39.72078376487054,
      "grad_norm": 0.3488365709781647,
      "learning_rate": 1.2357482517482518e-05,
      "loss": 0.5851258087158203,
      "step": 28400
    },
    {
      "epoch": 39.755773268019595,
      "grad_norm": 0.32712554931640625,
      "learning_rate": 1.2315524475524474e-05,
      "loss": 0.5517578506469727,
      "step": 28425
    },
    {
      "epoch": 39.79076277116865,
      "grad_norm": 1.4048124551773071,
      "learning_rate": 1.2273566433566434e-05,
      "loss": 0.5783472061157227,
      "step": 28450
    },
    {
      "epoch": 39.82575227431771,
      "grad_norm": 0.38608577847480774,
      "learning_rate": 1.2231608391608392e-05,
      "loss": 0.5937903213500977,
      "step": 28475
    },
    {
      "epoch": 39.86074177746676,
      "grad_norm": 1.3895678520202637,
      "learning_rate": 1.218965034965035e-05,
      "loss": 0.569785385131836,
      "step": 28500
    },
    {
      "epoch": 39.89573128061581,
      "grad_norm": 0.3882584869861603,
      "learning_rate": 1.2147692307692308e-05,
      "loss": 0.5291624069213867,
      "step": 28525
    },
    {
      "epoch": 39.93072078376487,
      "grad_norm": 0.4323296844959259,
      "learning_rate": 1.2105734265734266e-05,
      "loss": 0.5913959503173828,
      "step": 28550
    },
    {
      "epoch": 39.965710286913925,
      "grad_norm": 2.9416005611419678,
      "learning_rate": 1.2063776223776224e-05,
      "loss": 0.5293656158447265,
      "step": 28575
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.5244597792625427,
      "learning_rate": 1.2021818181818182e-05,
      "loss": 0.5697238540649414,
      "step": 28600
    },
    {
      "epoch": 40.0,
      "eval_loss": 0.2723241448402405,
      "eval_mean_accuracy": 0.8025925027366728,
      "eval_mean_iou": 0.7085157151849056,
      "eval_overall_accuracy": 0.8885278671697864,
      "eval_per_category_accuracy": [
        0.9113976001070236,
        0.8637115291031737,
        0.5928103196764681,
        0.6221040261739953,
        0.8339821030079331,
        0.9915494383514428
      ],
      "eval_per_category_iou": [
        0.7942328857857798,
        0.7306998424367132,
        0.49106979478404833,
        0.5389039715517041,
        0.7105296411308805,
        0.9856581554203075
      ],
      "eval_runtime": 29.1488,
      "eval_samples_per_second": 10.875,
      "eval_steps_per_second": 5.455,
      "step": 28600
    },
    {
      "epoch": 40.034989503149056,
      "grad_norm": 0.30170926451683044,
      "learning_rate": 1.197986013986014e-05,
      "loss": 0.5712070465087891,
      "step": 28625
    },
    {
      "epoch": 40.06997900629811,
      "grad_norm": 0.30143260955810547,
      "learning_rate": 1.1937902097902098e-05,
      "loss": 0.5567679977416993,
      "step": 28650
    },
    {
      "epoch": 40.10496850944717,
      "grad_norm": 0.3633961081504822,
      "learning_rate": 1.1895944055944056e-05,
      "loss": 0.5495080184936524,
      "step": 28675
    },
    {
      "epoch": 40.139958012596225,
      "grad_norm": 0.5221279263496399,
      "learning_rate": 1.1853986013986015e-05,
      "loss": 0.5350308990478516,
      "step": 28700
    },
    {
      "epoch": 40.174947515745274,
      "grad_norm": 0.43306487798690796,
      "learning_rate": 1.1812027972027972e-05,
      "loss": 0.5394387054443359,
      "step": 28725
    },
    {
      "epoch": 40.20993701889433,
      "grad_norm": 0.45633313059806824,
      "learning_rate": 1.1770069930069931e-05,
      "loss": 0.5790089797973633,
      "step": 28750
    },
    {
      "epoch": 40.244926522043386,
      "grad_norm": 0.4613235294818878,
      "learning_rate": 1.1728111888111887e-05,
      "loss": 0.5422514343261718,
      "step": 28775
    },
    {
      "epoch": 40.27991602519244,
      "grad_norm": 1.1030573844909668,
      "learning_rate": 1.1686153846153847e-05,
      "loss": 0.5327951049804688,
      "step": 28800
    },
    {
      "epoch": 40.3149055283415,
      "grad_norm": 0.24350447952747345,
      "learning_rate": 1.1644195804195803e-05,
      "loss": 0.5887596130371093,
      "step": 28825
    },
    {
      "epoch": 40.349895031490554,
      "grad_norm": 0.23182442784309387,
      "learning_rate": 1.1602237762237763e-05,
      "loss": 0.5680994415283203,
      "step": 28850
    },
    {
      "epoch": 40.38488453463961,
      "grad_norm": 0.3809875249862671,
      "learning_rate": 1.156027972027972e-05,
      "loss": 0.5557815551757812,
      "step": 28875
    },
    {
      "epoch": 40.41987403778867,
      "grad_norm": 0.6556278467178345,
      "learning_rate": 1.1518321678321679e-05,
      "loss": 0.5233135986328125,
      "step": 28900
    },
    {
      "epoch": 40.454863540937716,
      "grad_norm": 0.38319751620292664,
      "learning_rate": 1.1476363636363635e-05,
      "loss": 0.5617900848388672,
      "step": 28925
    },
    {
      "epoch": 40.48985304408677,
      "grad_norm": 0.37813010811805725,
      "learning_rate": 1.1434405594405595e-05,
      "loss": 0.5541060256958008,
      "step": 28950
    },
    {
      "epoch": 40.52484254723583,
      "grad_norm": 0.3110434114933014,
      "learning_rate": 1.1392447552447553e-05,
      "loss": 0.5434075927734375,
      "step": 28975
    },
    {
      "epoch": 40.559832050384884,
      "grad_norm": 0.5554558038711548,
      "learning_rate": 1.1352167832167832e-05,
      "loss": 0.5318222045898438,
      "step": 29000
    },
    {
      "epoch": 40.59482155353394,
      "grad_norm": 0.5346034169197083,
      "learning_rate": 1.1310209790209791e-05,
      "loss": 0.5567483901977539,
      "step": 29025
    },
    {
      "epoch": 40.629811056682996,
      "grad_norm": 0.39166656136512756,
      "learning_rate": 1.1268251748251748e-05,
      "loss": 0.5692497634887695,
      "step": 29050
    },
    {
      "epoch": 40.66480055983205,
      "grad_norm": 0.6291293501853943,
      "learning_rate": 1.1226293706293707e-05,
      "loss": 0.5464270782470703,
      "step": 29075
    },
    {
      "epoch": 40.69979006298111,
      "grad_norm": 0.33323177695274353,
      "learning_rate": 1.1184335664335663e-05,
      "loss": 0.5488148880004883,
      "step": 29100
    },
    {
      "epoch": 40.73477956613016,
      "grad_norm": 0.35961446166038513,
      "learning_rate": 1.1142377622377623e-05,
      "loss": 0.5469348907470704,
      "step": 29125
    },
    {
      "epoch": 40.769769069279214,
      "grad_norm": 0.5414283275604248,
      "learning_rate": 1.110041958041958e-05,
      "loss": 0.5355710983276367,
      "step": 29150
    },
    {
      "epoch": 40.80475857242827,
      "grad_norm": 0.2699940800666809,
      "learning_rate": 1.1058461538461539e-05,
      "loss": 0.5529683685302734,
      "step": 29175
    },
    {
      "epoch": 40.839748075577326,
      "grad_norm": 0.5916764736175537,
      "learning_rate": 1.1016503496503495e-05,
      "loss": 0.5048091125488281,
      "step": 29200
    },
    {
      "epoch": 40.87473757872638,
      "grad_norm": 0.7103211879730225,
      "learning_rate": 1.0974545454545455e-05,
      "loss": 0.5734378433227539,
      "step": 29225
    },
    {
      "epoch": 40.90972708187544,
      "grad_norm": 0.9701446890830994,
      "learning_rate": 1.0932587412587413e-05,
      "loss": 0.5363559722900391,
      "step": 29250
    },
    {
      "epoch": 40.944716585024494,
      "grad_norm": 0.5257514715194702,
      "learning_rate": 1.0890629370629371e-05,
      "loss": 0.5612339019775391,
      "step": 29275
    },
    {
      "epoch": 40.97970608817355,
      "grad_norm": 1.6206083297729492,
      "learning_rate": 1.0848671328671329e-05,
      "loss": 0.5630210876464844,
      "step": 29300
    },
    {
      "epoch": 41.0,
      "eval_loss": 0.2722874879837036,
      "eval_mean_accuracy": 0.8015302918174981,
      "eval_mean_iou": 0.708095901848686,
      "eval_overall_accuracy": 0.8884892147798268,
      "eval_per_category_accuracy": [
        0.9089881361047619,
        0.8639266401317919,
        0.587421514826133,
        0.6212895567582135,
        0.8356335474042093,
        0.9919223556798783
      ],
      "eval_per_category_iou": [
        0.7942298418021304,
        0.730674270799415,
        0.48916827205830415,
        0.5383407469223088,
        0.7105126389348412,
        0.9856496405751165
      ],
      "eval_runtime": 28.3785,
      "eval_samples_per_second": 11.17,
      "eval_steps_per_second": 5.603,
      "step": 29315
    },
    {
      "epoch": 41.013995801259625,
      "grad_norm": 0.22225627303123474,
      "learning_rate": 1.0806713286713287e-05,
      "loss": 0.5573562240600586,
      "step": 29325
    },
    {
      "epoch": 41.048985304408674,
      "grad_norm": 0.5482975840568542,
      "learning_rate": 1.0764755244755245e-05,
      "loss": 0.5545416641235351,
      "step": 29350
    },
    {
      "epoch": 41.08397480755773,
      "grad_norm": 0.8674829006195068,
      "learning_rate": 1.0722797202797203e-05,
      "loss": 0.5803125381469727,
      "step": 29375
    },
    {
      "epoch": 41.11896431070679,
      "grad_norm": 0.2478172481060028,
      "learning_rate": 1.068083916083916e-05,
      "loss": 0.5890670013427735,
      "step": 29400
    },
    {
      "epoch": 41.15395381385584,
      "grad_norm": 0.3076089024543762,
      "learning_rate": 1.0638881118881119e-05,
      "loss": 0.5718034744262696,
      "step": 29425
    },
    {
      "epoch": 41.1889433170049,
      "grad_norm": 0.3108978569507599,
      "learning_rate": 1.0596923076923077e-05,
      "loss": 0.5173099899291992,
      "step": 29450
    },
    {
      "epoch": 41.223932820153955,
      "grad_norm": 1.5808448791503906,
      "learning_rate": 1.0554965034965035e-05,
      "loss": 0.5500963211059571,
      "step": 29475
    },
    {
      "epoch": 41.25892232330301,
      "grad_norm": 0.30324557423591614,
      "learning_rate": 1.0513006993006993e-05,
      "loss": 0.5367975234985352,
      "step": 29500
    },
    {
      "epoch": 41.29391182645207,
      "grad_norm": 0.3270486891269684,
      "learning_rate": 1.0471048951048952e-05,
      "loss": 0.5548620986938476,
      "step": 29525
    },
    {
      "epoch": 41.328901329601116,
      "grad_norm": 0.3053951561450958,
      "learning_rate": 1.0429090909090908e-05,
      "loss": 0.5067691802978516,
      "step": 29550
    },
    {
      "epoch": 41.36389083275017,
      "grad_norm": 0.7729083299636841,
      "learning_rate": 1.0387132867132868e-05,
      "loss": 0.5447526550292969,
      "step": 29575
    },
    {
      "epoch": 41.39888033589923,
      "grad_norm": 0.41154858469963074,
      "learning_rate": 1.0345174825174824e-05,
      "loss": 0.5098091888427735,
      "step": 29600
    },
    {
      "epoch": 41.433869839048285,
      "grad_norm": 0.34770864248275757,
      "learning_rate": 1.0303216783216784e-05,
      "loss": 0.5115238189697265,
      "step": 29625
    },
    {
      "epoch": 41.46885934219734,
      "grad_norm": 0.20149953663349152,
      "learning_rate": 1.026125874125874e-05,
      "loss": 0.5813362503051758,
      "step": 29650
    },
    {
      "epoch": 41.5038488453464,
      "grad_norm": 0.30595916509628296,
      "learning_rate": 1.02193006993007e-05,
      "loss": 0.562796630859375,
      "step": 29675
    },
    {
      "epoch": 41.53883834849545,
      "grad_norm": 0.6651882529258728,
      "learning_rate": 1.0177342657342656e-05,
      "loss": 0.5477270889282226,
      "step": 29700
    },
    {
      "epoch": 41.57382785164451,
      "grad_norm": 0.32108402252197266,
      "learning_rate": 1.0135384615384616e-05,
      "loss": 0.5280309677124023,
      "step": 29725
    },
    {
      "epoch": 41.608817354793565,
      "grad_norm": 0.5755075812339783,
      "learning_rate": 1.0093426573426574e-05,
      "loss": 0.5569377899169922,
      "step": 29750
    },
    {
      "epoch": 41.643806857942614,
      "grad_norm": 0.4250805079936981,
      "learning_rate": 1.0051468531468532e-05,
      "loss": 0.520923957824707,
      "step": 29775
    },
    {
      "epoch": 41.67879636109167,
      "grad_norm": 0.4253261387348175,
      "learning_rate": 1.000951048951049e-05,
      "loss": 0.5724120330810547,
      "step": 29800
    },
    {
      "epoch": 41.71378586424073,
      "grad_norm": 0.2937345802783966,
      "learning_rate": 9.967552447552448e-06,
      "loss": 0.5843131637573242,
      "step": 29825
    },
    {
      "epoch": 41.74877536738978,
      "grad_norm": 0.3722572922706604,
      "learning_rate": 9.925594405594406e-06,
      "loss": 0.5816690063476563,
      "step": 29850
    },
    {
      "epoch": 41.78376487053884,
      "grad_norm": 0.28141161799430847,
      "learning_rate": 9.883636363636364e-06,
      "loss": 0.543703956604004,
      "step": 29875
    },
    {
      "epoch": 41.818754373687895,
      "grad_norm": 1.0368843078613281,
      "learning_rate": 9.841678321678323e-06,
      "loss": 0.558647689819336,
      "step": 29900
    },
    {
      "epoch": 41.85374387683695,
      "grad_norm": 0.515565037727356,
      "learning_rate": 9.79972027972028e-06,
      "loss": 0.5492588424682617,
      "step": 29925
    },
    {
      "epoch": 41.88873337998601,
      "grad_norm": 0.644529402256012,
      "learning_rate": 9.757762237762239e-06,
      "loss": 0.5730354690551758,
      "step": 29950
    },
    {
      "epoch": 41.923722883135056,
      "grad_norm": 0.32930973172187805,
      "learning_rate": 9.715804195804195e-06,
      "loss": 0.5665654373168946,
      "step": 29975
    },
    {
      "epoch": 41.95871238628411,
      "grad_norm": 0.40075212717056274,
      "learning_rate": 9.673846153846155e-06,
      "loss": 0.535405158996582,
      "step": 30000
    },
    {
      "epoch": 41.99370188943317,
      "grad_norm": 0.19374820590019226,
      "learning_rate": 9.631888111888111e-06,
      "loss": 0.5204108428955078,
      "step": 30025
    },
    {
      "epoch": 42.0,
      "eval_loss": 0.2717898488044739,
      "eval_mean_accuracy": 0.8029225454966444,
      "eval_mean_iou": 0.709128302952815,
      "eval_overall_accuracy": 0.8886932108304478,
      "eval_per_category_accuracy": [
        0.9093873266489703,
        0.8632258951987077,
        0.595471596764467,
        0.6228005857838426,
        0.8350028661348,
        0.9916470024490788
      ],
      "eval_per_category_iou": [
        0.7946803918707908,
        0.730837419080861,
        0.4923178294241742,
        0.540286458176642,
        0.7109684998488441,
        0.9856792193155784
      ],
      "eval_runtime": 28.7413,
      "eval_samples_per_second": 11.029,
      "eval_steps_per_second": 5.532,
      "step": 30030
    },
    {
      "epoch": 42.02799160251924,
      "grad_norm": 0.3046818673610687,
      "learning_rate": 9.589930069930071e-06,
      "loss": 0.5578059768676757,
      "step": 30050
    },
    {
      "epoch": 42.0629811056683,
      "grad_norm": 3.605863571166992,
      "learning_rate": 9.547972027972029e-06,
      "loss": 0.5460987091064453,
      "step": 30075
    },
    {
      "epoch": 42.097970608817356,
      "grad_norm": 0.21172726154327393,
      "learning_rate": 9.506013986013987e-06,
      "loss": 0.5488594436645508,
      "step": 30100
    },
    {
      "epoch": 42.13296011196641,
      "grad_norm": 0.6086469292640686,
      "learning_rate": 9.464055944055945e-06,
      "loss": 0.5125604248046876,
      "step": 30125
    },
    {
      "epoch": 42.16794961511547,
      "grad_norm": 0.8418154716491699,
      "learning_rate": 9.422097902097903e-06,
      "loss": 0.515859260559082,
      "step": 30150
    },
    {
      "epoch": 42.202939118264524,
      "grad_norm": 0.2791895568370819,
      "learning_rate": 9.38013986013986e-06,
      "loss": 0.5646797561645508,
      "step": 30175
    },
    {
      "epoch": 42.23792862141357,
      "grad_norm": 1.1325465440750122,
      "learning_rate": 9.338181818181819e-06,
      "loss": 0.5963893127441406,
      "step": 30200
    },
    {
      "epoch": 42.27291812456263,
      "grad_norm": 0.2552729845046997,
      "learning_rate": 9.296223776223777e-06,
      "loss": 0.5130685424804687,
      "step": 30225
    },
    {
      "epoch": 42.307907627711685,
      "grad_norm": 0.32009610533714294,
      "learning_rate": 9.254265734265735e-06,
      "loss": 0.5379290390014648,
      "step": 30250
    },
    {
      "epoch": 42.34289713086074,
      "grad_norm": 0.38517048954963684,
      "learning_rate": 9.212307692307693e-06,
      "loss": 0.5610607147216797,
      "step": 30275
    },
    {
      "epoch": 42.3778866340098,
      "grad_norm": 0.4459473490715027,
      "learning_rate": 9.17034965034965e-06,
      "loss": 0.5757244110107422,
      "step": 30300
    },
    {
      "epoch": 42.412876137158854,
      "grad_norm": 0.3945732116699219,
      "learning_rate": 9.128391608391609e-06,
      "loss": 0.5765081024169922,
      "step": 30325
    },
    {
      "epoch": 42.44786564030791,
      "grad_norm": 0.432070791721344,
      "learning_rate": 9.086433566433568e-06,
      "loss": 0.5722270965576172,
      "step": 30350
    },
    {
      "epoch": 42.482855143456966,
      "grad_norm": 1.1488293409347534,
      "learning_rate": 9.044475524475524e-06,
      "loss": 0.5749463272094727,
      "step": 30375
    },
    {
      "epoch": 42.517844646606015,
      "grad_norm": 4.351038455963135,
      "learning_rate": 9.002517482517484e-06,
      "loss": 0.5042228317260742,
      "step": 30400
    },
    {
      "epoch": 42.55283414975507,
      "grad_norm": 1.0850661993026733,
      "learning_rate": 8.96055944055944e-06,
      "loss": 0.5465306091308594,
      "step": 30425
    },
    {
      "epoch": 42.58782365290413,
      "grad_norm": 1.5502506494522095,
      "learning_rate": 8.9186013986014e-06,
      "loss": 0.5876327514648437,
      "step": 30450
    },
    {
      "epoch": 42.62281315605318,
      "grad_norm": 0.3451738953590393,
      "learning_rate": 8.876643356643356e-06,
      "loss": 0.5500315475463867,
      "step": 30475
    },
    {
      "epoch": 42.65780265920224,
      "grad_norm": 0.3460814654827118,
      "learning_rate": 8.834685314685316e-06,
      "loss": 0.5158565521240235,
      "step": 30500
    },
    {
      "epoch": 42.692792162351296,
      "grad_norm": 0.22632735967636108,
      "learning_rate": 8.792727272727272e-06,
      "loss": 0.5083647918701172,
      "step": 30525
    },
    {
      "epoch": 42.72778166550035,
      "grad_norm": 0.40394777059555054,
      "learning_rate": 8.750769230769232e-06,
      "loss": 0.5283424758911133,
      "step": 30550
    },
    {
      "epoch": 42.76277116864941,
      "grad_norm": 0.2370431274175644,
      "learning_rate": 8.708811188811188e-06,
      "loss": 0.5701679611206054,
      "step": 30575
    },
    {
      "epoch": 42.79776067179846,
      "grad_norm": 0.26850345730781555,
      "learning_rate": 8.666853146853148e-06,
      "loss": 0.558592529296875,
      "step": 30600
    },
    {
      "epoch": 42.83275017494751,
      "grad_norm": 0.3971259891986847,
      "learning_rate": 8.624895104895106e-06,
      "loss": 0.5953705978393554,
      "step": 30625
    },
    {
      "epoch": 42.86773967809657,
      "grad_norm": 0.9954419136047363,
      "learning_rate": 8.582937062937064e-06,
      "loss": 0.5688414001464843,
      "step": 30650
    },
    {
      "epoch": 42.902729181245626,
      "grad_norm": 0.3032522201538086,
      "learning_rate": 8.540979020979022e-06,
      "loss": 0.5181986236572266,
      "step": 30675
    },
    {
      "epoch": 42.93771868439468,
      "grad_norm": 1.2863672971725464,
      "learning_rate": 8.49902097902098e-06,
      "loss": 0.5588911056518555,
      "step": 30700
    },
    {
      "epoch": 42.97270818754374,
      "grad_norm": 0.9118609428405762,
      "learning_rate": 8.457062937062938e-06,
      "loss": 0.55342529296875,
      "step": 30725
    },
    {
      "epoch": 43.0,
      "eval_loss": 0.27213120460510254,
      "eval_mean_accuracy": 0.8029670847979195,
      "eval_mean_iou": 0.7087934774453037,
      "eval_overall_accuracy": 0.8886018386022526,
      "eval_per_category_accuracy": [
        0.911298667682166,
        0.8649563229805237,
        0.5924124584839524,
        0.6242898163579543,
        0.8333460053410741,
        0.9914992379418457
      ],
      "eval_per_category_iou": [
        0.7946770850471908,
        0.7308885007149096,
        0.4912535504154177,
        0.5397740025872335,
        0.7104862689629549,
        0.9856814569441156
      ],
      "eval_runtime": 28.5272,
      "eval_samples_per_second": 11.112,
      "eval_steps_per_second": 5.574,
      "step": 30745
    },
    {
      "epoch": 43.00699790062981,
      "grad_norm": 0.3689621686935425,
      "learning_rate": 8.415104895104895e-06,
      "loss": 0.5570938873291016,
      "step": 30750
    },
    {
      "epoch": 43.04198740377887,
      "grad_norm": 0.5031446218490601,
      "learning_rate": 8.373146853146853e-06,
      "loss": 0.578545913696289,
      "step": 30775
    },
    {
      "epoch": 43.076976906927925,
      "grad_norm": 0.4329338073730469,
      "learning_rate": 8.331188811188811e-06,
      "loss": 0.5206899642944336,
      "step": 30800
    },
    {
      "epoch": 43.111966410076974,
      "grad_norm": 0.2667541205883026,
      "learning_rate": 8.28923076923077e-06,
      "loss": 0.5390827178955078,
      "step": 30825
    },
    {
      "epoch": 43.14695591322603,
      "grad_norm": 0.20953671634197235,
      "learning_rate": 8.247272727272729e-06,
      "loss": 0.5547474288940429,
      "step": 30850
    },
    {
      "epoch": 43.181945416375086,
      "grad_norm": 7.519964694976807,
      "learning_rate": 8.205314685314685e-06,
      "loss": 0.5201763153076172,
      "step": 30875
    },
    {
      "epoch": 43.21693491952414,
      "grad_norm": 0.3197386860847473,
      "learning_rate": 8.163356643356645e-06,
      "loss": 0.5231234741210937,
      "step": 30900
    },
    {
      "epoch": 43.2519244226732,
      "grad_norm": 0.855815052986145,
      "learning_rate": 8.121398601398601e-06,
      "loss": 0.5322111511230468,
      "step": 30925
    },
    {
      "epoch": 43.286913925822255,
      "grad_norm": 0.3062503933906555,
      "learning_rate": 8.07944055944056e-06,
      "loss": 0.542504005432129,
      "step": 30950
    },
    {
      "epoch": 43.32190342897131,
      "grad_norm": 0.69554203748703,
      "learning_rate": 8.037482517482517e-06,
      "loss": 0.5662565612792969,
      "step": 30975
    },
    {
      "epoch": 43.35689293212037,
      "grad_norm": 0.29079994559288025,
      "learning_rate": 7.997202797202798e-06,
      "loss": 0.563226089477539,
      "step": 31000
    },
    {
      "epoch": 43.391882435269416,
      "grad_norm": 0.48080945014953613,
      "learning_rate": 7.955244755244756e-06,
      "loss": 0.5584482192993164,
      "step": 31025
    },
    {
      "epoch": 43.42687193841847,
      "grad_norm": 0.28833484649658203,
      "learning_rate": 7.913286713286714e-06,
      "loss": 0.5075073623657227,
      "step": 31050
    },
    {
      "epoch": 43.46186144156753,
      "grad_norm": 0.5601829886436462,
      "learning_rate": 7.871328671328671e-06,
      "loss": 0.54233154296875,
      "step": 31075
    },
    {
      "epoch": 43.496850944716584,
      "grad_norm": 0.44663721323013306,
      "learning_rate": 7.82937062937063e-06,
      "loss": 0.5208878707885742,
      "step": 31100
    },
    {
      "epoch": 43.53184044786564,
      "grad_norm": 0.3320581316947937,
      "learning_rate": 7.787412587412589e-06,
      "loss": 0.5435289001464844,
      "step": 31125
    },
    {
      "epoch": 43.5668299510147,
      "grad_norm": 0.30741292238235474,
      "learning_rate": 7.745454545454545e-06,
      "loss": 0.5701169967651367,
      "step": 31150
    },
    {
      "epoch": 43.60181945416375,
      "grad_norm": 0.42658621072769165,
      "learning_rate": 7.703496503496505e-06,
      "loss": 0.5678354644775391,
      "step": 31175
    },
    {
      "epoch": 43.63680895731281,
      "grad_norm": 0.3870907723903656,
      "learning_rate": 7.661538461538461e-06,
      "loss": 0.5474755096435547,
      "step": 31200
    },
    {
      "epoch": 43.67179846046186,
      "grad_norm": 0.1636395901441574,
      "learning_rate": 7.61958041958042e-06,
      "loss": 0.5573839569091796,
      "step": 31225
    },
    {
      "epoch": 43.706787963610914,
      "grad_norm": 0.29343464970588684,
      "learning_rate": 7.577622377622377e-06,
      "loss": 0.5160100173950195,
      "step": 31250
    },
    {
      "epoch": 43.74177746675997,
      "grad_norm": 0.49490416049957275,
      "learning_rate": 7.535664335664336e-06,
      "loss": 0.5728203582763672,
      "step": 31275
    },
    {
      "epoch": 43.776766969909026,
      "grad_norm": 0.6370674967765808,
      "learning_rate": 7.493706293706294e-06,
      "loss": 0.5912434005737305,
      "step": 31300
    },
    {
      "epoch": 43.81175647305808,
      "grad_norm": 0.2872733771800995,
      "learning_rate": 7.451748251748252e-06,
      "loss": 0.5840276336669922,
      "step": 31325
    },
    {
      "epoch": 43.84674597620714,
      "grad_norm": 1.1300822496414185,
      "learning_rate": 7.40979020979021e-06,
      "loss": 0.5572877883911133,
      "step": 31350
    },
    {
      "epoch": 43.881735479356195,
      "grad_norm": 0.6368117928504944,
      "learning_rate": 7.367832167832168e-06,
      "loss": 0.5323922729492188,
      "step": 31375
    },
    {
      "epoch": 43.91672498250525,
      "grad_norm": 0.33337995409965515,
      "learning_rate": 7.325874125874126e-06,
      "loss": 0.5364396286010742,
      "step": 31400
    },
    {
      "epoch": 43.95171448565431,
      "grad_norm": 2.4588398933410645,
      "learning_rate": 7.283916083916084e-06,
      "loss": 0.5810300827026367,
      "step": 31425
    },
    {
      "epoch": 43.986703988803356,
      "grad_norm": 2.07716703414917,
      "learning_rate": 7.241958041958042e-06,
      "loss": 0.5468104553222656,
      "step": 31450
    },
    {
      "epoch": 44.0,
      "eval_loss": 0.27186623215675354,
      "eval_mean_accuracy": 0.8024271751741113,
      "eval_mean_iou": 0.7089130471708883,
      "eval_overall_accuracy": 0.8886922601645677,
      "eval_per_category_accuracy": [
        0.9096883008051715,
        0.8645142059252636,
        0.59036190189793,
        0.6231117091373164,
        0.8353141318162831,
        0.9915728014627029
      ],
      "eval_per_category_iou": [
        0.7949870109363859,
        0.7309415044378911,
        0.4909174875247148,
        0.5401477847321177,
        0.7108063323777224,
        0.9856781630164978
      ],
      "eval_runtime": 28.9776,
      "eval_samples_per_second": 10.939,
      "eval_steps_per_second": 5.487,
      "step": 31460
    },
    {
      "epoch": 44.02099370188943,
      "grad_norm": 1.4330538511276245,
      "learning_rate": 7.2e-06,
      "loss": 0.5465479278564453,
      "step": 31475
    },
    {
      "epoch": 44.05598320503849,
      "grad_norm": 0.44031789898872375,
      "learning_rate": 7.158041958041958e-06,
      "loss": 0.5576197052001953,
      "step": 31500
    },
    {
      "epoch": 44.09097270818754,
      "grad_norm": 0.15685506165027618,
      "learning_rate": 7.1160839160839164e-06,
      "loss": 0.536134910583496,
      "step": 31525
    },
    {
      "epoch": 44.1259622113366,
      "grad_norm": 0.3349081873893738,
      "learning_rate": 7.074125874125874e-06,
      "loss": 0.5536564636230469,
      "step": 31550
    },
    {
      "epoch": 44.160951714485655,
      "grad_norm": 0.6829960942268372,
      "learning_rate": 7.032167832167832e-06,
      "loss": 0.5397949600219727,
      "step": 31575
    },
    {
      "epoch": 44.19594121763471,
      "grad_norm": 0.22264081239700317,
      "learning_rate": 6.99020979020979e-06,
      "loss": 0.5424152374267578,
      "step": 31600
    },
    {
      "epoch": 44.23093072078377,
      "grad_norm": 0.35535717010498047,
      "learning_rate": 6.948251748251748e-06,
      "loss": 0.5395198440551758,
      "step": 31625
    },
    {
      "epoch": 44.26592022393282,
      "grad_norm": 0.31406018137931824,
      "learning_rate": 6.906293706293706e-06,
      "loss": 0.5621606063842773,
      "step": 31650
    },
    {
      "epoch": 44.30090972708187,
      "grad_norm": 0.35473209619522095,
      "learning_rate": 6.864335664335664e-06,
      "loss": 0.5338006210327149,
      "step": 31675
    },
    {
      "epoch": 44.33589923023093,
      "grad_norm": 0.5077213644981384,
      "learning_rate": 6.822377622377622e-06,
      "loss": 0.5426163864135742,
      "step": 31700
    },
    {
      "epoch": 44.370888733379985,
      "grad_norm": 0.2946932017803192,
      "learning_rate": 6.78041958041958e-06,
      "loss": 0.5732745742797851,
      "step": 31725
    },
    {
      "epoch": 44.40587823652904,
      "grad_norm": 0.5476751327514648,
      "learning_rate": 6.738461538461538e-06,
      "loss": 0.5574369812011719,
      "step": 31750
    },
    {
      "epoch": 44.4408677396781,
      "grad_norm": 0.6827191710472107,
      "learning_rate": 6.696503496503496e-06,
      "loss": 0.5130380630493164,
      "step": 31775
    },
    {
      "epoch": 44.47585724282715,
      "grad_norm": 0.5629139542579651,
      "learning_rate": 6.654545454545455e-06,
      "loss": 0.5637790298461914,
      "step": 31800
    },
    {
      "epoch": 44.51084674597621,
      "grad_norm": 0.7402380108833313,
      "learning_rate": 6.612587412587413e-06,
      "loss": 0.5465436172485352,
      "step": 31825
    },
    {
      "epoch": 44.545836249125266,
      "grad_norm": 0.9747424721717834,
      "learning_rate": 6.570629370629371e-06,
      "loss": 0.5480456924438477,
      "step": 31850
    },
    {
      "epoch": 44.580825752274315,
      "grad_norm": 0.6195854544639587,
      "learning_rate": 6.528671328671329e-06,
      "loss": 0.5826597213745117,
      "step": 31875
    },
    {
      "epoch": 44.61581525542337,
      "grad_norm": 0.4593654274940491,
      "learning_rate": 6.486713286713287e-06,
      "loss": 0.5690353012084961,
      "step": 31900
    },
    {
      "epoch": 44.65080475857243,
      "grad_norm": 0.42223232984542847,
      "learning_rate": 6.444755244755245e-06,
      "loss": 0.5517231750488282,
      "step": 31925
    },
    {
      "epoch": 44.68579426172148,
      "grad_norm": 0.492626428604126,
      "learning_rate": 6.4027972027972026e-06,
      "loss": 0.5492419052124023,
      "step": 31950
    },
    {
      "epoch": 44.72078376487054,
      "grad_norm": 0.37416353821754456,
      "learning_rate": 6.3608391608391605e-06,
      "loss": 0.5902960205078125,
      "step": 31975
    },
    {
      "epoch": 44.755773268019595,
      "grad_norm": 0.30660662055015564,
      "learning_rate": 6.3188811188811185e-06,
      "loss": 0.5426148223876953,
      "step": 32000
    },
    {
      "epoch": 44.79076277116865,
      "grad_norm": 0.6173593401908875,
      "learning_rate": 6.2769230769230764e-06,
      "loss": 0.5317655563354492,
      "step": 32025
    },
    {
      "epoch": 44.82575227431771,
      "grad_norm": 0.1887216717004776,
      "learning_rate": 6.234965034965034e-06,
      "loss": 0.5271593475341797,
      "step": 32050
    },
    {
      "epoch": 44.86074177746676,
      "grad_norm": 0.4628385603427887,
      "learning_rate": 6.193006993006993e-06,
      "loss": 0.567253761291504,
      "step": 32075
    },
    {
      "epoch": 44.89573128061581,
      "grad_norm": 0.1661812961101532,
      "learning_rate": 6.151048951048951e-06,
      "loss": 0.5299856185913085,
      "step": 32100
    },
    {
      "epoch": 44.93072078376487,
      "grad_norm": 0.4547748565673828,
      "learning_rate": 6.109090909090909e-06,
      "loss": 0.5495219421386719,
      "step": 32125
    },
    {
      "epoch": 44.965710286913925,
      "grad_norm": 0.3023133873939514,
      "learning_rate": 6.067132867132867e-06,
      "loss": 0.5401161956787109,
      "step": 32150
    },
    {
      "epoch": 45.0,
      "grad_norm": 0.3042685091495514,
      "learning_rate": 6.025174825174825e-06,
      "loss": 0.520624885559082,
      "step": 32175
    },
    {
      "epoch": 45.0,
      "eval_loss": 0.27153241634368896,
      "eval_mean_accuracy": 0.8025082360041852,
      "eval_mean_iou": 0.708871913936188,
      "eval_overall_accuracy": 0.888710960604791,
      "eval_per_category_accuracy": [
        0.9099540698162184,
        0.8640715000087151,
        0.5870281179647722,
        0.6259891047375962,
        0.8363915899444936,
        0.9916150335533163
      ],
      "eval_per_category_iou": [
        0.794764796172248,
        0.7308857870916592,
        0.4898331274558158,
        0.5409573015197532,
        0.7110974041724603,
        0.9856930672051919
      ],
      "eval_runtime": 29.4587,
      "eval_samples_per_second": 10.761,
      "eval_steps_per_second": 5.397,
      "step": 32175
    },
    {
      "epoch": 45.034989503149056,
      "grad_norm": 0.8792059421539307,
      "learning_rate": 5.983216783216783e-06,
      "loss": 0.5372975158691407,
      "step": 32200
    },
    {
      "epoch": 45.06997900629811,
      "grad_norm": 0.29848822951316833,
      "learning_rate": 5.941258741258741e-06,
      "loss": 0.5572083663940429,
      "step": 32225
    },
    {
      "epoch": 45.10496850944717,
      "grad_norm": 0.25791457295417786,
      "learning_rate": 5.899300699300699e-06,
      "loss": 0.5504449462890625,
      "step": 32250
    },
    {
      "epoch": 45.139958012596225,
      "grad_norm": 1.4536323547363281,
      "learning_rate": 5.857342657342657e-06,
      "loss": 0.5500970077514649,
      "step": 32275
    },
    {
      "epoch": 45.174947515745274,
      "grad_norm": 0.4497101902961731,
      "learning_rate": 5.815384615384615e-06,
      "loss": 0.5509422302246094,
      "step": 32300
    },
    {
      "epoch": 45.20993701889433,
      "grad_norm": 0.6423898339271545,
      "learning_rate": 5.773426573426574e-06,
      "loss": 0.5859440231323242,
      "step": 32325
    },
    {
      "epoch": 45.244926522043386,
      "grad_norm": 0.8013075590133667,
      "learning_rate": 5.731468531468532e-06,
      "loss": 0.5632476043701172,
      "step": 32350
    },
    {
      "epoch": 45.27991602519244,
      "grad_norm": 0.39388856291770935,
      "learning_rate": 5.6895104895104895e-06,
      "loss": 0.5460266876220703,
      "step": 32375
    },
    {
      "epoch": 45.3149055283415,
      "grad_norm": 2.479039192199707,
      "learning_rate": 5.6475524475524475e-06,
      "loss": 0.48347705841064453,
      "step": 32400
    },
    {
      "epoch": 45.349895031490554,
      "grad_norm": 0.9453637003898621,
      "learning_rate": 5.6055944055944055e-06,
      "loss": 0.5111436462402343,
      "step": 32425
    },
    {
      "epoch": 45.38488453463961,
      "grad_norm": 0.324587345123291,
      "learning_rate": 5.563636363636363e-06,
      "loss": 0.5290962600708008,
      "step": 32450
    },
    {
      "epoch": 45.41987403778867,
      "grad_norm": 0.26420217752456665,
      "learning_rate": 5.521678321678321e-06,
      "loss": 0.5732305526733399,
      "step": 32475
    },
    {
      "epoch": 45.454863540937716,
      "grad_norm": 0.562772810459137,
      "learning_rate": 5.479720279720279e-06,
      "loss": 0.5575555419921875,
      "step": 32500
    },
    {
      "epoch": 45.48985304408677,
      "grad_norm": 0.35916420817375183,
      "learning_rate": 5.437762237762237e-06,
      "loss": 0.5785786437988282,
      "step": 32525
    },
    {
      "epoch": 45.52484254723583,
      "grad_norm": 0.2363002449274063,
      "learning_rate": 5.395804195804195e-06,
      "loss": 0.4797877502441406,
      "step": 32550
    },
    {
      "epoch": 45.559832050384884,
      "grad_norm": 0.2264758199453354,
      "learning_rate": 5.353846153846153e-06,
      "loss": 0.5382250595092773,
      "step": 32575
    },
    {
      "epoch": 45.59482155353394,
      "grad_norm": 0.6702804565429688,
      "learning_rate": 5.311888111888112e-06,
      "loss": 0.5774351119995117,
      "step": 32600
    },
    {
      "epoch": 45.629811056682996,
      "grad_norm": 0.4841509163379669,
      "learning_rate": 5.26993006993007e-06,
      "loss": 0.5700375366210938,
      "step": 32625
    },
    {
      "epoch": 45.66480055983205,
      "grad_norm": 0.8449516892433167,
      "learning_rate": 5.227972027972028e-06,
      "loss": 0.5591346740722656,
      "step": 32650
    },
    {
      "epoch": 45.69979006298111,
      "grad_norm": 0.28512343764305115,
      "learning_rate": 5.186013986013986e-06,
      "loss": 0.5656628799438477,
      "step": 32675
    },
    {
      "epoch": 45.73477956613016,
      "grad_norm": 0.4015260636806488,
      "learning_rate": 5.144055944055944e-06,
      "loss": 0.5513787078857422,
      "step": 32700
    },
    {
      "epoch": 45.769769069279214,
      "grad_norm": 0.3041984438896179,
      "learning_rate": 5.102097902097902e-06,
      "loss": 0.5329537200927734,
      "step": 32725
    },
    {
      "epoch": 45.80475857242827,
      "grad_norm": 0.34256792068481445,
      "learning_rate": 5.06013986013986e-06,
      "loss": 0.5541187286376953,
      "step": 32750
    },
    {
      "epoch": 45.839748075577326,
      "grad_norm": 0.5793954730033875,
      "learning_rate": 5.0181818181818186e-06,
      "loss": 0.5661712265014649,
      "step": 32775
    },
    {
      "epoch": 45.87473757872638,
      "grad_norm": 0.5221676826477051,
      "learning_rate": 4.9762237762237765e-06,
      "loss": 0.5303021240234375,
      "step": 32800
    },
    {
      "epoch": 45.90972708187544,
      "grad_norm": 0.5084432363510132,
      "learning_rate": 4.9342657342657345e-06,
      "loss": 0.5526786804199219,
      "step": 32825
    },
    {
      "epoch": 45.944716585024494,
      "grad_norm": 0.39194679260253906,
      "learning_rate": 4.8923076923076924e-06,
      "loss": 0.5306171798706054,
      "step": 32850
    },
    {
      "epoch": 45.97970608817355,
      "grad_norm": 0.26201948523521423,
      "learning_rate": 4.850349650349651e-06,
      "loss": 0.5656985855102539,
      "step": 32875
    },
    {
      "epoch": 46.0,
      "eval_loss": 0.2718743681907654,
      "eval_mean_accuracy": 0.802058030732352,
      "eval_mean_iou": 0.7086639553943687,
      "eval_overall_accuracy": 0.8886520915226982,
      "eval_per_category_accuracy": [
        0.9094078530386873,
        0.8647983903830001,
        0.5850855981926959,
        0.6250398812578896,
        0.8364889838575157,
        0.9915274776643238
      ],
      "eval_per_category_iou": [
        0.7949333527502513,
        0.7309085617261253,
        0.4888323867349308,
        0.540710651370422,
        0.7109217617606369,
        0.9856770180238461
      ],
      "eval_runtime": 28.5951,
      "eval_samples_per_second": 11.086,
      "eval_steps_per_second": 5.56,
      "step": 32890
    },
    {
      "epoch": 46.013995801259625,
      "grad_norm": 0.2549523413181305,
      "learning_rate": 4.808391608391609e-06,
      "loss": 0.5859625244140625,
      "step": 32900
    },
    {
      "epoch": 46.048985304408674,
      "grad_norm": 0.24589045345783234,
      "learning_rate": 4.766433566433567e-06,
      "loss": 0.5301256561279297,
      "step": 32925
    },
    {
      "epoch": 46.08397480755773,
      "grad_norm": 0.2347610741853714,
      "learning_rate": 4.724475524475525e-06,
      "loss": 0.5833869171142578,
      "step": 32950
    },
    {
      "epoch": 46.11896431070679,
      "grad_norm": 0.16605933010578156,
      "learning_rate": 4.682517482517483e-06,
      "loss": 0.5704380416870117,
      "step": 32975
    },
    {
      "epoch": 46.15395381385584,
      "grad_norm": 0.22423619031906128,
      "learning_rate": 4.640559440559441e-06,
      "loss": 0.5509510040283203,
      "step": 33000
    },
    {
      "epoch": 46.1889433170049,
      "grad_norm": 0.30750975012779236,
      "learning_rate": 4.598601398601399e-06,
      "loss": 0.552238540649414,
      "step": 33025
    },
    {
      "epoch": 46.223932820153955,
      "grad_norm": 0.23283874988555908,
      "learning_rate": 4.556643356643357e-06,
      "loss": 0.5330587768554688,
      "step": 33050
    },
    {
      "epoch": 46.25892232330301,
      "grad_norm": 0.24291716516017914,
      "learning_rate": 4.514685314685315e-06,
      "loss": 0.5323178482055664,
      "step": 33075
    },
    {
      "epoch": 46.29391182645207,
      "grad_norm": 0.3600797653198242,
      "learning_rate": 4.472727272727273e-06,
      "loss": 0.5609060668945313,
      "step": 33100
    },
    {
      "epoch": 46.328901329601116,
      "grad_norm": 4.714094638824463,
      "learning_rate": 4.430769230769232e-06,
      "loss": 0.5652193832397461,
      "step": 33125
    },
    {
      "epoch": 46.36389083275017,
      "grad_norm": 0.4333115518093109,
      "learning_rate": 4.38881118881119e-06,
      "loss": 0.5350560760498047,
      "step": 33150
    },
    {
      "epoch": 46.39888033589923,
      "grad_norm": 0.5316377282142639,
      "learning_rate": 4.346853146853148e-06,
      "loss": 0.5204353332519531,
      "step": 33175
    },
    {
      "epoch": 46.433869839048285,
      "grad_norm": 0.3393827974796295,
      "learning_rate": 4.3048951048951056e-06,
      "loss": 0.5727806472778321,
      "step": 33200
    },
    {
      "epoch": 46.46885934219734,
      "grad_norm": 0.9360653758049011,
      "learning_rate": 4.2629370629370635e-06,
      "loss": 0.590174674987793,
      "step": 33225
    },
    {
      "epoch": 46.5038488453464,
      "grad_norm": 0.27246972918510437,
      "learning_rate": 4.2209790209790215e-06,
      "loss": 0.5485932922363281,
      "step": 33250
    },
    {
      "epoch": 46.53883834849545,
      "grad_norm": 0.2514174282550812,
      "learning_rate": 4.1790209790209794e-06,
      "loss": 0.5477318954467774,
      "step": 33275
    },
    {
      "epoch": 46.57382785164451,
      "grad_norm": 0.35868367552757263,
      "learning_rate": 4.137062937062937e-06,
      "loss": 0.5186243820190429,
      "step": 33300
    },
    {
      "epoch": 46.608817354793565,
      "grad_norm": 0.3283415734767914,
      "learning_rate": 4.095104895104895e-06,
      "loss": 0.5605089569091797,
      "step": 33325
    },
    {
      "epoch": 46.643806857942614,
      "grad_norm": 0.361948698759079,
      "learning_rate": 4.053146853146853e-06,
      "loss": 0.5377865982055664,
      "step": 33350
    },
    {
      "epoch": 46.67879636109167,
      "grad_norm": 0.5506464242935181,
      "learning_rate": 4.011188811188811e-06,
      "loss": 0.5782038116455078,
      "step": 33375
    },
    {
      "epoch": 46.71378586424073,
      "grad_norm": 0.978445291519165,
      "learning_rate": 3.96923076923077e-06,
      "loss": 0.5820782089233398,
      "step": 33400
    },
    {
      "epoch": 46.74877536738978,
      "grad_norm": 0.19700367748737335,
      "learning_rate": 3.927272727272728e-06,
      "loss": 0.5204209899902343,
      "step": 33425
    },
    {
      "epoch": 46.78376487053884,
      "grad_norm": 0.8181057572364807,
      "learning_rate": 3.885314685314686e-06,
      "loss": 0.5291571044921874,
      "step": 33450
    },
    {
      "epoch": 46.818754373687895,
      "grad_norm": 0.25582608580589294,
      "learning_rate": 3.843356643356644e-06,
      "loss": 0.5583609771728516,
      "step": 33475
    },
    {
      "epoch": 46.85374387683695,
      "grad_norm": 0.21891652047634125,
      "learning_rate": 3.8013986013986015e-06,
      "loss": 0.5415291213989257,
      "step": 33500
    },
    {
      "epoch": 46.88873337998601,
      "grad_norm": 0.3373849391937256,
      "learning_rate": 3.7594405594405594e-06,
      "loss": 0.5441638565063477,
      "step": 33525
    },
    {
      "epoch": 46.923722883135056,
      "grad_norm": 0.25151968002319336,
      "learning_rate": 3.7174825174825174e-06,
      "loss": 0.5408815765380859,
      "step": 33550
    },
    {
      "epoch": 46.95871238628411,
      "grad_norm": 0.30007821321487427,
      "learning_rate": 3.6755244755244753e-06,
      "loss": 0.5521137619018555,
      "step": 33575
    },
    {
      "epoch": 46.99370188943317,
      "grad_norm": 0.2199898511171341,
      "learning_rate": 3.6335664335664337e-06,
      "loss": 0.53517822265625,
      "step": 33600
    },
    {
      "epoch": 47.0,
      "eval_loss": 0.27130401134490967,
      "eval_mean_accuracy": 0.8032339157816849,
      "eval_mean_iou": 0.7093276338435434,
      "eval_overall_accuracy": 0.8888031992626491,
      "eval_per_category_accuracy": [
        0.9112683554554908,
        0.8632542783128041,
        0.5923074574151915,
        0.6255035937719843,
        0.8354579953821921,
        0.9916118143524469
      ],
      "eval_per_category_iou": [
        0.7950155136603607,
        0.7307813327191862,
        0.4920631553306095,
        0.5411734954115464,
        0.7112377854865308,
        0.9856945204530262
      ],
      "eval_runtime": 28.2983,
      "eval_samples_per_second": 11.202,
      "eval_steps_per_second": 5.619,
      "step": 33605
    }
  ],
  "logging_steps": 25,
  "max_steps": 35750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 7,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.666988246041887e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
